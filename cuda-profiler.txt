==PROF== Connected to process 3717363 (/home/kotlafi2/Double-pendulums/build/Test)
==PROF== Profiling "cuda_right_hand_side" - 0: 0%....50%....100% - 31 passes
==PROF== Profiling "cuda_aux_computation" - 1: 0%....50%....100% - 31 passes
==PROF== Profiling "cuda_right_hand_side" - 2: 0%....50%....100% - 31 passes
==PROF== Profiling "cuda_aux_computation" - 3: 0%....50%....100% - 31 passes
==PROF== Profiling "cuda_right_hand_side" - 4: 0%....50%....100% - 31 passes
==PROF== Profiling "cuda_aux_computation" - 5: 0%....50%....100% - 31 passes
==PROF== Profiling "cuda_right_hand_side" - 6: 0%....50%....100% - 31 passes
==PROF== Profiling "cuda_final_update" - 7: 0%....50%....100% - 31 passes
==PROF== Profiling "cuda_right_hand_side" - 8: 0%....50%....100% - 31 passes
==PROF== Profiling "cuda_aux_computation" - 9: 0%....50%....100% - 31 passes
==PROF== Profiling "cuda_right_hand_side" - 10: 0%....50%....100% - 31 passes
==PROF== Profiling "cuda_aux_computation" - 11: 0%....50%....100% - 31 passes
==PROF== Profiling "cuda_right_hand_side" - 12: 0%....50%....100% - 31 passes
==PROF== Profiling "cuda_aux_computation" - 13: 0%....50%....100% - 31 passes
==PROF== Profiling "cuda_right_hand_side" - 14: 0%....50%....100% - 31 passes
==PROF== Profiling "cuda_final_update" - 15: 0%....50%....100% - 33 passes
==PROF== Profiling "cuda_right_hand_side" - 16: 0%....50%....100% - 31 passes
==PROF== Profiling "cuda_aux_computation" - 17: 0%....50%....100% - 31 passes
==PROF== Profiling "cuda_right_hand_side" - 18: 0%....50%....100% - 31 passes
==PROF== Profiling "cuda_aux_computation" - 19: 0%....50%....100% - 31 passes
==PROF== Profiling "cuda_right_hand_side" - 20: 0%....50%....100% - 31 passes
==PROF== Profiling "cuda_aux_computation" - 21: 0%....50%....100% - 31 passes
==PROF== Profiling "cuda_right_hand_side" - 22: 0%....50%....100% - 31 passes
==PROF== Profiling "cuda_final_update" - 23: 0%....50%....100% - 31 passes
==PROF== Profiling "cuda_right_hand_side" - 24: 0%....50%....100% - 31 passes
==PROF== Profiling "cuda_aux_computation" - 25: 0%....50%....100% - 31 passes
==PROF== Profiling "cuda_right_hand_side" - 26: 0%....50%....100% - 31 passes
==PROF== Profiling "cuda_aux_computation" - 27: 0%....50%....100% - 31 passes
==PROF== Profiling "cuda_right_hand_side" - 28: 0%....50%....100% - 31 passes
==PROF== Profiling "cuda_aux_computation" - 29: 0%....50%....100% - 31 passes
==PROF== Profiling "cuda_right_hand_side" - 30: 0%....50%....100% - 31 passes
==PROF== Profiling "cuda_final_update" - 31: 0%....50%....100% - 31 passes
==PROF== Profiling "cuda_right_hand_side" - 32: 0%....50%....100% - 31 passes
==PROF== Profiling "cuda_aux_computation" - 33: 0%....50%....100% - 31 passes
==PROF== Profiling "cuda_right_hand_side" - 34: 0%....50%....100% - 31 passes
==PROF== Profiling "cuda_aux_computation" - 35: 0%....50%....100% - 31 passes
==PROF== Profiling "cuda_right_hand_side" - 36: 0%....50%....100% - 31 passes
==PROF== Profiling "cuda_aux_computation" - 37: 0%....50%....100% - 31 passes
==PROF== Profiling "cuda_right_hand_side" - 38: 0%....50%....100% - 31 passes
==PROF== Profiling "cuda_final_update" - 39: 0%....50%....100% - 31 passes
==PROF== Profiling "cuda_right_hand_side" - 40: 0%....50%....100% - 31 passes
==PROF== Profiling "cuda_aux_computation" - 41: 0%....50%....100% - 31 passes
==PROF== Profiling "cuda_right_hand_side" - 42: 0%....50%....100% - 31 passes
==PROF== Profiling "cuda_aux_computation" - 43: 0%....50%....100% - 31 passes
==PROF== Profiling "cuda_right_hand_side" - 44: 0%....50%....100% - 31 passes
==PROF== Profiling "cuda_aux_computation" - 45: 0%....50%....100% - 31 passes
==PROF== Profiling "cuda_right_hand_side" - 46: 0%....50%....100% - 31 passes
==PROF== Profiling "cuda_final_update" - 47: 0%....50%....100% - 31 passes
==PROF== Profiling "cuda_right_hand_side" - 48: 0%....50%....100% - 31 passes
==PROF== Profiling "cuda_aux_computation" - 49: 0%....50%....100% - 31 passes
==PROF== Profiling "cuda_right_hand_side" - 50: 0%....50%....100% - 31 passes
==PROF== Profiling "cuda_aux_computation" - 51: 0%....50%....100% - 31 passes
==PROF== Profiling "cuda_right_hand_side" - 52: 0%....50%....100% - 31 passes
==PROF== Profiling "cuda_aux_computation" - 53: 0%....50%....100% - 31 passes
==PROF== Profiling "cuda_right_hand_side" - 54: 0%....50%....100% - 31 passes
==PROF== Profiling "cuda_final_update" - 55: 0%....50%....100% - 31 passes
==PROF== Profiling "cuda_right_hand_side" - 56: 0%....50%....100% - 31 passes
==PROF== Profiling "cuda_aux_computation" - 57: 0%....50%....100% - 31 passes
==PROF== Profiling "cuda_right_hand_side" - 58: 0%....50%....100% - 31 passes
==PROF== Profiling "cuda_aux_computation" - 59: 0%....50%....100% - 31 passes
==PROF== Profiling "cuda_right_hand_side" - 60: 0%....50%....100% - 31 passes
==PROF== Profiling "cuda_aux_computation" - 61: 0%....50%....100% - 31 passes
==PROF== Profiling "cuda_right_hand_side" - 62: 0%....50%....100% - 31 passes
==PROF== Profiling "cuda_final_update" - 63: 0%....50%....100% - 31 passes
==PROF== Profiling "cuda_right_hand_side" - 64: 0%....==PROF== Trying to shutdown target application
 - 16 passes
==ERROR== Failed to profile "cuda_right_hand_side" in process 3717363
==PROF== Trying to shutdown target application
==ERROR== An error occurred while trying to profile.
[3717363] Test@127.0.0.1
  cuda_right_hand_side(const double *, double *, int, double, double, double, double) (256, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.85
    SM Frequency                    Ghz         1.60
    Elapsed Cycles                cycle      366,033
    Memory Throughput                 %         3.48
    DRAM Throughput                   %         3.48
    Duration                         us       228.19
    L1/TEX Cache Throughput           %         4.38
    L2 Cache Throughput               %         2.95
    SM Active Cycles              cycle   317,361.60
    Compute (SM) Throughput           %        75.23
    ----------------------- ----------- ------------

    OPT   Compute is more heavily utilized than Memory: Look at the Compute Workload Analysis section to see what the   
          compute pipelines are spending their time doing. Also, consider whether any computation is redundant and      
          could be reduced or moved to look-up tables.                                                                  

    Section: GPU Speed Of Light Roofline Chart
    OPT   Est. Speedup: 82.07%                                                                                          
          The ratio of peak float (FP32) to double (FP64) performance on this device is 32:1. The workload achieved     
          close to 0% of this device's FP32 peak performance and 54% of its FP64 peak performance. If Compute Workload  
          Analysis determines that this workload is FP64 bound, consider using 32-bit precision floating point          
          operations to improve its performance. See the Profiling Guide                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Kbyte       393.22
    Dropped Samples                sample            0
    Maximum Sampling Interval       cycle       20,000
    # Pass Groups                                    1
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.11
    Executed Ipc Elapsed  inst/cycle         0.09
    Issue Slots Busy               %         2.74
    Issued Ipc Active     inst/cycle         0.11
    SM Busy                        %        86.71
    -------------------- ----------- ------------

    OPT   FP64 is the highest-utilized pipeline (86.7%) based on active cycles, taking into account the rates of its    
          different instructions. It executes 64-bit floating point operations. The pipeline is over-utilized and       
          likely a performance bottleneck. Based on the number of executed instructions, the highest utilized pipeline  
          (86.7%) is FP64. It executes 64-bit floating point operations. Comparing the two, the overall pipeline        
          utilization appears to be caused by frequent, low-latency instructions. See the Profiling Guide               
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-decoder) or hover over the          
          pipeline name to understand the workloads handled by each pipeline. The Instruction Statistics section shows  
          the mix of executed instructions for this workload. Check the Warp State Statistics section for which         
          reasons cause warps to stall.                                                                                 

    Section: Memory Workload Analysis
    ----------------- ----------- ------------
    Metric Name       Metric Unit Metric Value
    ----------------- ----------- ------------
    Memory Throughput     Gbyte/s        15.27
    Mem Busy                    %         2.92
    Max Bandwidth               %         3.48
    L1/TEX Hit Rate             %        79.31
    L2 Hit Rate                 %        80.46
    Mem Pipes Busy              %        19.50
    ----------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 3.138%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 9.1 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 3.288%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32     
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %         2.75
    Issued Warp Per Scheduler                        0.03
    No Eligible                            %        97.25
    Active Warps Per Scheduler          warp         6.57
    Eligible Warps Per Scheduler        warp         0.07
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 24.77%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 36.3 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 8 warps per scheduler, this workload allocates an average of  
          6.57 active warps per scheduler, but only an average of 0.07 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle       238.38
    Warp Cycles Per Executed Instruction           cycle       239.48
    Avg. Active Threads Per Warp                                31.19
    Avg. Not Predicated Off Threads Per Warp                    29.48
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 24.77%                                                                                          
          On average, each warp of this workload spends 116.7 cycles being stalled waiting for a scoreboard dependency  
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 48.9% of the total average of 238.4 cycles       
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 24.77%                                                                                          
          On average, each warp of this workload spends 114.0 cycles being stalled waiting for the L1 instruction queue 
          for texture operations to be not full. This stall reason is high in cases of extreme utilization of the       
          L1TEX pipeline. Try issuing fewer texture fetches, surface loads, surface stores, or decoupled math           
          operations. If applicable, consider combining multiple lower-width memory operations into fewer wider memory  
          operations and try interleaving memory operations and math instructions. Consider converting texture lookups  
          or surface loads into global memory lookups. Texture can accept four threads' requests per cycle, whereas     
          global accepts 32 threads. This stall type represents about 47.8% of the total average of 238.4 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Profiling Guide                                                                            
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst     8,664.83
    Executed Instructions                           inst    1,386,372
    Avg. Issued Instructions Per Scheduler          inst     8,704.89
    Issued Instructions                             inst    1,392,783
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 16.74%                                                                                          
          This kernel executes 392846 fused and 247204 non-fused FP64 instructions. By converting pairs of non-fused    
          instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point),           
          higher-throughput equivalent, the achieved FP64 performance could be increased by up to 19% (relative to its  
          current performance).                                                                                         

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    256
    Registers Per Thread             register/thread              56
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              40
    Stack Size                                                 1,024
    Threads                                   thread          65,536
    # TPCs                                                        20
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                1.60
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 97 thread blocks.   
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 50.0% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        81.77
    Achieved Active Warps Per SM           warp        26.17
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 18.23%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (81.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       54,445
    Total DRAM Elapsed Cycles        cycle   12,505,088
    Average L1 Active Cycles         cycle   317,361.60
    Total L1 Elapsed Cycles          cycle   14,632,232
    Average L2 Active Cycles         cycle    44,439.59
    Total L2 Elapsed Cycles          cycle   11,168,768
    Average SM Active Cycles         cycle   317,361.60
    Total SM Elapsed Cycles          cycle   14,632,232
    Average SMSP Active Cycles       cycle      316,043
    Total SMSP Elapsed Cycles        cycle   58,528,928
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 11.29%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 13.01% above the average, while the minimum instance value is 1.97% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.35%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 13.13% above the average, while the minimum instance value is 2.42% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.29%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 13.01% above the average, while the minimum instance value is 1.97% below the       
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.07
    Branch Instructions              inst       92,524
    Branch Efficiency                   %        99.11
    Avg. Divergent Branches                       3.12
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 8.775%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 421875 excessive sectors (69% of the      
          total 612171 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source         
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  cuda_aux_computation(const double *, const double *, double *, double, int, double) (88, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.82
    SM Frequency                    Ghz         1.57
    Elapsed Cycles                cycle       25,637
    Memory Throughput                 %        86.17
    DRAM Throughput                   %        86.17
    Duration                         us        16.29
    L1/TEX Cache Throughput           %        18.12
    L2 Cache Throughput               %        24.13
    SM Active Cycles              cycle    17,965.35
    Compute (SM) Throughput           %        15.72
    ----------------------- ----------- ------------

    INF   This workload is utilizing greater than 80.0% of the available compute or memory performance of the device.   
          To further improve performance, work will likely need to be shifted from the most utilized to another unit.   
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: GPU Speed Of Light Roofline Chart
    OPT   Est. Speedup: 18.38%                                                                                          
          The ratio of peak float (FP32) to double (FP64) performance on this device is 32:1. The workload achieved 0%  
          of this device's FP32 peak performance and 15% of its FP64 peak performance. If Compute Workload Analysis     
          determines that this workload is FP64 bound, consider using 32-bit precision floating point operations to     
          improve its performance. See the Profiling Guide                                                              
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Kbyte       393.22
    Dropped Samples                sample            0
    Maximum Sampling Interval       cycle       20,000
    # Pass Groups                                    1
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.14
    Executed Ipc Elapsed  inst/cycle         0.12
    Issue Slots Busy               %         3.64
    Issued Ipc Active     inst/cycle         0.15
    SM Busy                        %        18.97
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 81.03%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ----------------- ----------- ------------
    Metric Name       Metric Unit Metric Value
    ----------------- ----------- ------------
    Memory Throughput     Gbyte/s       321.15
    Mem Busy                    %        24.13
    Max Bandwidth               %        86.17
    L1/TEX Hit Rate             %            0
    L2 Hit Rate                 %        33.88
    Mem Pipes Busy              %         7.37
    ----------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %         3.67
    Issued Warp Per Scheduler                        0.04
    No Eligible                            %        96.33
    Active Warps Per Scheduler          warp         4.29
    Eligible Warps Per Scheduler        warp         0.05
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 13.83%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 27.3 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 8 warps per scheduler, this workload allocates an average of  
          4.29 active warps per scheduler, but only an average of 0.05 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp  
          State Statistics and Source Counters sections.                                                                

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle       116.99
    Warp Cycles Per Executed Instruction           cycle       120.11
    Avg. Active Threads Per Warp                                31.99
    Avg. Not Predicated Off Threads Per Warp                    30.04
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 13.83%                                                                                          
          On average, each warp of this workload spends 107.6 cycles being stalled waiting for a scoreboard dependency  
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 92.0% of the total average of 117.0 cycles       
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Profiling Guide                                                                            
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       636.09
    Executed Instructions                           inst      101,775
    Avg. Issued Instructions Per Scheduler          inst       653.09
    Issued Instructions                             inst      104,494
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     88
    Registers Per Thread             register/thread              28
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              40
    Stack Size                                                 1,024
    Threads                                   thread          22,528
    # TPCs                                                        20
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                0.55
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        53.68
    Achieved Active Warps Per SM           warp        17.18
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 13.83%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (53.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    81,731.50
    Total DRAM Elapsed Cycles        cycle      758,784
    Average L1 Active Cycles         cycle    17,965.35
    Total L1 Elapsed Cycles          cycle      867,266
    Average L2 Active Cycles         cycle    16,870.31
    Total L2 Elapsed Cycles          cycle      786,752
    Average SM Active Cycles         cycle    17,965.35
    Total SM Elapsed Cycles          cycle      867,266
    Average SMSP Active Cycles       cycle    17,802.66
    Total SMSP Elapsed Cycles        cycle    3,469,064
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 11.23%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 13.55% above the average, while the minimum instance value is 5.60% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.354%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 10.17% above the average, while the minimum instance value is 4.57% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.23%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 13.55% above the average, while the minimum instance value is 5.60% below the       
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.07
    Branch Instructions              inst        6,907
    Branch Efficiency                   %        99.95
    Avg. Divergent Branches                       0.01
    ------------------------- ----------- ------------

  cuda_right_hand_side(const double *, double *, int, double, double, double, double) (256, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.85
    SM Frequency                    Ghz         1.60
    Elapsed Cycles                cycle      360,556
    Memory Throughput                 %         3.56
    DRAM Throughput                   %         3.56
    Duration                         us       224.83
    L1/TEX Cache Throughput           %         4.46
    L2 Cache Throughput               %         3.00
    SM Active Cycles              cycle   312,436.08
    Compute (SM) Throughput           %        75.41
    ----------------------- ----------- ------------

    OPT   Compute is more heavily utilized than Memory: Look at the Compute Workload Analysis section to see what the   
          compute pipelines are spending their time doing. Also, consider whether any computation is redundant and      
          could be reduced or moved to look-up tables.                                                                  

    Section: GPU Speed Of Light Roofline Chart
    OPT   Est. Speedup: 82.11%                                                                                          
          The ratio of peak float (FP32) to double (FP64) performance on this device is 32:1. The workload achieved     
          close to 0% of this device's FP32 peak performance and 55% of its FP64 peak performance. If Compute Workload  
          Analysis determines that this workload is FP64 bound, consider using 32-bit precision floating point          
          operations to improve its performance. See the Profiling Guide                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Kbyte       393.22
    Dropped Samples                sample            0
    Maximum Sampling Interval       cycle       20,000
    # Pass Groups                                    1
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.11
    Executed Ipc Elapsed  inst/cycle         0.09
    Issue Slots Busy               %         2.70
    Issued Ipc Active     inst/cycle         0.11
    SM Busy                        %        86.77
    -------------------- ----------- ------------

    OPT   FP64 is the highest-utilized pipeline (86.8%) based on active cycles, taking into account the rates of its    
          different instructions. It executes 64-bit floating point operations. The pipeline is over-utilized and       
          likely a performance bottleneck. Based on the number of executed instructions, the highest utilized pipeline  
          (86.8%) is FP64. It executes 64-bit floating point operations. Comparing the two, the overall pipeline        
          utilization appears to be caused by frequent, low-latency instructions. See the Profiling Guide               
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-decoder) or hover over the          
          pipeline name to understand the workloads handled by each pipeline. The Instruction Statistics section shows  
          the mix of executed instructions for this workload. Check the Warp State Statistics section for which         
          reasons cause warps to stall.                                                                                 

    Section: Memory Workload Analysis
    ----------------- ----------- ------------
    Metric Name       Metric Unit Metric Value
    ----------------- ----------- ------------
    Memory Throughput     Gbyte/s        15.62
    Mem Busy                    %         2.95
    Max Bandwidth               %         3.56
    L1/TEX Hit Rate             %        79.33
    L2 Hit Rate                 %        80.73
    Mem Pipes Busy              %        19.72
    ----------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 3.194%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 9.1 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 3.347%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32     
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %         2.71
    Issued Warp Per Scheduler                        0.03
    No Eligible                            %        97.29
    Active Warps Per Scheduler          warp         6.54
    Eligible Warps Per Scheduler        warp         0.07
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 24.59%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 36.9 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 8 warps per scheduler, this workload allocates an average of  
          6.54 active warps per scheduler, but only an average of 0.07 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle       241.38
    Warp Cycles Per Executed Instruction           cycle       242.53
    Avg. Active Threads Per Warp                                31.02
    Avg. Not Predicated Off Threads Per Warp                    29.73
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 24.59%                                                                                          
          On average, each warp of this workload spends 118.7 cycles being stalled waiting for a scoreboard dependency  
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 49.2% of the total average of 241.4 cycles       
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 24.59%                                                                                          
          On average, each warp of this workload spends 115.0 cycles being stalled waiting for the L1 instruction queue 
          for texture operations to be not full. This stall reason is high in cases of extreme utilization of the       
          L1TEX pipeline. Try issuing fewer texture fetches, surface loads, surface stores, or decoupled math           
          operations. If applicable, consider combining multiple lower-width memory operations into fewer wider memory  
          operations and try interleaving memory operations and math instructions. Consider converting texture lookups  
          or surface loads into global memory lookups. Texture can accept four threads' requests per cycle, whereas     
          global accepts 32 threads. This stall type represents about 47.6% of the total average of 241.4 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Profiling Guide                                                                            
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst     8,384.80
    Executed Instructions                           inst    1,341,568
    Avg. Issued Instructions Per Scheduler          inst     8,424.80
    Issued Instructions                             inst    1,347,968
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 16.46%                                                                                          
          This kernel executes 393096 fused and 240388 non-fused FP64 instructions. By converting pairs of non-fused    
          instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point),           
          higher-throughput equivalent, the achieved FP64 performance could be increased by up to 19% (relative to its  
          current performance).                                                                                         

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    256
    Registers Per Thread             register/thread              56
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              40
    Stack Size                                                 1,024
    Threads                                   thread          65,536
    # TPCs                                                        20
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                1.60
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 97 thread blocks.   
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 50.0% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        81.35
    Achieved Active Warps Per SM           warp        26.03
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 18.65%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (81.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       54,876
    Total DRAM Elapsed Cycles        cycle   12,320,768
    Average L1 Active Cycles         cycle   312,436.08
    Total L1 Elapsed Cycles          cycle   14,378,764
    Average L2 Active Cycles         cycle    44,482.91
    Total L2 Elapsed Cycles          cycle   11,002,336
    Average SM Active Cycles         cycle   312,436.08
    Total SM Elapsed Cycles          cycle   14,378,764
    Average SMSP Active Cycles       cycle   311,076.75
    Total SMSP Elapsed Cycles        cycle   57,515,056
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 11.16%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 12.84% above the average, while the minimum instance value is 1.98% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.55%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 13.35% above the average, while the minimum instance value is 2.62% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.16%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 12.84% above the average, while the minimum instance value is 1.98% below the       
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.07
    Branch Instructions              inst       93,024
    Branch Efficiency                   %        98.67
    Avg. Divergent Branches                       4.69
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 8.916%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 421875 excessive sectors (69% of the      
          total 612171 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source         
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  cuda_aux_computation(const double *, const double *, double *, double, int, double) (88, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.76
    SM Frequency                    Ghz         1.57
    Elapsed Cycles                cycle       25,809
    Memory Throughput                 %        88.63
    DRAM Throughput                   %        88.63
    Duration                         us        16.35
    L1/TEX Cache Throughput           %        18.33
    L2 Cache Throughput               %        23.97
    SM Active Cycles              cycle    17,932.88
    Compute (SM) Throughput           %        15.90
    ----------------------- ----------- ------------

    INF   This workload is utilizing greater than 80.0% of the available compute or memory performance of the device.   
          To further improve performance, work will likely need to be shifted from the most utilized to another unit.   
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: GPU Speed Of Light Roofline Chart
    OPT   Est. Speedup: 18.41%                                                                                          
          The ratio of peak float (FP32) to double (FP64) performance on this device is 32:1. The workload achieved 0%  
          of this device's FP32 peak performance and 15% of its FP64 peak performance. If Compute Workload Analysis     
          determines that this workload is FP64 bound, consider using 32-bit precision floating point operations to     
          improve its performance. See the Profiling Guide                                                              
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Kbyte       393.22
    Dropped Samples                sample            0
    Maximum Sampling Interval       cycle       20,000
    # Pass Groups                                    1
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.14
    Executed Ipc Elapsed  inst/cycle         0.12
    Issue Slots Busy               %         3.64
    Issued Ipc Active     inst/cycle         0.15
    SM Busy                        %        19.00
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 81%                                                                                       
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ----------------- ----------- ------------
    Metric Name       Metric Unit Metric Value
    ----------------- ----------- ------------
    Memory Throughput     Gbyte/s       326.81
    Mem Busy                    %        23.97
    Max Bandwidth               %        88.63
    L1/TEX Hit Rate             %            0
    L2 Hit Rate                 %        33.89
    Mem Pipes Busy              %         7.46
    ----------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %         3.61
    Issued Warp Per Scheduler                        0.04
    No Eligible                            %        96.39
    Active Warps Per Scheduler          warp         4.26
    Eligible Warps Per Scheduler        warp         0.04
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 11.37%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 27.7 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 8 warps per scheduler, this workload allocates an average of  
          4.26 active warps per scheduler, but only an average of 0.04 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp  
          State Statistics and Source Counters sections.                                                                

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle       118.03
    Warp Cycles Per Executed Instruction           cycle       121.19
    Avg. Active Threads Per Warp                                31.99
    Avg. Not Predicated Off Threads Per Warp                    30.04
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 11.37%                                                                                          
          On average, each warp of this workload spends 107.7 cycles being stalled waiting for a scoreboard dependency  
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 91.2% of the total average of 118.0 cycles       
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Profiling Guide                                                                            
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       636.09
    Executed Instructions                           inst      101,775
    Avg. Issued Instructions Per Scheduler          inst       653.13
    Issued Instructions                             inst      104,501
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     88
    Registers Per Thread             register/thread              28
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              40
    Stack Size                                                 1,024
    Threads                                   thread          22,528
    # TPCs                                                        20
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                0.55
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        53.73
    Achieved Active Warps Per SM           warp        17.20
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 11.37%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (53.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    83,499.50
    Total DRAM Elapsed Cycles        cycle      753,664
    Average L1 Active Cycles         cycle    17,932.88
    Total L1 Elapsed Cycles          cycle      857,424
    Average L2 Active Cycles         cycle    17,286.84
    Total L2 Elapsed Cycles          cycle      791,200
    Average SM Active Cycles         cycle    17,932.88
    Total SM Elapsed Cycles          cycle      857,424
    Average SMSP Active Cycles       cycle    18,100.54
    Total SMSP Elapsed Cycles        cycle    3,429,696
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 9.536%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 11.40% above the average, while the minimum instance value is 6.32% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.637%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 10.23% above the average, while the minimum instance value is 5.57% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 9.536%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 11.40% above the average, while the minimum instance value is 6.32% below the       
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.07
    Branch Instructions              inst        6,907
    Branch Efficiency                   %        99.95
    Avg. Divergent Branches                       0.01
    ------------------------- ----------- ------------

  cuda_right_hand_side(const double *, double *, int, double, double, double, double) (256, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.78
    SM Frequency                    Ghz         1.60
    Elapsed Cycles                cycle      356,534
    Memory Throughput                 %         3.62
    DRAM Throughput                   %         3.62
    Duration                         us       222.34
    L1/TEX Cache Throughput           %         4.51
    L2 Cache Throughput               %         3.02
    SM Active Cycles              cycle   309,250.10
    Compute (SM) Throughput           %        75.52
    ----------------------- ----------- ------------

    OPT   Compute is more heavily utilized than Memory: Look at the Compute Workload Analysis section to see what the   
          compute pipelines are spending their time doing. Also, consider whether any computation is redundant and      
          could be reduced or moved to look-up tables.                                                                  

    Section: GPU Speed Of Light Roofline Chart
    OPT   Est. Speedup: 82.22%                                                                                          
          The ratio of peak float (FP32) to double (FP64) performance on this device is 32:1. The workload achieved     
          close to 0% of this device's FP32 peak performance and 56% of its FP64 peak performance. If Compute Workload  
          Analysis determines that this workload is FP64 bound, consider using 32-bit precision floating point          
          operations to improve its performance. See the Profiling Guide                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Kbyte       393.22
    Dropped Samples                sample            0
    Maximum Sampling Interval       cycle       20,000
    # Pass Groups                                    1
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.11
    Executed Ipc Elapsed  inst/cycle         0.09
    Issue Slots Busy               %         2.65
    Issued Ipc Active     inst/cycle         0.11
    SM Busy                        %        86.88
    -------------------- ----------- ------------

    OPT   FP64 is the highest-utilized pipeline (86.9%) based on active cycles, taking into account the rates of its    
          different instructions. It executes 64-bit floating point operations. The pipeline is over-utilized and       
          likely a performance bottleneck. Based on the number of executed instructions, the highest utilized pipeline  
          (86.9%) is FP64. It executes 64-bit floating point operations. Comparing the two, the overall pipeline        
          utilization appears to be caused by frequent, low-latency instructions. See the Profiling Guide               
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-decoder) or hover over the          
          pipeline name to understand the workloads handled by each pipeline. The Instruction Statistics section shows  
          the mix of executed instructions for this workload. Check the Warp State Statistics section for which         
          reasons cause warps to stall.                                                                                 

    Section: Memory Workload Analysis
    ----------------- ----------- ------------
    Metric Name       Metric Unit Metric Value
    ----------------- ----------- ------------
    Memory Throughput     Gbyte/s        15.72
    Mem Busy                    %         2.99
    Max Bandwidth               %         3.62
    L1/TEX Hit Rate             %        79.32
    L2 Hit Rate                 %        80.37
    Mem Pipes Busy              %        19.73
    ----------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 3.227%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 9.1 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 3.382%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32     
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %         2.66
    Issued Warp Per Scheduler                        0.03
    No Eligible                            %        97.34
    Active Warps Per Scheduler          warp         6.62
    Eligible Warps Per Scheduler        warp         0.07
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 24.48%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 37.6 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 8 warps per scheduler, this workload allocates an average of  
          6.62 active warps per scheduler, but only an average of 0.07 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle       249.01
    Warp Cycles Per Executed Instruction           cycle       250.23
    Avg. Active Threads Per Warp                                31.84
    Avg. Not Predicated Off Threads Per Warp                    30.52
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 24.48%                                                                                          
          On average, each warp of this workload spends 121.0 cycles being stalled waiting for a scoreboard dependency  
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 48.6% of the total average of 249.0 cycles       
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 24.48%                                                                                          
          On average, each warp of this workload spends 120.3 cycles being stalled waiting for the L1 instruction queue 
          for texture operations to be not full. This stall reason is high in cases of extreme utilization of the       
          L1TEX pipeline. Try issuing fewer texture fetches, surface loads, surface stores, or decoupled math           
          operations. If applicable, consider combining multiple lower-width memory operations into fewer wider memory  
          operations and try interleaving memory operations and math instructions. Consider converting texture lookups  
          or surface loads into global memory lookups. Texture can accept four threads' requests per cycle, whereas     
          global accepts 32 threads. This stall type represents about 48.3% of the total average of 249.0 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Profiling Guide                                                                            
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst     8,162.93
    Executed Instructions                           inst    1,306,068
    Avg. Issued Instructions Per Scheduler          inst     8,202.94
    Issued Instructions                             inst    1,312,471
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 16.55%                                                                                          
          This kernel executes 389096 fused and 239388 non-fused FP64 instructions. By converting pairs of non-fused    
          instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point),           
          higher-throughput equivalent, the achieved FP64 performance could be increased by up to 19% (relative to its  
          current performance).                                                                                         

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    256
    Registers Per Thread             register/thread              56
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              40
    Stack Size                                                 1,024
    Threads                                   thread          65,536
    # TPCs                                                        20
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                1.60
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 97 thread blocks.   
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 50.0% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        82.59
    Achieved Active Warps Per SM           warp        26.43
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 17.41%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (82.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    54,603.50
    Total DRAM Elapsed Cycles        cycle   12,059,648
    Average L1 Active Cycles         cycle   309,250.10
    Total L1 Elapsed Cycles          cycle   14,230,918
    Average L2 Active Cycles         cycle    42,468.56
    Total L2 Elapsed Cycles          cycle   10,880,224
    Average SM Active Cycles         cycle   309,250.10
    Total SM Elapsed Cycles          cycle   14,230,918
    Average SMSP Active Cycles       cycle   308,726.67
    Total SMSP Elapsed Cycles        cycle   56,923,672
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 11.05%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 12.71% above the average, while the minimum instance value is 1.87% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.18%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 12.89% above the average, while the minimum instance value is 1.97% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.05%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 12.71% above the average, while the minimum instance value is 1.87% below the       
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.07
    Branch Instructions              inst       88,524
    Branch Efficiency                   %        99.53
    Avg. Divergent Branches                       1.56
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 8.608%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 421875 excessive sectors (69% of the      
          total 612171 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source         
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  cuda_aux_computation(const double *, const double *, double *, double, int, double) (88, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.72
    SM Frequency                    Ghz         1.57
    Elapsed Cycles                cycle       25,840
    Memory Throughput                 %        88.92
    DRAM Throughput                   %        88.92
    Duration                         us        16.35
    L1/TEX Cache Throughput           %        18.18
    L2 Cache Throughput               %        23.94
    SM Active Cycles              cycle    17,896.17
    Compute (SM) Throughput           %        15.76
    ----------------------- ----------- ------------

    INF   This workload is utilizing greater than 80.0% of the available compute or memory performance of the device.   
          To further improve performance, work will likely need to be shifted from the most utilized to another unit.   
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: GPU Speed Of Light Roofline Chart
    OPT   Est. Speedup: 18.45%                                                                                          
          The ratio of peak float (FP32) to double (FP64) performance on this device is 32:1. The workload achieved 0%  
          of this device's FP32 peak performance and 15% of its FP64 peak performance. If Compute Workload Analysis     
          determines that this workload is FP64 bound, consider using 32-bit precision floating point operations to     
          improve its performance. See the Profiling Guide                                                              
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Kbyte       393.22
    Dropped Samples                sample            0
    Maximum Sampling Interval       cycle       20,000
    # Pass Groups                                    1
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.14
    Executed Ipc Elapsed  inst/cycle         0.12
    Issue Slots Busy               %         3.65
    Issued Ipc Active     inst/cycle         0.15
    SM Busy                        %        19.04
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 80.96%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ----------------- ----------- ------------
    Metric Name       Metric Unit Metric Value
    ----------------- ----------- ------------
    Memory Throughput     Gbyte/s       325.63
    Mem Busy                    %        23.94
    Max Bandwidth               %        88.92
    L1/TEX Hit Rate             %            0
    L2 Hit Rate                 %        33.89
    Mem Pipes Busy              %         7.39
    ----------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %         3.65
    Issued Warp Per Scheduler                        0.04
    No Eligible                            %        96.35
    Active Warps Per Scheduler          warp         4.30
    Eligible Warps Per Scheduler        warp         0.05
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 11.08%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 27.4 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 8 warps per scheduler, this workload allocates an average of  
          4.30 active warps per scheduler, but only an average of 0.05 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp  
          State Statistics and Source Counters sections.                                                                

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle       117.93
    Warp Cycles Per Executed Instruction           cycle       121.08
    Avg. Active Threads Per Warp                                31.99
    Avg. Not Predicated Off Threads Per Warp                    30.04
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 11.08%                                                                                          
          On average, each warp of this workload spends 107.5 cycles being stalled waiting for a scoreboard dependency  
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 91.2% of the total average of 117.9 cycles       
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Profiling Guide                                                                            
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       636.09
    Executed Instructions                           inst      101,775
    Avg. Issued Instructions Per Scheduler          inst       653.09
    Issued Instructions                             inst      104,494
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     88
    Registers Per Thread             register/thread              28
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              40
    Stack Size                                                 1,024
    Threads                                   thread          22,528
    # TPCs                                                        20
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                0.55
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        53.60
    Achieved Active Warps Per SM           warp        17.15
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 11.08%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (53.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    83,197.50
    Total DRAM Elapsed Cycles        cycle      748,544
    Average L1 Active Cycles         cycle    17,896.17
    Total L1 Elapsed Cycles          cycle      865,176
    Average L2 Active Cycles         cycle    17,210.25
    Total L2 Elapsed Cycles          cycle      792,032
    Average SM Active Cycles         cycle    17,896.17
    Total SM Elapsed Cycles          cycle      865,176
    Average SMSP Active Cycles       cycle    17,911.47
    Total SMSP Elapsed Cycles        cycle    3,460,704
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 11.84%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 14.31% above the average, while the minimum instance value is 3.88% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.928%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 10.78% above the average, while the minimum instance value is 6.96% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.84%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 14.31% above the average, while the minimum instance value is 3.88% below the       
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.07
    Branch Instructions              inst        6,907
    Branch Efficiency                   %        99.95
    Avg. Divergent Branches                       0.01
    ------------------------- ----------- ------------

  cuda_right_hand_side(const double *, double *, int, double, double, double, double) (256, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.80
    SM Frequency                    Ghz         1.59
    Elapsed Cycles                cycle      355,085
    Memory Throughput                 %         3.60
    DRAM Throughput                   %         3.60
    Duration                         us       222.56
    L1/TEX Cache Throughput           %         4.52
    L2 Cache Throughput               %         3.03
    SM Active Cycles              cycle   308,183.22
    Compute (SM) Throughput           %        75.48
    ----------------------- ----------- ------------

    OPT   Compute is more heavily utilized than Memory: Look at the Compute Workload Analysis section to see what the   
          compute pipelines are spending their time doing. Also, consider whether any computation is redundant and      
          could be reduced or moved to look-up tables.                                                                  

    Section: GPU Speed Of Light Roofline Chart
    OPT   Est. Speedup: 82.32%                                                                                          
          The ratio of peak float (FP32) to double (FP64) performance on this device is 32:1. The workload achieved     
          close to 0% of this device's FP32 peak performance and 56% of its FP64 peak performance. If Compute Workload  
          Analysis determines that this workload is FP64 bound, consider using 32-bit precision floating point          
          operations to improve its performance. See the Profiling Guide                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Kbyte       393.22
    Dropped Samples                sample            0
    Maximum Sampling Interval       cycle       20,000
    # Pass Groups                                    1
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.11
    Executed Ipc Elapsed  inst/cycle         0.09
    Issue Slots Busy               %         2.65
    Issued Ipc Active     inst/cycle         0.11
    SM Busy                        %        86.99
    -------------------- ----------- ------------

    OPT   FP64 is the highest-utilized pipeline (87.0%) based on active cycles, taking into account the rates of its    
          different instructions. It executes 64-bit floating point operations. The pipeline is over-utilized and       
          likely a performance bottleneck. Based on the number of executed instructions, the highest utilized pipeline  
          (87.0%) is FP64. It executes 64-bit floating point operations. Comparing the two, the overall pipeline        
          utilization appears to be caused by frequent, low-latency instructions. See the Profiling Guide               
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-decoder) or hover over the          
          pipeline name to understand the workloads handled by each pipeline. The Instruction Statistics section shows  
          the mix of executed instructions for this workload. Check the Warp State Statistics section for which         
          reasons cause warps to stall.                                                                                 

    Section: Memory Workload Analysis
    ----------------- ----------- ------------
    Metric Name       Metric Unit Metric Value
    ----------------- ----------- ------------
    Memory Throughput     Gbyte/s        15.66
    Mem Busy                    %         3.00
    Max Bandwidth               %         3.60
    L1/TEX Hit Rate             %        79.22
    L2 Hit Rate                 %        80.16
    Mem Pipes Busy              %        19.73
    ----------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 3.232%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 9.1 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 3.387%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32     
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %         2.65
    Issued Warp Per Scheduler                        0.03
    No Eligible                            %        97.35
    Active Warps Per Scheduler          warp         6.62
    Eligible Warps Per Scheduler        warp         0.07
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 24.52%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 37.7 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 8 warps per scheduler, this workload allocates an average of  
          6.62 active warps per scheduler, but only an average of 0.07 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle       249.79
    Warp Cycles Per Executed Instruction           cycle       251.02
    Avg. Active Threads Per Warp                                31.99
    Avg. Not Predicated Off Threads Per Warp                    30.66
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 24.52%                                                                                          
          On average, each warp of this workload spends 121.2 cycles being stalled waiting for a scoreboard dependency  
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 48.5% of the total average of 249.8 cycles       
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 24.52%                                                                                          
          On average, each warp of this workload spends 120.8 cycles being stalled waiting for the L1 instruction queue 
          for texture operations to be not full. This stall reason is high in cases of extreme utilization of the       
          L1TEX pipeline. Try issuing fewer texture fetches, surface loads, surface stores, or decoupled math           
          operations. If applicable, consider combining multiple lower-width memory operations into fewer wider memory  
          operations and try interleaving memory operations and math instructions. Consider converting texture lookups  
          or surface loads into global memory lookups. Texture can accept four threads' requests per cycle, whereas     
          global accepts 32 threads. This stall type represents about 48.4% of the total average of 249.8 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Profiling Guide                                                                            
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst     8,125.43
    Executed Instructions                           inst    1,300,068
    Avg. Issued Instructions Per Scheduler          inst     8,165.43
    Issued Instructions                             inst    1,306,469
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 16.53%                                                                                          
          This kernel executes 388846 fused and 238388 non-fused FP64 instructions. By converting pairs of non-fused    
          instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point),           
          higher-throughput equivalent, the achieved FP64 performance could be increased by up to 19% (relative to its  
          current performance).                                                                                         

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    256
    Registers Per Thread             register/thread              56
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              40
    Stack Size                                                 1,024
    Threads                                   thread          65,536
    # TPCs                                                        20
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                1.60
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 97 thread blocks.   
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 50.0% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        82.79
    Achieved Active Warps Per SM           warp        26.49
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 17.21%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (82.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       54,448
    Total DRAM Elapsed Cycles        cycle   12,107,776
    Average L1 Active Cycles         cycle   308,183.22
    Total L1 Elapsed Cycles          cycle   14,206,326
    Average L2 Active Cycles         cycle    42,037.19
    Total L2 Elapsed Cycles          cycle   10,872,288
    Average SM Active Cycles         cycle   308,183.22
    Total SM Elapsed Cycles          cycle   14,206,326
    Average SMSP Active Cycles       cycle   307,935.48
    Total SMSP Elapsed Cycles        cycle   56,825,304
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 11.06%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 12.74% above the average, while the minimum instance value is 1.95% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.09%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 12.79% above the average, while the minimum instance value is 1.87% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.06%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 12.74% above the average, while the minimum instance value is 1.95% below the       
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.07
    Branch Instructions              inst       88,024
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 8.527%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 421875 excessive sectors (69% of the      
          total 612171 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source         
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  cuda_final_update(double *, const double *, const double *, const double *, const double *, double, int) (88, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Ghz         1.57
    Elapsed Cycles                cycle       54,761
    Memory Throughput                 %        89.72
    DRAM Throughput                   %        89.72
    Duration                         us        34.69
    L1/TEX Cache Throughput           %        17.21
    L2 Cache Throughput               %        22.44
    SM Active Cycles              cycle    45,387.05
    Compute (SM) Throughput           %        25.73
    ----------------------- ----------- ------------

    INF   This workload is utilizing greater than 80.0% of the available compute or memory performance of the device.   
          To further improve performance, work will likely need to be shifted from the most utilized to another unit.   
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: GPU Speed Of Light Roofline Chart
    OPT   Est. Speedup: 27.29%                                                                                          
          The ratio of peak float (FP32) to double (FP64) performance on this device is 32:1. The workload achieved 0%  
          of this device's FP32 peak performance and 22% of its FP64 peak performance. If Compute Workload Analysis     
          determines that this workload is FP64 bound, consider using 32-bit precision floating point operations to     
          improve its performance. See the Profiling Guide                                                              
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Kbyte       393.22
    Dropped Samples                sample            0
    Maximum Sampling Interval       cycle       20,000
    # Pass Groups                                    1
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.09
    Executed Ipc Elapsed  inst/cycle         0.08
    Issue Slots Busy               %         2.32
    Issued Ipc Active     inst/cycle         0.09
    SM Busy                        %        28.17
    -------------------- ----------- ------------

    INF   FP64 is the highest-utilized pipeline (28.2%) based on active cycles, taking into account the rates of its    
          different instructions. It executes 64-bit floating point operations. It is well-utilized, but should not be  
          a bottleneck.                                                                                                 

    Section: Memory Workload Analysis
    ----------------- ----------- ------------
    Metric Name       Metric Unit Metric Value
    ----------------- ----------- ------------
    Memory Throughput     Gbyte/s       355.13
    Mem Busy                    %        22.44
    Max Bandwidth               %        89.72
    L1/TEX Hit Rate             %        14.14
    L2 Hit Rate                 %        17.08
    Mem Pipes Busy              %         7.93
    ----------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %         2.34
    Issued Warp Per Scheduler                        0.02
    No Eligible                            %        97.66
    Active Warps Per Scheduler          warp         4.28
    Eligible Warps Per Scheduler        warp         0.03
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 10.28%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 42.7 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 8 warps per scheduler, this workload allocates an average of  
          4.28 active warps per scheduler, but only an average of 0.03 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp  
          State Statistics and Source Counters sections.                                                                

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle       182.65
    Warp Cycles Per Executed Instruction           cycle       185.95
    Avg. Active Threads Per Warp                                31.99
    Avg. Not Predicated Off Threads Per Warp                    30.79
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 10.28%                                                                                          
          On average, each warp of this workload spends 175.4 cycles being stalled waiting for a scoreboard dependency  
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 96.0% of the total average of 182.7 cycles       
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Profiling Guide                                                                            
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst     1,034.84
    Executed Instructions                           inst      165,575
    Avg. Issued Instructions Per Scheduler          inst     1,053.54
    Issued Instructions                             inst      168,566
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 3.754%                                                                                          
          This kernel executes 23448 fused and 8520 non-fused FP64 instructions. By converting pairs of non-fused       
          instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point),           
          higher-throughput equivalent, the achieved FP64 performance could be increased by up to 13% (relative to its  
          current performance).                                                                                         

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     88
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              40
    Stack Size                                                 1,024
    Threads                                   thread          22,528
    # TPCs                                                        20
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                0.55
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        53.70
    Achieved Active Warps Per SM           warp        17.18
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 10.28%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (53.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      192,478
    Total DRAM Elapsed Cycles        cycle    1,716,224
    Average L1 Active Cycles         cycle    45,387.05
    Total L1 Elapsed Cycles          cycle    1,987,990
    Average L2 Active Cycles         cycle    42,991.44
    Total L2 Elapsed Cycles          cycle    1,680,992
    Average SM Active Cycles         cycle    45,387.05
    Total SM Elapsed Cycles          cycle    1,987,990
    Average SMSP Active Cycles       cycle    44,990.97
    Total SMSP Elapsed Cycles        cycle    7,951,960
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.918%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 6.48% above the average, while the minimum instance value is 3.05% below the average.       
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.025%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 6.66% above the average, while the minimum instance value is 3.76% below the        
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.918%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 6.48% above the average, while the minimum instance value is 3.05% below the        
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.04
    Branch Instructions              inst        6,907
    Branch Efficiency                   %        99.95
    Avg. Divergent Branches                       0.01
    ------------------------- ----------- ------------

  cuda_right_hand_side(const double *, double *, int, double, double, double, double) (256, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.80
    SM Frequency                    Ghz         1.60
    Elapsed Cycles                cycle      355,169
    Memory Throughput                 %         3.63
    DRAM Throughput                   %         3.63
    Duration                         us       221.47
    L1/TEX Cache Throughput           %         4.52
    L2 Cache Throughput               %         3.07
    SM Active Cycles              cycle   308,256.85
    Compute (SM) Throughput           %        75.57
    ----------------------- ----------- ------------

    OPT   Compute is more heavily utilized than Memory: Look at the Compute Workload Analysis section to see what the   
          compute pipelines are spending their time doing. Also, consider whether any computation is redundant and      
          could be reduced or moved to look-up tables.                                                                  

    Section: GPU Speed Of Light Roofline Chart
    OPT   Est. Speedup: 82.3%                                                                                           
          The ratio of peak float (FP32) to double (FP64) performance on this device is 32:1. The workload achieved     
          close to 0% of this device's FP32 peak performance and 56% of its FP64 peak performance. If Compute Workload  
          Analysis determines that this workload is FP64 bound, consider using 32-bit precision floating point          
          operations to improve its performance. See the Profiling Guide                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Kbyte       393.22
    Dropped Samples                sample            0
    Maximum Sampling Interval       cycle       20,000
    # Pass Groups                                    1
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.11
    Executed Ipc Elapsed  inst/cycle         0.09
    Issue Slots Busy               %         2.65
    Issued Ipc Active     inst/cycle         0.11
    SM Busy                        %        86.97
    -------------------- ----------- ------------

    OPT   FP64 is the highest-utilized pipeline (87.0%) based on active cycles, taking into account the rates of its    
          different instructions. It executes 64-bit floating point operations. The pipeline is over-utilized and       
          likely a performance bottleneck. Based on the number of executed instructions, the highest utilized pipeline  
          (87.0%) is FP64. It executes 64-bit floating point operations. Comparing the two, the overall pipeline        
          utilization appears to be caused by frequent, low-latency instructions. See the Profiling Guide               
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-decoder) or hover over the          
          pipeline name to understand the workloads handled by each pipeline. The Instruction Statistics section shows  
          the mix of executed instructions for this workload. Check the Warp State Statistics section for which         
          reasons cause warps to stall.                                                                                 

    Section: Memory Workload Analysis
    ----------------- ----------- ------------
    Metric Name       Metric Unit Metric Value
    ----------------- ----------- ------------
    Memory Throughput     Gbyte/s        15.78
    Mem Busy                    %         3.07
    Max Bandwidth               %         3.63
    L1/TEX Hit Rate             %        79.25
    L2 Hit Rate                 %        78.82
    Mem Pipes Busy              %        19.75
    ----------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 3.237%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 9.1 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 3.392%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32     
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %         2.65
    Issued Warp Per Scheduler                        0.03
    No Eligible                            %        97.35
    Active Warps Per Scheduler          warp         6.63
    Eligible Warps Per Scheduler        warp         0.07
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 24.43%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 37.7 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 8 warps per scheduler, this workload allocates an average of  
          6.63 active warps per scheduler, but only an average of 0.07 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle       249.96
    Warp Cycles Per Executed Instruction           cycle       251.19
    Avg. Active Threads Per Warp                                31.99
    Avg. Not Predicated Off Threads Per Warp                    30.66
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 24.43%                                                                                          
          On average, each warp of this workload spends 121.1 cycles being stalled waiting for a scoreboard dependency  
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 48.5% of the total average of 250.0 cycles       
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 24.43%                                                                                          
          On average, each warp of this workload spends 120.9 cycles being stalled waiting for the L1 instruction queue 
          for texture operations to be not full. This stall reason is high in cases of extreme utilization of the       
          L1TEX pipeline. Try issuing fewer texture fetches, surface loads, surface stores, or decoupled math           
          operations. If applicable, consider combining multiple lower-width memory operations into fewer wider memory  
          operations and try interleaving memory operations and math instructions. Consider converting texture lookups  
          or surface loads into global memory lookups. Texture can accept four threads' requests per cycle, whereas     
          global accepts 32 threads. This stall type represents about 48.4% of the total average of 250.0 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Profiling Guide                                                                            
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst     8,125.43
    Executed Instructions                           inst    1,300,068
    Avg. Issued Instructions Per Scheduler          inst     8,165.46
    Issued Instructions                             inst    1,306,474
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 16.53%                                                                                          
          This kernel executes 388846 fused and 238388 non-fused FP64 instructions. By converting pairs of non-fused    
          instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point),           
          higher-throughput equivalent, the achieved FP64 performance could be increased by up to 19% (relative to its  
          current performance).                                                                                         

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    256
    Registers Per Thread             register/thread              56
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              40
    Stack Size                                                 1,024
    Threads                                   thread          65,536
    # TPCs                                                        20
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                1.60
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 97 thread blocks.   
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 50.0% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        82.77
    Achieved Active Warps Per SM           warp        26.49
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 17.23%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (82.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    54,612.50
    Total DRAM Elapsed Cycles        cycle   12,046,336
    Average L1 Active Cycles         cycle   308,256.85
    Total L1 Elapsed Cycles          cycle   14,190,456
    Average L2 Active Cycles         cycle    41,804.34
    Total L2 Elapsed Cycles          cycle   10,837,248
    Average SM Active Cycles         cycle   308,256.85
    Total SM Elapsed Cycles          cycle   14,190,456
    Average SMSP Active Cycles       cycle   307,902.65
    Total SMSP Elapsed Cycles        cycle   56,761,824
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 11.1%                                                                                           
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 12.77% above the average, while the minimum instance value is 1.84% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.12%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 12.81% above the average, while the minimum instance value is 1.82% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.1%                                                                                           
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 12.77% above the average, while the minimum instance value is 1.84% below the       
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.07
    Branch Instructions              inst       88,024
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 8.507%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 421875 excessive sectors (69% of the      
          total 612171 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source         
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  cuda_aux_computation(const double *, const double *, double *, double, int, double) (88, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.78
    SM Frequency                    Ghz         1.57
    Elapsed Cycles                cycle       25,654
    Memory Throughput                 %        86.95
    DRAM Throughput                   %        86.95
    Duration                         us        16.32
    L1/TEX Cache Throughput           %        18.13
    L2 Cache Throughput               %        24.04
    SM Active Cycles              cycle    18,098.45
    Compute (SM) Throughput           %        15.72
    ----------------------- ----------- ------------

    INF   This workload is utilizing greater than 80.0% of the available compute or memory performance of the device.   
          To further improve performance, work will likely need to be shifted from the most utilized to another unit.   
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: GPU Speed Of Light Roofline Chart
    OPT   Est. Speedup: 18.24%                                                                                          
          The ratio of peak float (FP32) to double (FP64) performance on this device is 32:1. The workload achieved 0%  
          of this device's FP32 peak performance and 15% of its FP64 peak performance. If Compute Workload Analysis     
          determines that this workload is FP64 bound, consider using 32-bit precision floating point operations to     
          improve its performance. See the Profiling Guide                                                              
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Kbyte       393.22
    Dropped Samples                sample            0
    Maximum Sampling Interval       cycle       20,000
    # Pass Groups                                    1
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.14
    Executed Ipc Elapsed  inst/cycle         0.12
    Issue Slots Busy               %         3.61
    Issued Ipc Active     inst/cycle         0.14
    SM Busy                        %        18.83
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 81.17%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ----------------- ----------- ------------
    Metric Name       Metric Unit Metric Value
    ----------------- ----------- ------------
    Memory Throughput     Gbyte/s       321.67
    Mem Busy                    %        24.04
    Max Bandwidth               %        86.95
    L1/TEX Hit Rate             %            0
    L2 Hit Rate                 %        33.94
    Mem Pipes Busy              %         7.37
    ----------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %         3.63
    Issued Warp Per Scheduler                        0.04
    No Eligible                            %        96.37
    Active Warps Per Scheduler          warp         4.27
    Eligible Warps Per Scheduler        warp         0.04
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 13.05%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 27.5 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 8 warps per scheduler, this workload allocates an average of  
          4.27 active warps per scheduler, but only an average of 0.04 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp  
          State Statistics and Source Counters sections.                                                                

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle       117.70
    Warp Cycles Per Executed Instruction           cycle       120.86
    Avg. Active Threads Per Warp                                31.99
    Avg. Not Predicated Off Threads Per Warp                    30.04
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 13.05%                                                                                          
          On average, each warp of this workload spends 107.6 cycles being stalled waiting for a scoreboard dependency  
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 91.4% of the total average of 117.7 cycles       
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Profiling Guide                                                                            
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       636.09
    Executed Instructions                           inst      101,775
    Avg. Issued Instructions Per Scheduler          inst       653.14
    Issued Instructions                             inst      104,502
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     88
    Registers Per Thread             register/thread              28
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              40
    Stack Size                                                 1,024
    Threads                                   thread          22,528
    # TPCs                                                        20
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                0.55
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        53.01
    Achieved Active Warps Per SM           warp        16.96
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 13.05%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (53.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       82,025
    Total DRAM Elapsed Cycles        cycle      754,688
    Average L1 Active Cycles         cycle    18,098.45
    Total L1 Elapsed Cycles          cycle      867,242
    Average L2 Active Cycles         cycle    17,141.72
    Total L2 Elapsed Cycles          cycle      787,904
    Average SM Active Cycles         cycle    18,098.45
    Total SM Elapsed Cycles          cycle      867,242
    Average SMSP Active Cycles       cycle    17,989.11
    Total SMSP Elapsed Cycles        cycle    3,468,968
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 9.584%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 11.48% above the average, while the minimum instance value is 5.28% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 9.63%                                                                                           
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 11.61% above the average, while the minimum instance value is 4.25% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 9.584%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 11.48% above the average, while the minimum instance value is 5.28% below the       
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.07
    Branch Instructions              inst        6,907
    Branch Efficiency                   %        99.95
    Avg. Divergent Branches                       0.01
    ------------------------- ----------- ------------

  cuda_right_hand_side(const double *, double *, int, double, double, double, double) (256, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.79
    SM Frequency                    Ghz         1.60
    Elapsed Cycles                cycle      355,131
    Memory Throughput                 %         3.63
    DRAM Throughput                   %         3.63
    Duration                         us       221.44
    L1/TEX Cache Throughput           %         4.52
    L2 Cache Throughput               %         3.04
    SM Active Cycles              cycle   308,181.50
    Compute (SM) Throughput           %        75.61
    ----------------------- ----------- ------------

    OPT   Compute is more heavily utilized than Memory: Look at the Compute Workload Analysis section to see what the   
          compute pipelines are spending their time doing. Also, consider whether any computation is redundant and      
          could be reduced or moved to look-up tables.                                                                  

    Section: GPU Speed Of Light Roofline Chart
    OPT   Est. Speedup: 82.32%                                                                                          
          The ratio of peak float (FP32) to double (FP64) performance on this device is 32:1. The workload achieved     
          close to 0% of this device's FP32 peak performance and 56% of its FP64 peak performance. If Compute Workload  
          Analysis determines that this workload is FP64 bound, consider using 32-bit precision floating point          
          operations to improve its performance. See the Profiling Guide                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Kbyte       393.22
    Dropped Samples                sample            0
    Maximum Sampling Interval       cycle       20,000
    # Pass Groups                                    1
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.11
    Executed Ipc Elapsed  inst/cycle         0.09
    Issue Slots Busy               %         2.65
    Issued Ipc Active     inst/cycle         0.11
    SM Busy                        %        86.99
    -------------------- ----------- ------------

    OPT   FP64 is the highest-utilized pipeline (87.0%) based on active cycles, taking into account the rates of its    
          different instructions. It executes 64-bit floating point operations. The pipeline is over-utilized and       
          likely a performance bottleneck. Based on the number of executed instructions, the highest utilized pipeline  
          (87.0%) is FP64. It executes 64-bit floating point operations. Comparing the two, the overall pipeline        
          utilization appears to be caused by frequent, low-latency instructions. See the Profiling Guide               
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-decoder) or hover over the          
          pipeline name to understand the workloads handled by each pipeline. The Instruction Statistics section shows  
          the mix of executed instructions for this workload. Check the Warp State Statistics section for which         
          reasons cause warps to stall.                                                                                 

    Section: Memory Workload Analysis
    ----------------- ----------- ------------
    Metric Name       Metric Unit Metric Value
    ----------------- ----------- ------------
    Memory Throughput     Gbyte/s        15.80
    Mem Busy                    %         3.00
    Max Bandwidth               %         3.63
    L1/TEX Hit Rate             %        79.19
    L2 Hit Rate                 %        80.68
    Mem Pipes Busy              %        19.76
    ----------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 3.238%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 9.1 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 3.393%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32     
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %         2.65
    Issued Warp Per Scheduler                        0.03
    No Eligible                            %        97.35
    Active Warps Per Scheduler          warp         6.63
    Eligible Warps Per Scheduler        warp         0.07
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 24.39%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 37.7 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 8 warps per scheduler, this workload allocates an average of  
          6.63 active warps per scheduler, but only an average of 0.07 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle       249.86
    Warp Cycles Per Executed Instruction           cycle       251.09
    Avg. Active Threads Per Warp                                31.99
    Avg. Not Predicated Off Threads Per Warp                    30.73
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 24.39%                                                                                          
          On average, each warp of this workload spends 121.2 cycles being stalled waiting for a scoreboard dependency  
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 48.5% of the total average of 249.9 cycles       
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 24.39%                                                                                          
          On average, each warp of this workload spends 120.9 cycles being stalled waiting for the L1 instruction queue 
          for texture operations to be not full. This stall reason is high in cases of extreme utilization of the       
          L1TEX pipeline. Try issuing fewer texture fetches, surface loads, surface stores, or decoupled math           
          operations. If applicable, consider combining multiple lower-width memory operations into fewer wider memory  
          operations and try interleaving memory operations and math instructions. Consider converting texture lookups  
          or surface loads into global memory lookups. Texture can accept four threads' requests per cycle, whereas     
          global accepts 32 threads. This stall type represents about 48.4% of the total average of 249.9 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Profiling Guide                                                                            
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst     8,125.43
    Executed Instructions                           inst    1,300,068
    Avg. Issued Instructions Per Scheduler          inst     8,165.41
    Issued Instructions                             inst    1,306,465
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 16.53%                                                                                          
          This kernel executes 388846 fused and 238388 non-fused FP64 instructions. By converting pairs of non-fused    
          instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point),           
          higher-throughput equivalent, the achieved FP64 performance could be increased by up to 19% (relative to its  
          current performance).                                                                                         

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    256
    Registers Per Thread             register/thread              56
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              40
    Stack Size                                                 1,024
    Threads                                   thread          65,536
    # TPCs                                                        20
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                1.60
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 97 thread blocks.   
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 50.0% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        82.75
    Achieved Active Warps Per SM           warp        26.48
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 17.25%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (82.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    54,653.50
    Total DRAM Elapsed Cycles        cycle   12,029,952
    Average L1 Active Cycles         cycle   308,181.50
    Total L1 Elapsed Cycles          cycle   14,183,450
    Average L2 Active Cycles         cycle    41,492.75
    Total L2 Elapsed Cycles          cycle   10,836,224
    Average SM Active Cycles         cycle   308,181.50
    Total SM Elapsed Cycles          cycle   14,183,450
    Average SMSP Active Cycles       cycle   307,880.64
    Total SMSP Elapsed Cycles        cycle   56,733,800
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 11.04%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 12.70% above the average, while the minimum instance value is 1.88% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.16%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 12.86% above the average, while the minimum instance value is 1.87% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.04%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 12.70% above the average, while the minimum instance value is 1.88% below the       
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.07
    Branch Instructions              inst       88,024
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 8.444%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 421875 excessive sectors (69% of the      
          total 612171 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source         
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  cuda_aux_computation(const double *, const double *, double *, double, int, double) (88, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.70
    SM Frequency                    Ghz         1.57
    Elapsed Cycles                cycle       25,796
    Memory Throughput                 %        88.84
    DRAM Throughput                   %        88.84
    Duration                         us        16.38
    L1/TEX Cache Throughput           %        18.24
    L2 Cache Throughput               %        23.90
    SM Active Cycles              cycle    17,905.35
    Compute (SM) Throughput           %        15.82
    ----------------------- ----------- ------------

    INF   This workload is utilizing greater than 80.0% of the available compute or memory performance of the device.   
          To further improve performance, work will likely need to be shifted from the most utilized to another unit.   
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: GPU Speed Of Light Roofline Chart
    OPT   Est. Speedup: 18.44%                                                                                          
          The ratio of peak float (FP32) to double (FP64) performance on this device is 32:1. The workload achieved 0%  
          of this device's FP32 peak performance and 15% of its FP64 peak performance. If Compute Workload Analysis     
          determines that this workload is FP64 bound, consider using 32-bit precision floating point operations to     
          improve its performance. See the Profiling Guide                                                              
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Kbyte       393.22
    Dropped Samples                sample            0
    Maximum Sampling Interval       cycle       20,000
    # Pass Groups                                    1
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.14
    Executed Ipc Elapsed  inst/cycle         0.12
    Issue Slots Busy               %         3.65
    Issued Ipc Active     inst/cycle         0.15
    SM Busy                        %        19.03
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 80.97%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ----------------- ----------- ------------
    Metric Name       Metric Unit Metric Value
    ----------------- ----------- ------------
    Memory Throughput     Gbyte/s       323.81
    Mem Busy                    %        23.90
    Max Bandwidth               %        88.84
    L1/TEX Hit Rate             %            0
    L2 Hit Rate                 %        33.95
    Mem Pipes Busy              %         7.42
    ----------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %         3.62
    Issued Warp Per Scheduler                        0.04
    No Eligible                            %        96.38
    Active Warps Per Scheduler          warp         4.30
    Eligible Warps Per Scheduler        warp         0.04
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 11.16%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 27.6 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 8 warps per scheduler, this workload allocates an average of  
          4.30 active warps per scheduler, but only an average of 0.04 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp  
          State Statistics and Source Counters sections.                                                                

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle       118.67
    Warp Cycles Per Executed Instruction           cycle       121.85
    Avg. Active Threads Per Warp                                31.99
    Avg. Not Predicated Off Threads Per Warp                    30.04
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 11.16%                                                                                          
          On average, each warp of this workload spends 107.0 cycles being stalled waiting for a scoreboard dependency  
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 90.2% of the total average of 118.7 cycles       
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Profiling Guide                                                                            
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       636.09
    Executed Instructions                           inst      101,775
    Avg. Issued Instructions Per Scheduler          inst       653.13
    Issued Instructions                             inst      104,501
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     88
    Registers Per Thread             register/thread              28
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              40
    Stack Size                                                 1,024
    Threads                                   thread          22,528
    # TPCs                                                        20
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                0.55
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        53.36
    Achieved Active Warps Per SM           warp        17.08
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 11.16%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (53.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       82,895
    Total DRAM Elapsed Cycles        cycle      746,496
    Average L1 Active Cycles         cycle    17,905.35
    Total L1 Elapsed Cycles          cycle      861,838
    Average L2 Active Cycles         cycle    17,117.91
    Total L2 Elapsed Cycles          cycle      792,544
    Average SM Active Cycles         cycle    17,905.35
    Total SM Elapsed Cycles          cycle      861,838
    Average SMSP Active Cycles       cycle    18,018.16
    Total SMSP Elapsed Cycles        cycle    3,447,352
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 8.513%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 10.24% above the average, while the minimum instance value is 4.10% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.836%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 10.57% above the average, while the minimum instance value is 5.28% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.513%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 10.24% above the average, while the minimum instance value is 4.10% below the       
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.07
    Branch Instructions              inst        6,907
    Branch Efficiency                   %        99.95
    Avg. Divergent Branches                       0.01
    ------------------------- ----------- ------------

  cuda_right_hand_side(const double *, double *, int, double, double, double, double) (256, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.74
    SM Frequency                    Ghz         1.59
    Elapsed Cycles                cycle      355,749
    Memory Throughput                 %         3.63
    DRAM Throughput                   %         3.63
    Duration                         us       222.85
    L1/TEX Cache Throughput           %         4.52
    L2 Cache Throughput               %         3.06
    SM Active Cycles              cycle   308,209.85
    Compute (SM) Throughput           %        75.60
    ----------------------- ----------- ------------

    OPT   Compute is more heavily utilized than Memory: Look at the Compute Workload Analysis section to see what the   
          compute pipelines are spending their time doing. Also, consider whether any computation is redundant and      
          could be reduced or moved to look-up tables.                                                                  

    Section: GPU Speed Of Light Roofline Chart
    OPT   Est. Speedup: 82.31%                                                                                          
          The ratio of peak float (FP32) to double (FP64) performance on this device is 32:1. The workload achieved     
          close to 0% of this device's FP32 peak performance and 56% of its FP64 peak performance. If Compute Workload  
          Analysis determines that this workload is FP64 bound, consider using 32-bit precision floating point          
          operations to improve its performance. See the Profiling Guide                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Kbyte       393.22
    Dropped Samples                sample            0
    Maximum Sampling Interval       cycle       20,000
    # Pass Groups                                    1
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.11
    Executed Ipc Elapsed  inst/cycle         0.09
    Issue Slots Busy               %         2.65
    Issued Ipc Active     inst/cycle         0.11
    SM Busy                        %        86.98
    -------------------- ----------- ------------

    OPT   FP64 is the highest-utilized pipeline (87.0%) based on active cycles, taking into account the rates of its    
          different instructions. It executes 64-bit floating point operations. The pipeline is over-utilized and       
          likely a performance bottleneck. Based on the number of executed instructions, the highest utilized pipeline  
          (87.0%) is FP64. It executes 64-bit floating point operations. Comparing the two, the overall pipeline        
          utilization appears to be caused by frequent, low-latency instructions. See the Profiling Guide               
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-decoder) or hover over the          
          pipeline name to understand the workloads handled by each pipeline. The Instruction Statistics section shows  
          the mix of executed instructions for this workload. Check the Warp State Statistics section for which         
          reasons cause warps to stall.                                                                                 

    Section: Memory Workload Analysis
    ----------------- ----------- ------------
    Metric Name       Metric Unit Metric Value
    ----------------- ----------- ------------
    Memory Throughput     Gbyte/s        15.67
    Mem Busy                    %         2.99
    Max Bandwidth               %         3.63
    L1/TEX Hit Rate             %        79.30
    L2 Hit Rate                 %        80.27
    Mem Pipes Busy              %        19.76
    ----------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 3.237%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 9.1 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 3.392%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32     
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %         2.65
    Issued Warp Per Scheduler                        0.03
    No Eligible                            %        97.35
    Active Warps Per Scheduler          warp         6.62
    Eligible Warps Per Scheduler        warp         0.07
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 24.4%                                                                                     
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 37.7 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 8 warps per scheduler, this workload allocates an average of  
          6.62 active warps per scheduler, but only an average of 0.07 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle       249.80
    Warp Cycles Per Executed Instruction           cycle       251.03
    Avg. Active Threads Per Warp                                31.99
    Avg. Not Predicated Off Threads Per Warp                    30.73
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 24.4%                                                                                           
          On average, each warp of this workload spends 121.1 cycles being stalled waiting for a scoreboard dependency  
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 48.5% of the total average of 249.8 cycles       
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 24.4%                                                                                           
          On average, each warp of this workload spends 121.0 cycles being stalled waiting for the L1 instruction queue 
          for texture operations to be not full. This stall reason is high in cases of extreme utilization of the       
          L1TEX pipeline. Try issuing fewer texture fetches, surface loads, surface stores, or decoupled math           
          operations. If applicable, consider combining multiple lower-width memory operations into fewer wider memory  
          operations and try interleaving memory operations and math instructions. Consider converting texture lookups  
          or surface loads into global memory lookups. Texture can accept four threads' requests per cycle, whereas     
          global accepts 32 threads. This stall type represents about 48.4% of the total average of 249.8 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Profiling Guide                                                                            
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst     8,125.43
    Executed Instructions                           inst    1,300,068
    Avg. Issued Instructions Per Scheduler          inst     8,165.52
    Issued Instructions                             inst    1,306,484
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 16.53%                                                                                          
          This kernel executes 388846 fused and 238388 non-fused FP64 instructions. By converting pairs of non-fused    
          instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point),           
          higher-throughput equivalent, the achieved FP64 performance could be increased by up to 19% (relative to its  
          current performance).                                                                                         

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    256
    Registers Per Thread             register/thread              56
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              40
    Stack Size                                                 1,024
    Threads                                   thread          65,536
    # TPCs                                                        20
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                1.60
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 97 thread blocks.   
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 50.0% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        82.77
    Achieved Active Warps Per SM           warp        26.49
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 17.23%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (82.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    54,549.50
    Total DRAM Elapsed Cycles        cycle   12,021,760
    Average L1 Active Cycles         cycle   308,209.85
    Total L1 Elapsed Cycles          cycle   14,183,980
    Average L2 Active Cycles         cycle    41,618.34
    Total L2 Elapsed Cycles          cycle   10,891,360
    Average SM Active Cycles         cycle   308,209.85
    Total SM Elapsed Cycles          cycle   14,183,980
    Average SMSP Active Cycles       cycle   307,953.03
    Total SMSP Elapsed Cycles        cycle   56,735,920
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 11.09%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 12.76% above the average, while the minimum instance value is 1.84% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.18%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 12.88% above the average, while the minimum instance value is 1.97% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.09%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 12.76% above the average, while the minimum instance value is 1.84% below the       
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.07
    Branch Instructions              inst       88,024
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 8.427%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 421875 excessive sectors (69% of the      
          total 612171 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source         
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  cuda_aux_computation(const double *, const double *, double *, double, int, double) (88, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.81
    SM Frequency                    Ghz         1.57
    Elapsed Cycles                cycle       25,852
    Memory Throughput                 %        86.89
    DRAM Throughput                   %        86.89
    Duration                         us        16.38
    L1/TEX Cache Throughput           %        18.01
    L2 Cache Throughput               %        23.92
    SM Active Cycles              cycle    17,949.30
    Compute (SM) Throughput           %        15.61
    ----------------------- ----------- ------------

    INF   This workload is utilizing greater than 80.0% of the available compute or memory performance of the device.   
          To further improve performance, work will likely need to be shifted from the most utilized to another unit.   
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (FP32) to double (FP64) performance on this device is 32:1. The workload achieved 0%  
          of this device's FP32 peak performance and 15% of its FP64 peak performance. See the Profiling Guide          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Kbyte       393.22
    Dropped Samples                sample            0
    Maximum Sampling Interval       cycle       20,000
    # Pass Groups                                    1
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.14
    Executed Ipc Elapsed  inst/cycle         0.12
    Issue Slots Busy               %         3.64
    Issued Ipc Active     inst/cycle         0.15
    SM Busy                        %        18.99
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 81.01%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ----------------- ----------- ------------
    Metric Name       Metric Unit Metric Value
    ----------------- ----------- ------------
    Memory Throughput     Gbyte/s       323.25
    Mem Busy                    %        23.92
    Max Bandwidth               %        86.89
    L1/TEX Hit Rate             %            0
    L2 Hit Rate                 %        34.00
    Mem Pipes Busy              %         7.32
    ----------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %         3.65
    Issued Warp Per Scheduler                        0.04
    No Eligible                            %        96.35
    Active Warps Per Scheduler          warp         4.32
    Eligible Warps Per Scheduler        warp         0.05
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 13.11%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 27.4 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 8 warps per scheduler, this workload allocates an average of  
          4.32 active warps per scheduler, but only an average of 0.05 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp  
          State Statistics and Source Counters sections.                                                                

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle       118.43
    Warp Cycles Per Executed Instruction           cycle       121.60
    Avg. Active Threads Per Warp                                31.99
    Avg. Not Predicated Off Threads Per Warp                    30.04
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 13.11%                                                                                          
          On average, each warp of this workload spends 106.5 cycles being stalled waiting for a scoreboard dependency  
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 90.0% of the total average of 118.4 cycles       
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Profiling Guide                                                                            
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       636.09
    Executed Instructions                           inst      101,775
    Avg. Issued Instructions Per Scheduler          inst       653.11
    Issued Instructions                             inst      104,497
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     88
    Registers Per Thread             register/thread              28
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              40
    Stack Size                                                 1,024
    Threads                                   thread          22,528
    # TPCs                                                        20
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                0.55
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        52.96
    Achieved Active Warps Per SM           warp        16.95
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 13.11%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (53.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       82,751
    Total DRAM Elapsed Cycles        cycle      761,856
    Average L1 Active Cycles         cycle    17,949.30
    Total L1 Elapsed Cycles          cycle      873,360
    Average L2 Active Cycles         cycle    17,319.09
    Total L2 Elapsed Cycles          cycle      792,672
    Average SM Active Cycles         cycle    17,949.30
    Total SM Elapsed Cycles          cycle      873,360
    Average SMSP Active Cycles       cycle    17,909.26
    Total SMSP Elapsed Cycles        cycle    3,493,440
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 7.842%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 9.54% above the average, while the minimum instance value is 6.53% below the average.       
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 9.99%                                                                                           
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 12.18% above the average, while the minimum instance value is 4.60% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.842%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 9.54% above the average, while the minimum instance value is 6.53% below the        
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.07
    Branch Instructions              inst        6,907
    Branch Efficiency                   %        99.95
    Avg. Divergent Branches                       0.01
    ------------------------- ----------- ------------

  cuda_right_hand_side(const double *, double *, int, double, double, double, double) (256, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.82
    SM Frequency                    Ghz         1.60
    Elapsed Cycles                cycle      355,353
    Memory Throughput                 %         3.61
    DRAM Throughput                   %         3.61
    Duration                         us       221.66
    L1/TEX Cache Throughput           %         4.52
    L2 Cache Throughput               %         3.01
    SM Active Cycles              cycle   308,310.20
    Compute (SM) Throughput           %        75.51
    ----------------------- ----------- ------------

    OPT   Compute is more heavily utilized than Memory: Look at the Compute Workload Analysis section to see what the   
          compute pipelines are spending their time doing. Also, consider whether any computation is redundant and      
          could be reduced or moved to look-up tables.                                                                  

    Section: GPU Speed Of Light Roofline Chart
    OPT   Est. Speedup: 82.29%                                                                                          
          The ratio of peak float (FP32) to double (FP64) performance on this device is 32:1. The workload achieved     
          close to 0% of this device's FP32 peak performance and 56% of its FP64 peak performance. If Compute Workload  
          Analysis determines that this workload is FP64 bound, consider using 32-bit precision floating point          
          operations to improve its performance. See the Profiling Guide                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Kbyte       393.22
    Dropped Samples                sample            0
    Maximum Sampling Interval       cycle       20,000
    # Pass Groups                                    1
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.11
    Executed Ipc Elapsed  inst/cycle         0.09
    Issue Slots Busy               %         2.65
    Issued Ipc Active     inst/cycle         0.11
    SM Busy                        %        86.95
    -------------------- ----------- ------------

    OPT   FP64 is the highest-utilized pipeline (87.0%) based on active cycles, taking into account the rates of its    
          different instructions. It executes 64-bit floating point operations. The pipeline is over-utilized and       
          likely a performance bottleneck. Based on the number of executed instructions, the highest utilized pipeline  
          (87.0%) is FP64. It executes 64-bit floating point operations. Comparing the two, the overall pipeline        
          utilization appears to be caused by frequent, low-latency instructions. See the Profiling Guide               
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-decoder) or hover over the          
          pipeline name to understand the workloads handled by each pipeline. The Instruction Statistics section shows  
          the mix of executed instructions for this workload. Check the Warp State Statistics section for which         
          reasons cause warps to stall.                                                                                 

    Section: Memory Workload Analysis
    ----------------- ----------- ------------
    Metric Name       Metric Unit Metric Value
    ----------------- ----------- ------------
    Memory Throughput     Gbyte/s        15.75
    Mem Busy                    %         3.00
    Max Bandwidth               %         3.61
    L1/TEX Hit Rate             %        79.32
    L2 Hit Rate                 %        80.08
    Mem Pipes Busy              %        19.73
    ----------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 3.233%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 9.1 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 3.388%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32     
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %         2.65
    Issued Warp Per Scheduler                        0.03
    No Eligible                            %        97.35
    Active Warps Per Scheduler          warp         6.63
    Eligible Warps Per Scheduler        warp         0.07
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 24.49%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 37.7 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 8 warps per scheduler, this workload allocates an average of  
          6.63 active warps per scheduler, but only an average of 0.07 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle       249.92
    Warp Cycles Per Executed Instruction           cycle       251.15
    Avg. Active Threads Per Warp                                31.99
    Avg. Not Predicated Off Threads Per Warp                    30.66
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 24.49%                                                                                          
          On average, each warp of this workload spends 121.1 cycles being stalled waiting for a scoreboard dependency  
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 48.5% of the total average of 249.9 cycles       
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 24.49%                                                                                          
          On average, each warp of this workload spends 121.0 cycles being stalled waiting for the L1 instruction queue 
          for texture operations to be not full. This stall reason is high in cases of extreme utilization of the       
          L1TEX pipeline. Try issuing fewer texture fetches, surface loads, surface stores, or decoupled math           
          operations. If applicable, consider combining multiple lower-width memory operations into fewer wider memory  
          operations and try interleaving memory operations and math instructions. Consider converting texture lookups  
          or surface loads into global memory lookups. Texture can accept four threads' requests per cycle, whereas     
          global accepts 32 threads. This stall type represents about 48.4% of the total average of 249.9 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Profiling Guide                                                                            
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst     8,125.43
    Executed Instructions                           inst    1,300,068
    Avg. Issued Instructions Per Scheduler          inst     8,165.45
    Issued Instructions                             inst    1,306,472
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 16.52%                                                                                          
          This kernel executes 388846 fused and 238388 non-fused FP64 instructions. By converting pairs of non-fused    
          instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point),           
          higher-throughput equivalent, the achieved FP64 performance could be increased by up to 19% (relative to its  
          current performance).                                                                                         

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    256
    Registers Per Thread             register/thread              56
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              40
    Stack Size                                                 1,024
    Threads                                   thread          65,536
    # TPCs                                                        20
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                1.60
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 97 thread blocks.   
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 50.0% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        82.74
    Achieved Active Warps Per SM           warp        26.48
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 17.26%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (82.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    54,566.50
    Total DRAM Elapsed Cycles        cycle   12,099,584
    Average L1 Active Cycles         cycle   308,310.20
    Total L1 Elapsed Cycles          cycle   14,200,962
    Average L2 Active Cycles         cycle    41,594.41
    Total L2 Elapsed Cycles          cycle   10,845,760
    Average SM Active Cycles         cycle   308,310.20
    Total SM Elapsed Cycles          cycle   14,200,962
    Average SMSP Active Cycles       cycle   307,913.08
    Total SMSP Elapsed Cycles        cycle   56,803,848
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 11.09%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 12.77% above the average, while the minimum instance value is 1.92% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.2%                                                                                           
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 12.91% above the average, while the minimum instance value is 1.86% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.09%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 12.77% above the average, while the minimum instance value is 1.92% below the       
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.07
    Branch Instructions              inst       88,024
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 8.457%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 421875 excessive sectors (69% of the      
          total 612171 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source         
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  cuda_final_update(double *, const double *, const double *, const double *, const double *, double, int) (88, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Ghz         1.57
    Elapsed Cycles                cycle       54,661
    Memory Throughput                 %        89.25
    DRAM Throughput                   %        89.25
    Duration                         us        34.56
    L1/TEX Cache Throughput           %        17.27
    L2 Cache Throughput               %        22.49
    SM Active Cycles              cycle    45,250.12
    Compute (SM) Throughput           %        25.51
    ----------------------- ----------- ------------

    INF   This workload is utilizing greater than 80.0% of the available compute or memory performance of the device.   
          To further improve performance, work will likely need to be shifted from the most utilized to another unit.   
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: GPU Speed Of Light Roofline Chart
    OPT   Est. Speedup: 27.38%                                                                                          
          The ratio of peak float (FP32) to double (FP64) performance on this device is 32:1. The workload achieved 0%  
          of this device's FP32 peak performance and 22% of its FP64 peak performance. If Compute Workload Analysis     
          determines that this workload is FP64 bound, consider using 32-bit precision floating point operations to     
          improve its performance. See the Profiling Guide                                                              
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Kbyte       393.22
    Dropped Samples                sample            0
    Maximum Sampling Interval       cycle       20,000
    # Pass Groups                                    1
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.09
    Executed Ipc Elapsed  inst/cycle         0.08
    Issue Slots Busy               %         2.33
    Issued Ipc Active     inst/cycle         0.09
    SM Busy                        %        28.26
    -------------------- ----------- ------------

    INF   FP64 is the highest-utilized pipeline (28.3%) based on active cycles, taking into account the rates of its    
          different instructions. It executes 64-bit floating point operations. It is well-utilized, but should not be  
          a bottleneck.                                                                                                 

    Section: Memory Workload Analysis
    ----------------- ----------- ------------
    Metric Name       Metric Unit Metric Value
    ----------------- ----------- ------------
    Memory Throughput     Gbyte/s       356.47
    Mem Busy                    %        22.49
    Max Bandwidth               %        89.25
    L1/TEX Hit Rate             %        14.11
    L2 Hit Rate                 %        17.07
    Mem Pipes Busy              %         7.87
    ----------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %         2.34
    Issued Warp Per Scheduler                        0.02
    No Eligible                            %        97.66
    Active Warps Per Scheduler          warp         4.33
    Eligible Warps Per Scheduler        warp         0.03
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 10.75%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 42.7 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 8 warps per scheduler, this workload allocates an average of  
          4.33 active warps per scheduler, but only an average of 0.03 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp  
          State Statistics and Source Counters sections.                                                                

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle       184.81
    Warp Cycles Per Executed Instruction           cycle       188.15
    Avg. Active Threads Per Warp                                31.99
    Avg. Not Predicated Off Threads Per Warp                    30.79
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 10.75%                                                                                          
          On average, each warp of this workload spends 174.2 cycles being stalled waiting for a scoreboard dependency  
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 94.3% of the total average of 184.8 cycles       
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Profiling Guide                                                                            
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst     1,034.84
    Executed Instructions                           inst      165,575
    Avg. Issued Instructions Per Scheduler          inst     1,053.54
    Issued Instructions                             inst      168,567
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 3.766%                                                                                          
          This kernel executes 23448 fused and 8520 non-fused FP64 instructions. By converting pairs of non-fused       
          instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point),           
          higher-throughput equivalent, the achieved FP64 performance could be increased by up to 13% (relative to its  
          current performance).                                                                                         

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     88
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              40
    Stack Size                                                 1,024
    Threads                                   thread          22,528
    # TPCs                                                        20
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                0.55
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        53.52
    Achieved Active Warps Per SM           warp        17.13
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 10.75%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (53.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   192,495.50
    Total DRAM Elapsed Cycles        cycle    1,725,440
    Average L1 Active Cycles         cycle    45,250.12
    Total L1 Elapsed Cycles          cycle    2,004,696
    Average L2 Active Cycles         cycle    43,112.31
    Total L2 Elapsed Cycles          cycle    1,677,152
    Average SM Active Cycles         cycle    45,250.12
    Total SM Elapsed Cycles          cycle    2,004,696
    Average SMSP Active Cycles       cycle    45,008.57
    Total SMSP Elapsed Cycles        cycle    8,018,784
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 6.005%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 6.69% above the average, while the minimum instance value is 2.90% below the        
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.04
    Branch Instructions              inst        6,907
    Branch Efficiency                   %        99.95
    Avg. Divergent Branches                       0.01
    ------------------------- ----------- ------------

  cuda_right_hand_side(const double *, double *, int, double, double, double, double) (256, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.84
    SM Frequency                    Ghz         1.60
    Elapsed Cycles                cycle      355,210
    Memory Throughput                 %         3.60
    DRAM Throughput                   %         3.60
    Duration                         us       221.47
    L1/TEX Cache Throughput           %         4.53
    L2 Cache Throughput               %         3.04
    SM Active Cycles              cycle   308,217.30
    Compute (SM) Throughput           %        75.67
    ----------------------- ----------- ------------

    OPT   Compute is more heavily utilized than Memory: Look at the Compute Workload Analysis section to see what the   
          compute pipelines are spending their time doing. Also, consider whether any computation is redundant and      
          could be reduced or moved to look-up tables.                                                                  

    Section: GPU Speed Of Light Roofline Chart
    OPT   Est. Speedup: 82.31%                                                                                          
          The ratio of peak float (FP32) to double (FP64) performance on this device is 32:1. The workload achieved     
          close to 0% of this device's FP32 peak performance and 56% of its FP64 peak performance. If Compute Workload  
          Analysis determines that this workload is FP64 bound, consider using 32-bit precision floating point          
          operations to improve its performance. See the Profiling Guide                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Kbyte       393.22
    Dropped Samples                sample            0
    Maximum Sampling Interval       cycle       20,000
    # Pass Groups                                    1
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.11
    Executed Ipc Elapsed  inst/cycle         0.09
    Issue Slots Busy               %         2.65
    Issued Ipc Active     inst/cycle         0.11
    SM Busy                        %        86.98
    -------------------- ----------- ------------

    OPT   FP64 is the highest-utilized pipeline (87.0%) based on active cycles, taking into account the rates of its    
          different instructions. It executes 64-bit floating point operations. The pipeline is over-utilized and       
          likely a performance bottleneck. Based on the number of executed instructions, the highest utilized pipeline  
          (87.0%) is FP64. It executes 64-bit floating point operations. Comparing the two, the overall pipeline        
          utilization appears to be caused by frequent, low-latency instructions. See the Profiling Guide               
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-decoder) or hover over the          
          pipeline name to understand the workloads handled by each pipeline. The Instruction Statistics section shows  
          the mix of executed instructions for this workload. Check the Warp State Statistics section for which         
          reasons cause warps to stall.                                                                                 

    Section: Memory Workload Analysis
    ----------------- ----------- ------------
    Metric Name       Metric Unit Metric Value
    ----------------- ----------- ------------
    Memory Throughput     Gbyte/s        15.76
    Mem Busy                    %         3.00
    Max Bandwidth               %         3.60
    L1/TEX Hit Rate             %        79.25
    L2 Hit Rate                 %        80.53
    Mem Pipes Busy              %        19.77
    ----------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 3.241%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 9.1 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 3.396%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32     
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %         2.65
    Issued Warp Per Scheduler                        0.03
    No Eligible                            %        97.35
    Active Warps Per Scheduler          warp         6.63
    Eligible Warps Per Scheduler        warp         0.07
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 24.33%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 37.7 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 8 warps per scheduler, this workload allocates an average of  
          6.63 active warps per scheduler, but only an average of 0.07 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle       249.92
    Warp Cycles Per Executed Instruction           cycle       251.15
    Avg. Active Threads Per Warp                                31.99
    Avg. Not Predicated Off Threads Per Warp                    30.66
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 24.33%                                                                                          
          On average, each warp of this workload spends 121.1 cycles being stalled waiting for a scoreboard dependency  
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 48.5% of the total average of 249.9 cycles       
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 24.33%                                                                                          
          On average, each warp of this workload spends 120.9 cycles being stalled waiting for the L1 instruction queue 
          for texture operations to be not full. This stall reason is high in cases of extreme utilization of the       
          L1TEX pipeline. Try issuing fewer texture fetches, surface loads, surface stores, or decoupled math           
          operations. If applicable, consider combining multiple lower-width memory operations into fewer wider memory  
          operations and try interleaving memory operations and math instructions. Consider converting texture lookups  
          or surface loads into global memory lookups. Texture can accept four threads' requests per cycle, whereas     
          global accepts 32 threads. This stall type represents about 48.4% of the total average of 249.9 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Profiling Guide                                                                            
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst     8,125.43
    Executed Instructions                           inst    1,300,068
    Avg. Issued Instructions Per Scheduler          inst     8,165.43
    Issued Instructions                             inst    1,306,468
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 16.53%                                                                                          
          This kernel executes 388846 fused and 238388 non-fused FP64 instructions. By converting pairs of non-fused    
          instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point),           
          higher-throughput equivalent, the achieved FP64 performance could be increased by up to 19% (relative to its  
          current performance).                                                                                         

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    256
    Registers Per Thread             register/thread              56
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              40
    Stack Size                                                 1,024
    Threads                                   thread          65,536
    # TPCs                                                        20
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                1.60
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 97 thread blocks.   
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 50.0% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        82.76
    Achieved Active Warps Per SM           warp        26.48
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 17.24%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (82.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    54,544.50
    Total DRAM Elapsed Cycles        cycle   12,115,968
    Average L1 Active Cycles         cycle   308,217.30
    Total L1 Elapsed Cycles          cycle   14,172,404
    Average L2 Active Cycles         cycle    42,189.16
    Total L2 Elapsed Cycles          cycle   10,837,728
    Average SM Active Cycles         cycle   308,217.30
    Total SM Elapsed Cycles          cycle   14,172,404
    Average SMSP Active Cycles       cycle   307,893.26
    Total SMSP Elapsed Cycles        cycle   56,689,616
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 11.1%                                                                                           
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 12.76% above the average, while the minimum instance value is 1.97% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.12%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 12.79% above the average, while the minimum instance value is 1.86% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.1%                                                                                           
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 12.76% above the average, while the minimum instance value is 1.97% below the       
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.07
    Branch Instructions              inst       88,024
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 8.585%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 421875 excessive sectors (69% of the      
          total 612171 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source         
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  cuda_aux_computation(const double *, const double *, double *, double, int, double) (88, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.88
    SM Frequency                    Ghz         1.57
    Elapsed Cycles                cycle       25,642
    Memory Throughput                 %        85.28
    DRAM Throughput                   %        85.28
    Duration                         us        16.29
    L1/TEX Cache Throughput           %        18.05
    L2 Cache Throughput               %        24.08
    SM Active Cycles              cycle    18,044.80
    Compute (SM) Throughput           %        15.66
    ----------------------- ----------- ------------

    INF   This workload is utilizing greater than 80.0% of the available compute or memory performance of the device.   
          To further improve performance, work will likely need to be shifted from the most utilized to another unit.   
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: GPU Speed Of Light Roofline Chart
    OPT   Est. Speedup: 18.3%                                                                                           
          The ratio of peak float (FP32) to double (FP64) performance on this device is 32:1. The workload achieved 0%  
          of this device's FP32 peak performance and 15% of its FP64 peak performance. If Compute Workload Analysis     
          determines that this workload is FP64 bound, consider using 32-bit precision floating point operations to     
          improve its performance. See the Profiling Guide                                                              
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Kbyte       393.22
    Dropped Samples                sample            0
    Maximum Sampling Interval       cycle       20,000
    # Pass Groups                                    1
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.14
    Executed Ipc Elapsed  inst/cycle         0.12
    Issue Slots Busy               %         3.62
    Issued Ipc Active     inst/cycle         0.14
    SM Busy                        %        18.89
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 81.11%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ----------------- ----------- ------------
    Metric Name       Metric Unit Metric Value
    ----------------- ----------- ------------
    Memory Throughput     Gbyte/s       320.82
    Mem Busy                    %        24.08
    Max Bandwidth               %        85.28
    L1/TEX Hit Rate             %            0
    L2 Hit Rate                 %        33.94
    Mem Pipes Busy              %         7.34
    ----------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %         3.66
    Issued Warp Per Scheduler                        0.04
    No Eligible                            %        96.34
    Active Warps Per Scheduler          warp         4.30
    Eligible Warps Per Scheduler        warp         0.05
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 14.72%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 27.3 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 8 warps per scheduler, this workload allocates an average of  
          4.30 active warps per scheduler, but only an average of 0.05 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp  
          State Statistics and Source Counters sections.                                                                

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle       117.39
    Warp Cycles Per Executed Instruction           cycle       120.53
    Avg. Active Threads Per Warp                                31.99
    Avg. Not Predicated Off Threads Per Warp                    30.04
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 14.72%                                                                                          
          On average, each warp of this workload spends 107.2 cycles being stalled waiting for a scoreboard dependency  
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 91.4% of the total average of 117.4 cycles       
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Profiling Guide                                                                            
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       636.09
    Executed Instructions                           inst      101,775
    Avg. Issued Instructions Per Scheduler          inst       653.09
    Issued Instructions                             inst      104,495
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     88
    Registers Per Thread             register/thread              28
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              40
    Stack Size                                                 1,024
    Threads                                   thread          22,528
    # TPCs                                                        20
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                0.55
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        52.99
    Achieved Active Warps Per SM           warp        16.96
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 14.72%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (53.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    81,649.50
    Total DRAM Elapsed Cycles        cycle      765,952
    Average L1 Active Cycles         cycle    18,044.80
    Total L1 Elapsed Cycles          cycle      870,500
    Average L2 Active Cycles         cycle    17,008.38
    Total L2 Elapsed Cycles          cycle      787,264
    Average SM Active Cycles         cycle    18,044.80
    Total SM Elapsed Cycles          cycle      870,500
    Average SMSP Active Cycles       cycle    17,834.38
    Total SMSP Elapsed Cycles        cycle    3,482,000
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 10.43%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 12.58% above the average, while the minimum instance value is 5.12% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 9.012%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 11.00% above the average, while the minimum instance value is 4.53% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 10.43%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 12.58% above the average, while the minimum instance value is 5.12% below the       
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.07
    Branch Instructions              inst        6,907
    Branch Efficiency                   %        99.95
    Avg. Divergent Branches                       0.01
    ------------------------- ----------- ------------

  cuda_right_hand_side(const double *, double *, int, double, double, double, double) (256, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.77
    SM Frequency                    Ghz         1.60
    Elapsed Cycles                cycle      356,075
    Memory Throughput                 %         3.61
    DRAM Throughput                   %         3.61
    Duration                         us       222.05
    L1/TEX Cache Throughput           %         4.52
    L2 Cache Throughput               %         3.03
    SM Active Cycles              cycle      308,269
    Compute (SM) Throughput           %        75.60
    ----------------------- ----------- ------------

    OPT   Compute is more heavily utilized than Memory: Look at the Compute Workload Analysis section to see what the   
          compute pipelines are spending their time doing. Also, consider whether any computation is redundant and      
          could be reduced or moved to look-up tables.                                                                  

    Section: GPU Speed Of Light Roofline Chart
    OPT   Est. Speedup: 82.3%                                                                                           
          The ratio of peak float (FP32) to double (FP64) performance on this device is 32:1. The workload achieved     
          close to 0% of this device's FP32 peak performance and 56% of its FP64 peak performance. If Compute Workload  
          Analysis determines that this workload is FP64 bound, consider using 32-bit precision floating point          
          operations to improve its performance. See the Profiling Guide                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Kbyte       393.22
    Dropped Samples                sample            0
    Maximum Sampling Interval       cycle       20,000
    # Pass Groups                                    1
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.11
    Executed Ipc Elapsed  inst/cycle         0.09
    Issue Slots Busy               %         2.65
    Issued Ipc Active     inst/cycle         0.11
    SM Busy                        %        86.97
    -------------------- ----------- ------------

    OPT   FP64 is the highest-utilized pipeline (87.0%) based on active cycles, taking into account the rates of its    
          different instructions. It executes 64-bit floating point operations. The pipeline is over-utilized and       
          likely a performance bottleneck. Based on the number of executed instructions, the highest utilized pipeline  
          (87.0%) is FP64. It executes 64-bit floating point operations. Comparing the two, the overall pipeline        
          utilization appears to be caused by frequent, low-latency instructions. See the Profiling Guide               
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-decoder) or hover over the          
          pipeline name to understand the workloads handled by each pipeline. The Instruction Statistics section shows  
          the mix of executed instructions for this workload. Check the Warp State Statistics section for which         
          reasons cause warps to stall.                                                                                 

    Section: Memory Workload Analysis
    ----------------- ----------- ------------
    Metric Name       Metric Unit Metric Value
    ----------------- ----------- ------------
    Memory Throughput     Gbyte/s        15.64
    Mem Busy                    %         3.00
    Max Bandwidth               %         3.61
    L1/TEX Hit Rate             %        79.27
    L2 Hit Rate                 %        80.22
    Mem Pipes Busy              %        19.76
    ----------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 3.237%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 9.1 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 3.392%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32     
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %         2.65
    Issued Warp Per Scheduler                        0.03
    No Eligible                            %        97.35
    Active Warps Per Scheduler          warp         6.63
    Eligible Warps Per Scheduler        warp         0.07
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 24.4%                                                                                     
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 37.7 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 8 warps per scheduler, this workload allocates an average of  
          6.63 active warps per scheduler, but only an average of 0.07 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle       249.92
    Warp Cycles Per Executed Instruction           cycle       251.15
    Avg. Active Threads Per Warp                                31.99
    Avg. Not Predicated Off Threads Per Warp                    30.70
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 24.4%                                                                                           
          On average, each warp of this workload spends 121.1 cycles being stalled waiting for a scoreboard dependency  
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 48.5% of the total average of 249.9 cycles       
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 24.4%                                                                                           
          On average, each warp of this workload spends 120.8 cycles being stalled waiting for the L1 instruction queue 
          for texture operations to be not full. This stall reason is high in cases of extreme utilization of the       
          L1TEX pipeline. Try issuing fewer texture fetches, surface loads, surface stores, or decoupled math           
          operations. If applicable, consider combining multiple lower-width memory operations into fewer wider memory  
          operations and try interleaving memory operations and math instructions. Consider converting texture lookups  
          or surface loads into global memory lookups. Texture can accept four threads' requests per cycle, whereas     
          global accepts 32 threads. This stall type represents about 48.3% of the total average of 249.9 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Profiling Guide                                                                            
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst     8,125.43
    Executed Instructions                           inst    1,300,068
    Avg. Issued Instructions Per Scheduler          inst     8,165.46
    Issued Instructions                             inst    1,306,474
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 16.53%                                                                                          
          This kernel executes 388846 fused and 238388 non-fused FP64 instructions. By converting pairs of non-fused    
          instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point),           
          higher-throughput equivalent, the achieved FP64 performance could be increased by up to 19% (relative to its  
          current performance).                                                                                         

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    256
    Registers Per Thread             register/thread              56
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              40
    Stack Size                                                 1,024
    Threads                                   thread          65,536
    # TPCs                                                        20
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                1.60
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 97 thread blocks.   
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 50.0% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        82.74
    Achieved Active Warps Per SM           warp        26.48
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 17.26%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (82.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    54,271.50
    Total DRAM Elapsed Cycles        cycle   12,029,952
    Average L1 Active Cycles         cycle      308,269
    Total L1 Elapsed Cycles          cycle   14,183,978
    Average L2 Active Cycles         cycle    41,700.16
    Total L2 Elapsed Cycles          cycle   10,864,640
    Average SM Active Cycles         cycle      308,269
    Total SM Elapsed Cycles          cycle   14,183,978
    Average SMSP Active Cycles       cycle   307,896.22
    Total SMSP Elapsed Cycles        cycle   56,735,912
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 11.07%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 12.73% above the average, while the minimum instance value is 1.90% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.12%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 12.80% above the average, while the minimum instance value is 1.90% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.07%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 12.73% above the average, while the minimum instance value is 1.90% below the       
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.07
    Branch Instructions              inst       88,024
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 8.464%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 421875 excessive sectors (69% of the      
          total 612171 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source         
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  cuda_aux_computation(const double *, const double *, double *, double, int, double) (88, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.63
    SM Frequency                    Ghz         1.57
    Elapsed Cycles                cycle       26,117
    Memory Throughput                 %        89.01
    DRAM Throughput                   %        89.01
    Duration                         us        16.54
    L1/TEX Cache Throughput           %        18.17
    L2 Cache Throughput               %        23.67
    SM Active Cycles              cycle    17,860.58
    Compute (SM) Throughput           %        15.76
    ----------------------- ----------- ------------

    INF   This workload is utilizing greater than 80.0% of the available compute or memory performance of the device.   
          To further improve performance, work will likely need to be shifted from the most utilized to another unit.   
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: GPU Speed Of Light Roofline Chart
    OPT   Est. Speedup: 18.48%                                                                                          
          The ratio of peak float (FP32) to double (FP64) performance on this device is 32:1. The workload achieved 0%  
          of this device's FP32 peak performance and 15% of its FP64 peak performance. If Compute Workload Analysis     
          determines that this workload is FP64 bound, consider using 32-bit precision floating point operations to     
          improve its performance. See the Profiling Guide                                                              
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Kbyte       393.22
    Dropped Samples                sample            0
    Maximum Sampling Interval       cycle       20,000
    # Pass Groups                                    1
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.14
    Executed Ipc Elapsed  inst/cycle         0.12
    Issue Slots Busy               %         3.66
    Issued Ipc Active     inst/cycle         0.15
    SM Busy                        %        19.08
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 80.92%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ----------------- ----------- ------------
    Metric Name       Metric Unit Metric Value
    ----------------- ----------- ------------
    Memory Throughput     Gbyte/s       320.88
    Mem Busy                    %        23.67
    Max Bandwidth               %        89.01
    L1/TEX Hit Rate             %            0
    L2 Hit Rate                 %        33.94
    Mem Pipes Busy              %         7.39
    ----------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %         3.59
    Issued Warp Per Scheduler                        0.04
    No Eligible                            %        96.41
    Active Warps Per Scheduler          warp         4.22
    Eligible Warps Per Scheduler        warp         0.04
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 10.99%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 27.9 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 8 warps per scheduler, this workload allocates an average of  
          4.22 active warps per scheduler, but only an average of 0.04 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp  
          State Statistics and Source Counters sections.                                                                

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle       117.61
    Warp Cycles Per Executed Instruction           cycle       120.75
    Avg. Active Threads Per Warp                                31.99
    Avg. Not Predicated Off Threads Per Warp                    30.04
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 10.99%                                                                                          
          On average, each warp of this workload spends 107.8 cycles being stalled waiting for a scoreboard dependency  
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 91.7% of the total average of 117.6 cycles       
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Profiling Guide                                                                            
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       636.09
    Executed Instructions                           inst      101,775
    Avg. Issued Instructions Per Scheduler          inst       653.08
    Issued Instructions                             inst      104,493
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     88
    Registers Per Thread             register/thread              28
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              40
    Stack Size                                                 1,024
    Threads                                   thread          22,528
    # TPCs                                                        20
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                0.55
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        53.85
    Achieved Active Warps Per SM           warp        17.23
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 10.99%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (53.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    82,946.50
    Total DRAM Elapsed Cycles        cycle      745,472
    Average L1 Active Cycles         cycle    17,860.58
    Total L1 Elapsed Cycles          cycle      865,156
    Average L2 Active Cycles         cycle    17,358.31
    Total L2 Elapsed Cycles          cycle      800,480
    Average SM Active Cycles         cycle    17,860.58
    Total SM Elapsed Cycles          cycle      865,156
    Average SMSP Active Cycles       cycle    18,200.53
    Total SMSP Elapsed Cycles        cycle    3,460,624
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 8.726%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 10.57% above the average, while the minimum instance value is 6.24% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 10.21%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 12.14% above the average, while the minimum instance value is 6.14% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.726%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 10.57% above the average, while the minimum instance value is 6.24% below the       
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.07
    Branch Instructions              inst        6,907
    Branch Efficiency                   %        99.95
    Avg. Divergent Branches                       0.01
    ------------------------- ----------- ------------

  cuda_right_hand_side(const double *, double *, int, double, double, double, double) (256, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.78
    SM Frequency                    Ghz         1.60
    Elapsed Cycles                cycle      355,883
    Memory Throughput                 %         3.65
    DRAM Throughput                   %         3.65
    Duration                         us       221.18
    L1/TEX Cache Throughput           %         4.52
    L2 Cache Throughput               %         3.04
    SM Active Cycles              cycle   308,273.25
    Compute (SM) Throughput           %        75.59
    ----------------------- ----------- ------------

    OPT   Compute is more heavily utilized than Memory: Look at the Compute Workload Analysis section to see what the   
          compute pipelines are spending their time doing. Also, consider whether any computation is redundant and      
          could be reduced or moved to look-up tables.                                                                  

    Section: GPU Speed Of Light Roofline Chart
    OPT   Est. Speedup: 82.3%                                                                                           
          The ratio of peak float (FP32) to double (FP64) performance on this device is 32:1. The workload achieved     
          close to 0% of this device's FP32 peak performance and 56% of its FP64 peak performance. If Compute Workload  
          Analysis determines that this workload is FP64 bound, consider using 32-bit precision floating point          
          operations to improve its performance. See the Profiling Guide                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Kbyte       393.22
    Dropped Samples                sample            0
    Maximum Sampling Interval       cycle       20,000
    # Pass Groups                                    1
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.11
    Executed Ipc Elapsed  inst/cycle         0.09
    Issue Slots Busy               %         2.65
    Issued Ipc Active     inst/cycle         0.11
    SM Busy                        %        86.96
    -------------------- ----------- ------------

    OPT   FP64 is the highest-utilized pipeline (87.0%) based on active cycles, taking into account the rates of its    
          different instructions. It executes 64-bit floating point operations. The pipeline is over-utilized and       
          likely a performance bottleneck. Based on the number of executed instructions, the highest utilized pipeline  
          (87.0%) is FP64. It executes 64-bit floating point operations. Comparing the two, the overall pipeline        
          utilization appears to be caused by frequent, low-latency instructions. See the Profiling Guide               
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-decoder) or hover over the          
          pipeline name to understand the workloads handled by each pipeline. The Instruction Statistics section shows  
          the mix of executed instructions for this workload. Check the Warp State Statistics section for which         
          reasons cause warps to stall.                                                                                 

    Section: Memory Workload Analysis
    ----------------- ----------- ------------
    Metric Name       Metric Unit Metric Value
    ----------------- ----------- ------------
    Memory Throughput     Gbyte/s        15.83
    Mem Busy                    %         3.00
    Max Bandwidth               %         3.65
    L1/TEX Hit Rate             %        79.29
    L2 Hit Rate                 %        80.60
    Mem Pipes Busy              %        19.75
    ----------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 3.237%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 9.1 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 3.392%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32     
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %         2.65
    Issued Warp Per Scheduler                        0.03
    No Eligible                            %        97.35
    Active Warps Per Scheduler          warp         6.62
    Eligible Warps Per Scheduler        warp         0.07
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 24.41%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 37.7 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 8 warps per scheduler, this workload allocates an average of  
          6.62 active warps per scheduler, but only an average of 0.07 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle       249.80
    Warp Cycles Per Executed Instruction           cycle       251.03
    Avg. Active Threads Per Warp                                31.99
    Avg. Not Predicated Off Threads Per Warp                    30.70
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 24.41%                                                                                          
          On average, each warp of this workload spends 121.2 cycles being stalled waiting for a scoreboard dependency  
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 48.5% of the total average of 249.8 cycles       
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 24.41%                                                                                          
          On average, each warp of this workload spends 120.9 cycles being stalled waiting for the L1 instruction queue 
          for texture operations to be not full. This stall reason is high in cases of extreme utilization of the       
          L1TEX pipeline. Try issuing fewer texture fetches, surface loads, surface stores, or decoupled math           
          operations. If applicable, consider combining multiple lower-width memory operations into fewer wider memory  
          operations and try interleaving memory operations and math instructions. Consider converting texture lookups  
          or surface loads into global memory lookups. Texture can accept four threads' requests per cycle, whereas     
          global accepts 32 threads. This stall type represents about 48.4% of the total average of 249.8 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Profiling Guide                                                                            
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst     8,125.43
    Executed Instructions                           inst    1,300,068
    Avg. Issued Instructions Per Scheduler          inst     8,165.49
    Issued Instructions                             inst    1,306,478
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 16.53%                                                                                          
          This kernel executes 388846 fused and 238388 non-fused FP64 instructions. By converting pairs of non-fused    
          instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point),           
          higher-throughput equivalent, the achieved FP64 performance could be increased by up to 19% (relative to its  
          current performance).                                                                                         

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    256
    Registers Per Thread             register/thread              56
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              40
    Stack Size                                                 1,024
    Threads                                   thread          65,536
    # TPCs                                                        20
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                1.60
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 97 thread blocks.   
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 50.0% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        82.74
    Achieved Active Warps Per SM           warp        26.48
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 17.26%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (82.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       54,711
    Total DRAM Elapsed Cycles        cycle   12,001,280
    Average L1 Active Cycles         cycle   308,273.25
    Total L1 Elapsed Cycles          cycle   14,186,290
    Average L2 Active Cycles         cycle    41,807.78
    Total L2 Elapsed Cycles          cycle   10,823,328
    Average SM Active Cycles         cycle   308,273.25
    Total SM Elapsed Cycles          cycle   14,186,290
    Average SMSP Active Cycles       cycle   307,887.09
    Total SMSP Elapsed Cycles        cycle   56,745,160
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 11.08%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 12.75% above the average, while the minimum instance value is 1.86% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.14%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 12.83% above the average, while the minimum instance value is 1.94% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.08%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 12.75% above the average, while the minimum instance value is 1.86% below the       
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.07
    Branch Instructions              inst       88,024
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 8.518%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 421875 excessive sectors (69% of the      
          total 612171 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source         
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  cuda_aux_computation(const double *, const double *, double *, double, int, double) (88, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.66
    SM Frequency                    Ghz         1.57
    Elapsed Cycles                cycle       26,112
    Memory Throughput                 %        88.68
    DRAM Throughput                   %        88.68
    Duration                         us        16.54
    L1/TEX Cache Throughput           %        18.04
    L2 Cache Throughput               %        23.72
    SM Active Cycles              cycle       17,982
    Compute (SM) Throughput           %        15.64
    ----------------------- ----------- ------------

    INF   This workload is utilizing greater than 80.0% of the available compute or memory performance of the device.   
          To further improve performance, work will likely need to be shifted from the most utilized to another unit.   
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (FP32) to double (FP64) performance on this device is 32:1. The workload achieved 0%  
          of this device's FP32 peak performance and 15% of its FP64 peak performance. See the Profiling Guide          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Kbyte       393.22
    Dropped Samples                sample            0
    Maximum Sampling Interval       cycle       20,000
    # Pass Groups                                    1
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.14
    Executed Ipc Elapsed  inst/cycle         0.12
    Issue Slots Busy               %         3.63
    Issued Ipc Active     inst/cycle         0.15
    SM Busy                        %        18.95
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 81.05%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ----------------- ----------- ------------
    Metric Name       Metric Unit Metric Value
    ----------------- ----------- ------------
    Memory Throughput     Gbyte/s       321.44
    Mem Busy                    %        23.72
    Max Bandwidth               %        88.68
    L1/TEX Hit Rate             %            0
    L2 Hit Rate                 %        33.93
    Mem Pipes Busy              %         7.34
    ----------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %         3.65
    Issued Warp Per Scheduler                        0.04
    No Eligible                            %        96.35
    Active Warps Per Scheduler          warp         4.32
    Eligible Warps Per Scheduler        warp         0.05
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 11.32%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 27.4 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 8 warps per scheduler, this workload allocates an average of  
          4.32 active warps per scheduler, but only an average of 0.05 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp  
          State Statistics and Source Counters sections.                                                                

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle       118.44
    Warp Cycles Per Executed Instruction           cycle       121.60
    Avg. Active Threads Per Warp                                31.99
    Avg. Not Predicated Off Threads Per Warp                    30.04
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 11.32%                                                                                          
          On average, each warp of this workload spends 108.1 cycles being stalled waiting for a scoreboard dependency  
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 91.3% of the total average of 118.4 cycles       
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Profiling Guide                                                                            
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       636.09
    Executed Instructions                           inst      101,775
    Avg. Issued Instructions Per Scheduler          inst       653.09
    Issued Instructions                             inst      104,494
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     88
    Registers Per Thread             register/thread              28
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              40
    Stack Size                                                 1,024
    Threads                                   thread          22,528
    # TPCs                                                        20
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                0.55
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        53.83
    Achieved Active Warps Per SM           warp        17.23
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 11.32%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (53.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    83,092.50
    Total DRAM Elapsed Cycles        cycle      749,568
    Average L1 Active Cycles         cycle       17,982
    Total L1 Elapsed Cycles          cycle      871,518
    Average L2 Active Cycles         cycle    16,979.81
    Total L2 Elapsed Cycles          cycle      798,880
    Average SM Active Cycles         cycle       17,982
    Total SM Elapsed Cycles          cycle      871,518
    Average SMSP Active Cycles       cycle    17,910.89
    Total SMSP Elapsed Cycles        cycle    3,486,072
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 8.916%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 10.80% above the average, while the minimum instance value is 4.99% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 9.835%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 11.96% above the average, while the minimum instance value is 4.67% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.916%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 10.80% above the average, while the minimum instance value is 4.99% below the       
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.07
    Branch Instructions              inst        6,907
    Branch Efficiency                   %        99.95
    Avg. Divergent Branches                       0.01
    ------------------------- ----------- ------------

  cuda_right_hand_side(const double *, double *, int, double, double, double, double) (256, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.74
    SM Frequency                    Ghz         1.59
    Elapsed Cycles                cycle      355,503
    Memory Throughput                 %         3.60
    DRAM Throughput                   %         3.60
    Duration                         us       222.88
    L1/TEX Cache Throughput           %         4.53
    L2 Cache Throughput               %         3.01
    SM Active Cycles              cycle   308,258.45
    Compute (SM) Throughput           %        75.66
    ----------------------- ----------- ------------

    OPT   Compute is more heavily utilized than Memory: Look at the Compute Workload Analysis section to see what the   
          compute pipelines are spending their time doing. Also, consider whether any computation is redundant and      
          could be reduced or moved to look-up tables.                                                                  

    Section: GPU Speed Of Light Roofline Chart
    OPT   Est. Speedup: 82.3%                                                                                           
          The ratio of peak float (FP32) to double (FP64) performance on this device is 32:1. The workload achieved     
          close to 0% of this device's FP32 peak performance and 56% of its FP64 peak performance. If Compute Workload  
          Analysis determines that this workload is FP64 bound, consider using 32-bit precision floating point          
          operations to improve its performance. See the Profiling Guide                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Kbyte       393.22
    Dropped Samples                sample            0
    Maximum Sampling Interval       cycle       20,000
    # Pass Groups                                    1
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.11
    Executed Ipc Elapsed  inst/cycle         0.09
    Issue Slots Busy               %         2.65
    Issued Ipc Active     inst/cycle         0.11
    SM Busy                        %        86.97
    -------------------- ----------- ------------

    OPT   FP64 is the highest-utilized pipeline (87.0%) based on active cycles, taking into account the rates of its    
          different instructions. It executes 64-bit floating point operations. The pipeline is over-utilized and       
          likely a performance bottleneck. Based on the number of executed instructions, the highest utilized pipeline  
          (87.0%) is FP64. It executes 64-bit floating point operations. Comparing the two, the overall pipeline        
          utilization appears to be caused by frequent, low-latency instructions. See the Profiling Guide               
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-decoder) or hover over the          
          pipeline name to understand the workloads handled by each pipeline. The Instruction Statistics section shows  
          the mix of executed instructions for this workload. Check the Warp State Statistics section for which         
          reasons cause warps to stall.                                                                                 

    Section: Memory Workload Analysis
    ----------------- ----------- ------------
    Metric Name       Metric Unit Metric Value
    ----------------- ----------- ------------
    Memory Throughput     Gbyte/s        15.54
    Mem Busy                    %         2.98
    Max Bandwidth               %         3.60
    L1/TEX Hit Rate             %        79.19
    L2 Hit Rate                 %        80.53
    Mem Pipes Busy              %        19.77
    ----------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 3.24%                                                                                           
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 9.1 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 3.395%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32     
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %         2.65
    Issued Warp Per Scheduler                        0.03
    No Eligible                            %        97.35
    Active Warps Per Scheduler          warp         6.62
    Eligible Warps Per Scheduler        warp         0.07
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 24.34%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 37.7 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 8 warps per scheduler, this workload allocates an average of  
          6.62 active warps per scheduler, but only an average of 0.07 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle       249.74
    Warp Cycles Per Executed Instruction           cycle       250.98
    Avg. Active Threads Per Warp                                31.99
    Avg. Not Predicated Off Threads Per Warp                    30.73
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 24.34%                                                                                          
          On average, each warp of this workload spends 121.2 cycles being stalled waiting for a scoreboard dependency  
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 48.5% of the total average of 249.7 cycles       
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 24.34%                                                                                          
          On average, each warp of this workload spends 120.9 cycles being stalled waiting for the L1 instruction queue 
          for texture operations to be not full. This stall reason is high in cases of extreme utilization of the       
          L1TEX pipeline. Try issuing fewer texture fetches, surface loads, surface stores, or decoupled math           
          operations. If applicable, consider combining multiple lower-width memory operations into fewer wider memory  
          operations and try interleaving memory operations and math instructions. Consider converting texture lookups  
          or surface loads into global memory lookups. Texture can accept four threads' requests per cycle, whereas     
          global accepts 32 threads. This stall type represents about 48.4% of the total average of 249.7 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Profiling Guide                                                                            
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst     8,125.43
    Executed Instructions                           inst    1,300,068
    Avg. Issued Instructions Per Scheduler          inst     8,165.46
    Issued Instructions                             inst    1,306,473
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 16.53%                                                                                          
          This kernel executes 388846 fused and 238388 non-fused FP64 instructions. By converting pairs of non-fused    
          instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point),           
          higher-throughput equivalent, the achieved FP64 performance could be increased by up to 19% (relative to its  
          current performance).                                                                                         

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    256
    Registers Per Thread             register/thread              56
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              40
    Stack Size                                                 1,024
    Threads                                   thread          65,536
    # TPCs                                                        20
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                1.60
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 97 thread blocks.   
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 50.0% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        82.74
    Achieved Active Warps Per SM           warp        26.48
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 17.26%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (82.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       54,132
    Total DRAM Elapsed Cycles        cycle   12,025,856
    Average L1 Active Cycles         cycle   308,258.45
    Total L1 Elapsed Cycles          cycle   14,173,828
    Average L2 Active Cycles         cycle    42,101.50
    Total L2 Elapsed Cycles          cycle   10,884,224
    Average SM Active Cycles         cycle   308,258.45
    Total SM Elapsed Cycles          cycle   14,173,828
    Average SMSP Active Cycles       cycle   307,915.54
    Total SMSP Elapsed Cycles        cycle   56,695,312
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 11.13%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 12.80% above the average, while the minimum instance value is 1.88% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.16%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 12.85% above the average, while the minimum instance value is 1.91% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.13%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 12.80% above the average, while the minimum instance value is 1.88% below the       
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.07
    Branch Instructions              inst       88,024
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 8.53%                                                                                           
          This kernel has uncoalesced global accesses resulting in a total of 421875 excessive sectors (69% of the      
          total 612171 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source         
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  cuda_final_update(double *, const double *, const double *, const double *, const double *, double, int) (88, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.03
    SM Frequency                    Ghz         1.58
    Elapsed Cycles                cycle       55,766
    Memory Throughput                 %        90.85
    DRAM Throughput                   %        90.85
    Duration                         us        35.26
    L1/TEX Cache Throughput           %        17.30
    L2 Cache Throughput               %        22.05
    SM Active Cycles              cycle    45,169.78
    Compute (SM) Throughput           %        25.56
    ----------------------- ----------- ------------

    INF   This workload is utilizing greater than 80.0% of the available compute or memory performance of the device.   
          To further improve performance, work will likely need to be shifted from the most utilized to another unit.   
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: GPU Speed Of Light Roofline Chart
    OPT   Est. Speedup: 27.42%                                                                                          
          The ratio of peak float (FP32) to double (FP64) performance on this device is 32:1. The workload achieved 0%  
          of this device's FP32 peak performance and 22% of its FP64 peak performance. If Compute Workload Analysis     
          determines that this workload is FP64 bound, consider using 32-bit precision floating point operations to     
          improve its performance. See the Profiling Guide                                                              
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Kbyte       393.22
    Dropped Samples                sample            0
    Maximum Sampling Interval       cycle       20,000
    # Pass Groups                                    1
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.09
    Executed Ipc Elapsed  inst/cycle         0.08
    Issue Slots Busy               %         2.33
    Issued Ipc Active     inst/cycle         0.09
    SM Busy                        %        28.31
    -------------------- ----------- ------------

    INF   FP64 is the highest-utilized pipeline (28.3%) based on active cycles, taking into account the rates of its    
          different instructions. It executes 64-bit floating point operations. It is well-utilized, but should not be  
          a bottleneck.                                                                                                 

    Section: Memory Workload Analysis
    ----------------- ----------- ------------
    Metric Name       Metric Unit Metric Value
    ----------------- ----------- ------------
    Memory Throughput     Gbyte/s       350.33
    Mem Busy                    %        22.05
    Max Bandwidth               %        90.85
    L1/TEX Hit Rate             %        14.11
    L2 Hit Rate                 %        17.08
    Mem Pipes Busy              %         7.88
    ----------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %         2.33
    Issued Warp Per Scheduler                        0.02
    No Eligible                            %        97.67
    Active Warps Per Scheduler          warp         4.30
    Eligible Warps Per Scheduler        warp         0.03
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 9.152%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 42.9 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 8 warps per scheduler, this workload allocates an average of  
          4.30 active warps per scheduler, but only an average of 0.03 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp  
          State Statistics and Source Counters sections.                                                                

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle       184.59
    Warp Cycles Per Executed Instruction           cycle       187.93
    Avg. Active Threads Per Warp                                31.99
    Avg. Not Predicated Off Threads Per Warp                    30.79
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 9.152%                                                                                          
          On average, each warp of this workload spends 173.4 cycles being stalled waiting for a scoreboard dependency  
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 93.9% of the total average of 184.6 cycles       
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Profiling Guide                                                                            
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst     1,034.84
    Executed Instructions                           inst      165,575
    Avg. Issued Instructions Per Scheduler          inst     1,053.57
    Issued Instructions                             inst      168,571
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 3.772%                                                                                          
          This kernel executes 23448 fused and 8520 non-fused FP64 instructions. By converting pairs of non-fused       
          instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point),           
          higher-throughput equivalent, the achieved FP64 performance could be increased by up to 13% (relative to its  
          current performance).                                                                                         

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     88
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              40
    Stack Size                                                 1,024
    Threads                                   thread          22,528
    # TPCs                                                        20
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                0.55
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        53.42
    Achieved Active Warps Per SM           warp        17.09
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 9.152%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (53.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   193,033.50
    Total DRAM Elapsed Cycles        cycle    1,699,840
    Average L1 Active Cycles         cycle    45,169.78
    Total L1 Elapsed Cycles          cycle    2,000,876
    Average L2 Active Cycles         cycle    43,270.41
    Total L2 Elapsed Cycles          cycle    1,709,696
    Average SM Active Cycles         cycle    45,169.78
    Total SM Elapsed Cycles          cycle    2,000,876
    Average SMSP Active Cycles       cycle    45,177.36
    Total SMSP Elapsed Cycles        cycle    8,003,504
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.353%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 5.93% above the average, while the minimum instance value is 2.74% below the average.       
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.362%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 7.04% above the average, while the minimum instance value is 3.56% below the        
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.353%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 5.93% above the average, while the minimum instance value is 2.74% below the        
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.04
    Branch Instructions              inst        6,907
    Branch Efficiency                   %        99.95
    Avg. Divergent Branches                       0.01
    ------------------------- ----------- ------------

  cuda_right_hand_side(const double *, double *, int, double, double, double, double) (256, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.79
    SM Frequency                    Ghz         1.60
    Elapsed Cycles                cycle      355,419
    Memory Throughput                 %         3.62
    DRAM Throughput                   %         3.62
    Duration                         us       220.93
    L1/TEX Cache Throughput           %         4.53
    L2 Cache Throughput               %         3.05
    SM Active Cycles              cycle   308,198.15
    Compute (SM) Throughput           %        75.64
    ----------------------- ----------- ------------

    OPT   Compute is more heavily utilized than Memory: Look at the Compute Workload Analysis section to see what the   
          compute pipelines are spending their time doing. Also, consider whether any computation is redundant and      
          could be reduced or moved to look-up tables.                                                                  

    Section: GPU Speed Of Light Roofline Chart
    OPT   Est. Speedup: 82.32%                                                                                          
          The ratio of peak float (FP32) to double (FP64) performance on this device is 32:1. The workload achieved     
          close to 0% of this device's FP32 peak performance and 56% of its FP64 peak performance. If Compute Workload  
          Analysis determines that this workload is FP64 bound, consider using 32-bit precision floating point          
          operations to improve its performance. See the Profiling Guide                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Kbyte       393.22
    Dropped Samples                sample            0
    Maximum Sampling Interval       cycle       20,000
    # Pass Groups                                    1
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.11
    Executed Ipc Elapsed  inst/cycle         0.09
    Issue Slots Busy               %         2.65
    Issued Ipc Active     inst/cycle         0.11
    SM Busy                        %        86.99
    -------------------- ----------- ------------

    OPT   FP64 is the highest-utilized pipeline (87.0%) based on active cycles, taking into account the rates of its    
          different instructions. It executes 64-bit floating point operations. The pipeline is over-utilized and       
          likely a performance bottleneck. Based on the number of executed instructions, the highest utilized pipeline  
          (87.0%) is FP64. It executes 64-bit floating point operations. Comparing the two, the overall pipeline        
          utilization appears to be caused by frequent, low-latency instructions. See the Profiling Guide               
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-decoder) or hover over the          
          pipeline name to understand the workloads handled by each pipeline. The Instruction Statistics section shows  
          the mix of executed instructions for this workload. Check the Warp State Statistics section for which         
          reasons cause warps to stall.                                                                                 

    Section: Memory Workload Analysis
    ----------------- ----------- ------------
    Metric Name       Metric Unit Metric Value
    ----------------- ----------- ------------
    Memory Throughput     Gbyte/s        15.74
    Mem Busy                    %         3.01
    Max Bandwidth               %         3.62
    L1/TEX Hit Rate             %        79.26
    L2 Hit Rate                 %        80.61
    Mem Pipes Busy              %        19.77
    ----------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 3.239%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 9.1 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 3.394%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32     
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %         2.65
    Issued Warp Per Scheduler                        0.03
    No Eligible                            %        97.35
    Active Warps Per Scheduler          warp         6.63
    Eligible Warps Per Scheduler        warp         0.07
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 24.36%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 37.7 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 8 warps per scheduler, this workload allocates an average of  
          6.63 active warps per scheduler, but only an average of 0.07 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle       249.81
    Warp Cycles Per Executed Instruction           cycle       251.04
    Avg. Active Threads Per Warp                                31.99
    Avg. Not Predicated Off Threads Per Warp                    30.73
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 24.36%                                                                                          
          On average, each warp of this workload spends 121.1 cycles being stalled waiting for a scoreboard dependency  
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 48.5% of the total average of 249.8 cycles       
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 24.36%                                                                                          
          On average, each warp of this workload spends 120.9 cycles being stalled waiting for the L1 instruction queue 
          for texture operations to be not full. This stall reason is high in cases of extreme utilization of the       
          L1TEX pipeline. Try issuing fewer texture fetches, surface loads, surface stores, or decoupled math           
          operations. If applicable, consider combining multiple lower-width memory operations into fewer wider memory  
          operations and try interleaving memory operations and math instructions. Consider converting texture lookups  
          or surface loads into global memory lookups. Texture can accept four threads' requests per cycle, whereas     
          global accepts 32 threads. This stall type represents about 48.4% of the total average of 249.8 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Profiling Guide                                                                            
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst     8,125.43
    Executed Instructions                           inst    1,300,068
    Avg. Issued Instructions Per Scheduler          inst     8,165.46
    Issued Instructions                             inst    1,306,474
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 16.53%                                                                                          
          This kernel executes 388846 fused and 238388 non-fused FP64 instructions. By converting pairs of non-fused    
          instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point),           
          higher-throughput equivalent, the achieved FP64 performance could be increased by up to 19% (relative to its  
          current performance).                                                                                         

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    256
    Registers Per Thread             register/thread              56
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              40
    Stack Size                                                 1,024
    Threads                                   thread          65,536
    # TPCs                                                        20
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                1.60
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 97 thread blocks.   
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 50.0% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        82.73
    Achieved Active Warps Per SM           warp        26.47
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 17.27%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (82.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       54,341
    Total DRAM Elapsed Cycles        cycle   12,007,424
    Average L1 Active Cycles         cycle   308,198.15
    Total L1 Elapsed Cycles          cycle   14,176,638
    Average L2 Active Cycles         cycle    42,641.75
    Total L2 Elapsed Cycles          cycle   10,811,008
    Average SM Active Cycles         cycle   308,198.15
    Total SM Elapsed Cycles          cycle   14,176,638
    Average SMSP Active Cycles       cycle   307,886.46
    Total SMSP Elapsed Cycles        cycle   56,706,552
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 11.13%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 12.80% above the average, while the minimum instance value is 1.97% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.15%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 12.83% above the average, while the minimum instance value is 1.93% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.13%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 12.80% above the average, while the minimum instance value is 1.97% below the       
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.07
    Branch Instructions              inst       88,024
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 8.698%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 421875 excessive sectors (69% of the      
          total 612171 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source         
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  cuda_aux_computation(const double *, const double *, double *, double, int, double) (88, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.77
    SM Frequency                    Ghz         1.58
    Elapsed Cycles                cycle       25,690
    Memory Throughput                 %        88.03
    DRAM Throughput                   %        88.03
    Duration                         us        16.16
    L1/TEX Cache Throughput           %        17.85
    L2 Cache Throughput               %        24.11
    SM Active Cycles              cycle       18,298
    Compute (SM) Throughput           %        15.50
    ----------------------- ----------- ------------

    INF   This workload is utilizing greater than 80.0% of the available compute or memory performance of the device.   
          To further improve performance, work will likely need to be shifted from the most utilized to another unit.   
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (FP32) to double (FP64) performance on this device is 32:1. The workload achieved 0%  
          of this device's FP32 peak performance and 15% of its FP64 peak performance. See the Profiling Guide          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Kbyte       393.22
    Dropped Samples                sample            0
    Maximum Sampling Interval       cycle       20,000
    # Pass Groups                                    1
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.14
    Executed Ipc Elapsed  inst/cycle         0.12
    Issue Slots Busy               %         3.57
    Issued Ipc Active     inst/cycle         0.14
    SM Busy                        %        18.62
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 81.38%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ----------------- ----------- ------------
    Metric Name       Metric Unit Metric Value
    ----------------- ----------- ------------
    Memory Throughput     Gbyte/s       324.88
    Mem Busy                    %        24.11
    Max Bandwidth               %        88.03
    L1/TEX Hit Rate             %            0
    L2 Hit Rate                 %        33.95
    Mem Pipes Busy              %         7.27
    ----------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %         3.65
    Issued Warp Per Scheduler                        0.04
    No Eligible                            %        96.35
    Active Warps Per Scheduler          warp         4.27
    Eligible Warps Per Scheduler        warp         0.05
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 11.97%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 27.4 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 8 warps per scheduler, this workload allocates an average of  
          4.27 active warps per scheduler, but only an average of 0.05 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp  
          State Statistics and Source Counters sections.                                                                

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle       116.81
    Warp Cycles Per Executed Instruction           cycle       119.93
    Avg. Active Threads Per Warp                                31.99
    Avg. Not Predicated Off Threads Per Warp                    30.04
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 11.97%                                                                                          
          On average, each warp of this workload spends 107.7 cycles being stalled waiting for a scoreboard dependency  
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 92.2% of the total average of 116.8 cycles       
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Profiling Guide                                                                            
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       636.09
    Executed Instructions                           inst      101,775
    Avg. Issued Instructions Per Scheduler          inst       653.12
    Issued Instructions                             inst      104,499
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     88
    Registers Per Thread             register/thread              28
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              40
    Stack Size                                                 1,024
    Threads                                   thread          22,528
    # TPCs                                                        20
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                0.55
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        52.51
    Achieved Active Warps Per SM           warp        16.80
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 11.97%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (52.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       82,033
    Total DRAM Elapsed Cycles        cycle      745,472
    Average L1 Active Cycles         cycle       18,298
    Total L1 Elapsed Cycles          cycle      879,556
    Average L2 Active Cycles         cycle    17,029.94
    Total L2 Elapsed Cycles          cycle      785,504
    Average SM Active Cycles         cycle       18,298
    Total SM Elapsed Cycles          cycle      879,556
    Average SMSP Active Cycles       cycle    17,870.59
    Total SMSP Elapsed Cycles        cycle    3,518,224
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 9.453%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 11.36% above the average, while the minimum instance value is 5.73% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.913%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 10.97% above the average, while the minimum instance value is 5.92% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 9.453%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 11.36% above the average, while the minimum instance value is 5.73% below the       
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.07
    Branch Instructions              inst        6,907
    Branch Efficiency                   %        99.95
    Avg. Divergent Branches                       0.01
    ------------------------- ----------- ------------

  cuda_right_hand_side(const double *, double *, int, double, double, double, double) (256, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.86
    SM Frequency                    Ghz         1.60
    Elapsed Cycles                cycle      355,718
    Memory Throughput                 %         3.58
    DRAM Throughput                   %         3.58
    Duration                         us       221.02
    L1/TEX Cache Throughput           %         4.52
    L2 Cache Throughput               %         3.05
    SM Active Cycles              cycle   308,216.95
    Compute (SM) Throughput           %        75.56
    ----------------------- ----------- ------------

    OPT   Compute is more heavily utilized than Memory: Look at the Compute Workload Analysis section to see what the   
          compute pipelines are spending their time doing. Also, consider whether any computation is redundant and      
          could be reduced or moved to look-up tables.                                                                  

    Section: GPU Speed Of Light Roofline Chart
    OPT   Est. Speedup: 82.31%                                                                                          
          The ratio of peak float (FP32) to double (FP64) performance on this device is 32:1. The workload achieved     
          close to 0% of this device's FP32 peak performance and 56% of its FP64 peak performance. If Compute Workload  
          Analysis determines that this workload is FP64 bound, consider using 32-bit precision floating point          
          operations to improve its performance. See the Profiling Guide                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Kbyte       393.22
    Dropped Samples                sample            0
    Maximum Sampling Interval       cycle       20,000
    # Pass Groups                                    1
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.11
    Executed Ipc Elapsed  inst/cycle         0.09
    Issue Slots Busy               %         2.65
    Issued Ipc Active     inst/cycle         0.11
    SM Busy                        %        86.98
    -------------------- ----------- ------------

    OPT   FP64 is the highest-utilized pipeline (87.0%) based on active cycles, taking into account the rates of its    
          different instructions. It executes 64-bit floating point operations. The pipeline is over-utilized and       
          likely a performance bottleneck. Based on the number of executed instructions, the highest utilized pipeline  
          (87.0%) is FP64. It executes 64-bit floating point operations. Comparing the two, the overall pipeline        
          utilization appears to be caused by frequent, low-latency instructions. See the Profiling Guide               
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-decoder) or hover over the          
          pipeline name to understand the workloads handled by each pipeline. The Instruction Statistics section shows  
          the mix of executed instructions for this workload. Check the Warp State Statistics section for which         
          reasons cause warps to stall.                                                                                 

    Section: Memory Workload Analysis
    ----------------- ----------- ------------
    Metric Name       Metric Unit Metric Value
    ----------------- ----------- ------------
    Memory Throughput     Gbyte/s        15.70
    Mem Busy                    %         3.00
    Max Bandwidth               %         3.58
    L1/TEX Hit Rate             %        79.25
    L2 Hit Rate                 %        80.64
    Mem Pipes Busy              %        19.75
    ----------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 3.235%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 9.1 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 3.39%                                                                                           
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32     
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %         2.65
    Issued Warp Per Scheduler                        0.03
    No Eligible                            %        97.35
    Active Warps Per Scheduler          warp         6.63
    Eligible Warps Per Scheduler        warp         0.07
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 24.44%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 37.7 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 8 warps per scheduler, this workload allocates an average of  
          6.63 active warps per scheduler, but only an average of 0.07 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle       249.76
    Warp Cycles Per Executed Instruction           cycle       251.12
    Avg. Active Threads Per Warp                                31.99
    Avg. Not Predicated Off Threads Per Warp                    30.68
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 24.44%                                                                                          
          On average, each warp of this workload spends 121.1 cycles being stalled waiting for a scoreboard dependency  
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 48.5% of the total average of 249.8 cycles       
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 24.44%                                                                                          
          On average, each warp of this workload spends 120.8 cycles being stalled waiting for the L1 instruction queue 
          for texture operations to be not full. This stall reason is high in cases of extreme utilization of the       
          L1TEX pipeline. Try issuing fewer texture fetches, surface loads, surface stores, or decoupled math           
          operations. If applicable, consider combining multiple lower-width memory operations into fewer wider memory  
          operations and try interleaving memory operations and math instructions. Consider converting texture lookups  
          or surface loads into global memory lookups. Texture can accept four threads' requests per cycle, whereas     
          global accepts 32 threads. This stall type represents about 48.4% of the total average of 249.8 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Profiling Guide                                                                            
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst     8,125.43
    Executed Instructions                           inst    1,300,068
    Avg. Issued Instructions Per Scheduler          inst     8,169.77
    Issued Instructions                             inst    1,307,164
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 16.53%                                                                                          
          This kernel executes 388846 fused and 238388 non-fused FP64 instructions. By converting pairs of non-fused    
          instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point),           
          higher-throughput equivalent, the achieved FP64 performance could be increased by up to 19% (relative to its  
          current performance).                                                                                         

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    256
    Registers Per Thread             register/thread              56
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              40
    Stack Size                                                 1,024
    Threads                                   thread          65,536
    # TPCs                                                        20
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                1.60
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 97 thread blocks.   
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 50.0% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        82.78
    Achieved Active Warps Per SM           warp        26.49
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 17.22%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (82.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       54,236
    Total DRAM Elapsed Cycles        cycle   12,136,448
    Average L1 Active Cycles         cycle   308,216.95
    Total L1 Elapsed Cycles          cycle   14,192,470
    Average L2 Active Cycles         cycle    42,488.03
    Total L2 Elapsed Cycles          cycle   10,814,720
    Average SM Active Cycles         cycle   308,216.95
    Total SM Elapsed Cycles          cycle   14,192,470
    Average SMSP Active Cycles       cycle   307,883.30
    Total SMSP Elapsed Cycles        cycle   56,769,880
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 11.09%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 12.76% above the average, while the minimum instance value is 1.90% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.13%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 12.83% above the average, while the minimum instance value is 1.83% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.09%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 12.76% above the average, while the minimum instance value is 1.90% below the       
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.07
    Branch Instructions              inst       88,024
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 8.664%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 421875 excessive sectors (69% of the      
          total 612171 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source         
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  cuda_aux_computation(const double *, const double *, double *, double, int, double) (88, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.82
    SM Frequency                    Ghz         1.57
    Elapsed Cycles                cycle       25,930
    Memory Throughput                 %        86.50
    DRAM Throughput                   %        86.50
    Duration                         us        16.45
    L1/TEX Cache Throughput           %        17.97
    L2 Cache Throughput               %        23.84
    SM Active Cycles              cycle    17,927.67
    Compute (SM) Throughput           %        15.59
    ----------------------- ----------- ------------

    INF   This workload is utilizing greater than 80.0% of the available compute or memory performance of the device.   
          To further improve performance, work will likely need to be shifted from the most utilized to another unit.   
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (FP32) to double (FP64) performance on this device is 32:1. The workload achieved 0%  
          of this device's FP32 peak performance and 15% of its FP64 peak performance. See the Profiling Guide          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Kbyte       393.22
    Dropped Samples                sample            0
    Maximum Sampling Interval       cycle       20,000
    # Pass Groups                                    1
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.14
    Executed Ipc Elapsed  inst/cycle         0.12
    Issue Slots Busy               %         3.64
    Issued Ipc Active     inst/cycle         0.15
    SM Busy                        %        19.01
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 80.99%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ----------------- ----------- ------------
    Metric Name       Metric Unit Metric Value
    ----------------- ----------- ------------
    Memory Throughput     Gbyte/s       322.26
    Mem Busy                    %        23.84
    Max Bandwidth               %        86.50
    L1/TEX Hit Rate             %            0
    L2 Hit Rate                 %        34.05
    Mem Pipes Busy              %         7.31
    ----------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %         3.64
    Issued Warp Per Scheduler                        0.04
    No Eligible                            %        96.36
    Active Warps Per Scheduler          warp         4.32
    Eligible Warps Per Scheduler        warp         0.04
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 13.5%                                                                                     
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 27.5 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 8 warps per scheduler, this workload allocates an average of  
          4.32 active warps per scheduler, but only an average of 0.04 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp  
          State Statistics and Source Counters sections.                                                                

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle       118.71
    Warp Cycles Per Executed Instruction           cycle       121.89
    Avg. Active Threads Per Warp                                31.99
    Avg. Not Predicated Off Threads Per Warp                    30.04
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 13.5%                                                                                           
          On average, each warp of this workload spends 107.4 cycles being stalled waiting for a scoreboard dependency  
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 90.5% of the total average of 118.7 cycles       
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Profiling Guide                                                                            
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       636.09
    Executed Instructions                           inst      101,775
    Avg. Issued Instructions Per Scheduler          inst       653.12
    Issued Instructions                             inst      104,499
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     88
    Registers Per Thread             register/thread              28
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              40
    Stack Size                                                 1,024
    Threads                                   thread          22,528
    # TPCs                                                        20
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                0.55
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        53.45
    Achieved Active Warps Per SM           warp        17.10
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 13.5%                                                                                           
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (53.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       82,820
    Total DRAM Elapsed Cycles        cycle      765,952
    Average L1 Active Cycles         cycle    17,927.67
    Total L1 Elapsed Cycles          cycle      874,422
    Average L2 Active Cycles         cycle    17,101.25
    Total L2 Elapsed Cycles          cycle      794,624
    Average SM Active Cycles         cycle    17,927.67
    Total SM Elapsed Cycles          cycle      874,422
    Average SMSP Active Cycles       cycle    17,944.31
    Total SMSP Elapsed Cycles        cycle    3,497,688
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 10.7%                                                                                           
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 13.05% above the average, while the minimum instance value is 4.83% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.903%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 9.63% above the average, while the minimum instance value is 6.22% below the        
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 10.7%                                                                                           
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 13.05% above the average, while the minimum instance value is 4.83% below the       
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.07
    Branch Instructions              inst        6,907
    Branch Efficiency                   %        99.95
    Avg. Divergent Branches                       0.01
    ------------------------- ----------- ------------

  cuda_right_hand_side(const double *, double *, int, double, double, double, double) (256, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.71
    SM Frequency                    Ghz         1.59
    Elapsed Cycles                cycle      356,976
    Memory Throughput                 %         3.61
    DRAM Throughput                   %         3.61
    Duration                         us       223.78
    L1/TEX Cache Throughput           %         4.52
    L2 Cache Throughput               %         3.01
    SM Active Cycles              cycle   308,214.67
    Compute (SM) Throughput           %        75.51
    ----------------------- ----------- ------------

    OPT   Compute is more heavily utilized than Memory: Look at the Compute Workload Analysis section to see what the   
          compute pipelines are spending their time doing. Also, consider whether any computation is redundant and      
          could be reduced or moved to look-up tables.                                                                  

    Section: GPU Speed Of Light Roofline Chart
    OPT   Est. Speedup: 82.31%                                                                                          
          The ratio of peak float (FP32) to double (FP64) performance on this device is 32:1. The workload achieved     
          close to 0% of this device's FP32 peak performance and 56% of its FP64 peak performance. If Compute Workload  
          Analysis determines that this workload is FP64 bound, consider using 32-bit precision floating point          
          operations to improve its performance. See the Profiling Guide                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Kbyte       393.22
    Dropped Samples                sample            0
    Maximum Sampling Interval       cycle       20,000
    # Pass Groups                                    1
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.11
    Executed Ipc Elapsed  inst/cycle         0.09
    Issue Slots Busy               %         2.65
    Issued Ipc Active     inst/cycle         0.11
    SM Busy                        %        86.98
    -------------------- ----------- ------------

    OPT   FP64 is the highest-utilized pipeline (87.0%) based on active cycles, taking into account the rates of its    
          different instructions. It executes 64-bit floating point operations. The pipeline is over-utilized and       
          likely a performance bottleneck. Based on the number of executed instructions, the highest utilized pipeline  
          (87.0%) is FP64. It executes 64-bit floating point operations. Comparing the two, the overall pipeline        
          utilization appears to be caused by frequent, low-latency instructions. See the Profiling Guide               
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-decoder) or hover over the          
          pipeline name to understand the workloads handled by each pipeline. The Instruction Statistics section shows  
          the mix of executed instructions for this workload. Check the Warp State Statistics section for which         
          reasons cause warps to stall.                                                                                 

    Section: Memory Workload Analysis
    ----------------- ----------- ------------
    Metric Name       Metric Unit Metric Value
    ----------------- ----------- ------------
    Memory Throughput     Gbyte/s        15.50
    Mem Busy                    %         2.98
    Max Bandwidth               %         3.61
    L1/TEX Hit Rate             %        79.34
    L2 Hit Rate                 %        80.25
    Mem Pipes Busy              %        19.73
    ----------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 3.233%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 9.1 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 3.388%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32     
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %         2.65
    Issued Warp Per Scheduler                        0.03
    No Eligible                            %        97.35
    Active Warps Per Scheduler          warp         6.62
    Eligible Warps Per Scheduler        warp         0.07
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 24.49%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 37.7 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 8 warps per scheduler, this workload allocates an average of  
          6.62 active warps per scheduler, but only an average of 0.07 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle       249.65
    Warp Cycles Per Executed Instruction           cycle       250.88
    Avg. Active Threads Per Warp                                31.99
    Avg. Not Predicated Off Threads Per Warp                    30.68
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 24.49%                                                                                          
          On average, each warp of this workload spends 121.1 cycles being stalled waiting for a scoreboard dependency  
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 48.5% of the total average of 249.6 cycles       
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 24.49%                                                                                          
          On average, each warp of this workload spends 120.9 cycles being stalled waiting for the L1 instruction queue 
          for texture operations to be not full. This stall reason is high in cases of extreme utilization of the       
          L1TEX pipeline. Try issuing fewer texture fetches, surface loads, surface stores, or decoupled math           
          operations. If applicable, consider combining multiple lower-width memory operations into fewer wider memory  
          operations and try interleaving memory operations and math instructions. Consider converting texture lookups  
          or surface loads into global memory lookups. Texture can accept four threads' requests per cycle, whereas     
          global accepts 32 threads. This stall type represents about 48.4% of the total average of 249.6 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Profiling Guide                                                                            
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst     8,125.43
    Executed Instructions                           inst    1,300,068
    Avg. Issued Instructions Per Scheduler          inst     8,165.46
    Issued Instructions                             inst    1,306,474
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 16.53%                                                                                          
          This kernel executes 388846 fused and 238388 non-fused FP64 instructions. By converting pairs of non-fused    
          instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point),           
          higher-throughput equivalent, the achieved FP64 performance could be increased by up to 19% (relative to its  
          current performance).                                                                                         

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    256
    Registers Per Thread             register/thread              56
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              40
    Stack Size                                                 1,024
    Threads                                   thread          65,536
    # TPCs                                                        20
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                1.60
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 97 thread blocks.   
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 50.0% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        82.75
    Achieved Active Warps Per SM           warp        26.48
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 17.25%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (82.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    54,181.50
    Total DRAM Elapsed Cycles        cycle   12,009,472
    Average L1 Active Cycles         cycle   308,214.67
    Total L1 Elapsed Cycles          cycle   14,200,912
    Average L2 Active Cycles         cycle    42,502.72
    Total L2 Elapsed Cycles          cycle   10,928,832
    Average SM Active Cycles         cycle   308,214.67
    Total SM Elapsed Cycles          cycle   14,200,912
    Average SMSP Active Cycles       cycle   307,894.44
    Total SMSP Elapsed Cycles        cycle   56,803,648
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 11.05%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 12.73% above the average, while the minimum instance value is 1.85% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.22%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 12.94% above the average, while the minimum instance value is 1.83% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.05%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 12.73% above the average, while the minimum instance value is 1.85% below the       
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.07
    Branch Instructions              inst       88,024
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 8.576%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 421875 excessive sectors (69% of the      
          total 612171 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source         
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  cuda_aux_computation(const double *, const double *, double *, double, int, double) (88, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.58
    SM Frequency                    Ghz         1.57
    Elapsed Cycles                cycle       26,371
    Memory Throughput                 %        89.10
    DRAM Throughput                   %        89.10
    Duration                         us        16.70
    L1/TEX Cache Throughput           %        17.80
    L2 Cache Throughput               %        23.44
    SM Active Cycles              cycle    18,001.65
    Compute (SM) Throughput           %        15.45
    ----------------------- ----------- ------------

    INF   This workload is utilizing greater than 80.0% of the available compute or memory performance of the device.   
          To further improve performance, work will likely need to be shifted from the most utilized to another unit.   
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (FP32) to double (FP64) performance on this device is 32:1. The workload achieved 0%  
          of this device's FP32 peak performance and 15% of its FP64 peak performance. See the Profiling Guide          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Kbyte       393.22
    Dropped Samples                sample            0
    Maximum Sampling Interval       cycle       20,000
    # Pass Groups                                    1
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.14
    Executed Ipc Elapsed  inst/cycle         0.12
    Issue Slots Busy               %         3.63
    Issued Ipc Active     inst/cycle         0.15
    SM Busy                        %        18.93
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 81.07%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ----------------- ----------- ------------
    Metric Name       Metric Unit Metric Value
    ----------------- ----------- ------------
    Memory Throughput     Gbyte/s       318.12
    Mem Busy                    %        23.44
    Max Bandwidth               %        89.10
    L1/TEX Hit Rate             %            0
    L2 Hit Rate                 %        33.95
    Mem Pipes Busy              %         7.24
    ----------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %         3.61
    Issued Warp Per Scheduler                        0.04
    No Eligible                            %        96.39
    Active Warps Per Scheduler          warp         4.25
    Eligible Warps Per Scheduler        warp         0.04
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 10.9%                                                                                     
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 27.7 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 8 warps per scheduler, this workload allocates an average of  
          4.25 active warps per scheduler, but only an average of 0.04 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp  
          State Statistics and Source Counters sections.                                                                

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle       117.77
    Warp Cycles Per Executed Instruction           cycle       120.92
    Avg. Active Threads Per Warp                                31.99
    Avg. Not Predicated Off Threads Per Warp                    30.04
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 10.9%                                                                                           
          On average, each warp of this workload spends 107.9 cycles being stalled waiting for a scoreboard dependency  
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 91.6% of the total average of 117.8 cycles       
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Profiling Guide                                                                            
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       636.09
    Executed Instructions                           inst      101,775
    Avg. Issued Instructions Per Scheduler          inst       653.12
    Issued Instructions                             inst      104,499
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     88
    Registers Per Thread             register/thread              28
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              40
    Stack Size                                                 1,024
    Threads                                   thread          22,528
    # TPCs                                                        20
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                0.55
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        53.42
    Achieved Active Warps Per SM           warp        17.10
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 10.9%                                                                                           
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (53.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       83,030
    Total DRAM Elapsed Cycles        cycle      745,472
    Average L1 Active Cycles         cycle    18,001.65
    Total L1 Elapsed Cycles          cycle      882,556
    Average L2 Active Cycles         cycle    17,482.59
    Total L2 Elapsed Cycles          cycle      808,032
    Average SM Active Cycles         cycle    18,001.65
    Total SM Elapsed Cycles          cycle      882,556
    Average SMSP Active Cycles       cycle    18,109.56
    Total SMSP Elapsed Cycles        cycle    3,530,224
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 9.048%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 11.09% above the average, while the minimum instance value is 5.32% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 9.677%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 11.79% above the average, while the minimum instance value is 7.95% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 9.048%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 11.09% above the average, while the minimum instance value is 5.32% below the       
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.07
    Branch Instructions              inst        6,907
    Branch Efficiency                   %        99.95
    Avg. Divergent Branches                       0.01
    ------------------------- ----------- ------------

  cuda_right_hand_side(const double *, double *, int, double, double, double, double) (256, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.72
    SM Frequency                    Ghz         1.59
    Elapsed Cycles                cycle      355,711
    Memory Throughput                 %         3.61
    DRAM Throughput                   %         3.61
    Duration                         us       223.04
    L1/TEX Cache Throughput           %         4.52
    L2 Cache Throughput               %         3.02
    SM Active Cycles              cycle   308,248.65
    Compute (SM) Throughput           %        75.55
    ----------------------- ----------- ------------

    OPT   Compute is more heavily utilized than Memory: Look at the Compute Workload Analysis section to see what the   
          compute pipelines are spending their time doing. Also, consider whether any computation is redundant and      
          could be reduced or moved to look-up tables.                                                                  

    Section: GPU Speed Of Light Roofline Chart
    OPT   Est. Speedup: 82.3%                                                                                           
          The ratio of peak float (FP32) to double (FP64) performance on this device is 32:1. The workload achieved     
          close to 0% of this device's FP32 peak performance and 56% of its FP64 peak performance. If Compute Workload  
          Analysis determines that this workload is FP64 bound, consider using 32-bit precision floating point          
          operations to improve its performance. See the Profiling Guide                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Kbyte       393.22
    Dropped Samples                sample            0
    Maximum Sampling Interval       cycle       20,000
    # Pass Groups                                    1
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.11
    Executed Ipc Elapsed  inst/cycle         0.09
    Issue Slots Busy               %         2.65
    Issued Ipc Active     inst/cycle         0.11
    SM Busy                        %        86.97
    -------------------- ----------- ------------

    OPT   FP64 is the highest-utilized pipeline (87.0%) based on active cycles, taking into account the rates of its    
          different instructions. It executes 64-bit floating point operations. The pipeline is over-utilized and       
          likely a performance bottleneck. Based on the number of executed instructions, the highest utilized pipeline  
          (87.0%) is FP64. It executes 64-bit floating point operations. Comparing the two, the overall pipeline        
          utilization appears to be caused by frequent, low-latency instructions. See the Profiling Guide               
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-decoder) or hover over the          
          pipeline name to understand the workloads handled by each pipeline. The Instruction Statistics section shows  
          the mix of executed instructions for this workload. Check the Warp State Statistics section for which         
          reasons cause warps to stall.                                                                                 

    Section: Memory Workload Analysis
    ----------------- ----------- ------------
    Metric Name       Metric Unit Metric Value
    ----------------- ----------- ------------
    Memory Throughput     Gbyte/s        15.51
    Mem Busy                    %         2.98
    Max Bandwidth               %         3.61
    L1/TEX Hit Rate             %        79.21
    L2 Hit Rate                 %        80.59
    Mem Pipes Busy              %        19.74
    ----------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 3.236%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 9.1 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 3.391%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32     
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %         2.65
    Issued Warp Per Scheduler                        0.03
    No Eligible                            %        97.35
    Active Warps Per Scheduler          warp         6.63
    Eligible Warps Per Scheduler        warp         0.07
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 24.45%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 37.7 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 8 warps per scheduler, this workload allocates an average of  
          6.63 active warps per scheduler, but only an average of 0.07 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle       249.98
    Warp Cycles Per Executed Instruction           cycle       251.21
    Avg. Active Threads Per Warp                                31.99
    Avg. Not Predicated Off Threads Per Warp                    30.66
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 24.45%                                                                                          
          On average, each warp of this workload spends 121.1 cycles being stalled waiting for a scoreboard dependency  
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 48.5% of the total average of 250.0 cycles       
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 24.45%                                                                                          
          On average, each warp of this workload spends 120.7 cycles being stalled waiting for the L1 instruction queue 
          for texture operations to be not full. This stall reason is high in cases of extreme utilization of the       
          L1TEX pipeline. Try issuing fewer texture fetches, surface loads, surface stores, or decoupled math           
          operations. If applicable, consider combining multiple lower-width memory operations into fewer wider memory  
          operations and try interleaving memory operations and math instructions. Consider converting texture lookups  
          or surface loads into global memory lookups. Texture can accept four threads' requests per cycle, whereas     
          global accepts 32 threads. This stall type represents about 48.3% of the total average of 250.0 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Profiling Guide                                                                            
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst     8,125.43
    Executed Instructions                           inst    1,300,068
    Avg. Issued Instructions Per Scheduler          inst     8,165.43
    Issued Instructions                             inst    1,306,469
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 16.53%                                                                                          
          This kernel executes 388846 fused and 238388 non-fused FP64 instructions. By converting pairs of non-fused    
          instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point),           
          higher-throughput equivalent, the achieved FP64 performance could be increased by up to 19% (relative to its  
          current performance).                                                                                         

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    256
    Registers Per Thread             register/thread              56
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              40
    Stack Size                                                 1,024
    Threads                                   thread          65,536
    # TPCs                                                        20
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                1.60
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 97 thread blocks.   
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 50.0% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        82.70
    Achieved Active Warps Per SM           warp        26.46
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 17.3%                                                                                           
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (82.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    54,067.50
    Total DRAM Elapsed Cycles        cycle   11,984,896
    Average L1 Active Cycles         cycle   308,248.65
    Total L1 Elapsed Cycles          cycle   14,193,094
    Average L2 Active Cycles         cycle    43,012.38
    Total L2 Elapsed Cycles          cycle   10,895,008
    Average SM Active Cycles         cycle   308,248.65
    Total SM Elapsed Cycles          cycle   14,193,094
    Average SMSP Active Cycles       cycle   307,978.93
    Total SMSP Elapsed Cycles        cycle   56,772,376
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 11.09%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 12.76% above the average, while the minimum instance value is 1.91% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.14%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 12.83% above the average, while the minimum instance value is 1.89% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.09%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 12.76% above the average, while the minimum instance value is 1.91% below the       
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.07
    Branch Instructions              inst       88,024
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 8.706%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 421875 excessive sectors (69% of the      
          total 612171 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source         
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  cuda_final_update(double *, const double *, const double *, const double *, const double *, double, int) (88, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.10
    SM Frequency                    Ghz         1.59
    Elapsed Cycles                cycle       56,101
    Memory Throughput                 %        89.74
    DRAM Throughput                   %        89.74
    Duration                         us        35.17
    L1/TEX Cache Throughput           %        17.33
    L2 Cache Throughput               %        21.96
    SM Active Cycles              cycle    45,081.78
    Compute (SM) Throughput           %        25.69
    ----------------------- ----------- ------------

    INF   This workload is utilizing greater than 80.0% of the available compute or memory performance of the device.   
          To further improve performance, work will likely need to be shifted from the most utilized to another unit.   
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: GPU Speed Of Light Roofline Chart
    OPT   Est. Speedup: 27.48%                                                                                          
          The ratio of peak float (FP32) to double (FP64) performance on this device is 32:1. The workload achieved 0%  
          of this device's FP32 peak performance and 22% of its FP64 peak performance. If Compute Workload Analysis     
          determines that this workload is FP64 bound, consider using 32-bit precision floating point operations to     
          improve its performance. See the Profiling Guide                                                              
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Kbyte       393.22
    Dropped Samples                sample            0
    Maximum Sampling Interval       cycle       20,000
    # Pass Groups                                    1
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.09
    Executed Ipc Elapsed  inst/cycle         0.08
    Issue Slots Busy               %         2.34
    Issued Ipc Active     inst/cycle         0.09
    SM Busy                        %        28.36
    -------------------- ----------- ------------

    INF   FP64 is the highest-utilized pipeline (28.4%) based on active cycles, taking into account the rates of its    
          different instructions. It executes 64-bit floating point operations. It is well-utilized, but should not be  
          a bottleneck.                                                                                                 

    Section: Memory Workload Analysis
    ----------------- ----------- ------------
    Metric Name       Metric Unit Metric Value
    ----------------- ----------- ------------
    Memory Throughput     Gbyte/s       350.36
    Mem Busy                    %        21.96
    Max Bandwidth               %        89.74
    L1/TEX Hit Rate             %        14.12
    L2 Hit Rate                 %        17.07
    Mem Pipes Busy              %         7.92
    ----------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %         2.34
    Issued Warp Per Scheduler                        0.02
    No Eligible                            %        97.66
    Active Warps Per Scheduler          warp         4.29
    Eligible Warps Per Scheduler        warp         0.03
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 10.26%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 42.8 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 8 warps per scheduler, this workload allocates an average of  
          4.29 active warps per scheduler, but only an average of 0.03 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp  
          State Statistics and Source Counters sections.                                                                

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle       183.77
    Warp Cycles Per Executed Instruction           cycle       187.10
    Avg. Active Threads Per Warp                                31.99
    Avg. Not Predicated Off Threads Per Warp                    30.79
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 10.26%                                                                                          
          On average, each warp of this workload spends 172.7 cycles being stalled waiting for a scoreboard dependency  
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 94.0% of the total average of 183.8 cycles       
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Profiling Guide                                                                            
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst     1,034.84
    Executed Instructions                           inst      165,575
    Avg. Issued Instructions Per Scheduler          inst     1,053.56
    Issued Instructions                             inst      168,570
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 3.78%                                                                                           
          This kernel executes 23448 fused and 8520 non-fused FP64 instructions. By converting pairs of non-fused       
          instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point),           
          higher-throughput equivalent, the achieved FP64 performance could be increased by up to 13% (relative to its  
          current performance).                                                                                         

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     88
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              40
    Stack Size                                                 1,024
    Threads                                   thread          22,528
    # TPCs                                                        20
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                0.55
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        53.31
    Achieved Active Warps Per SM           warp        17.06
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 10.26%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (53.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      192,522
    Total DRAM Elapsed Cycles        cycle    1,716,224
    Average L1 Active Cycles         cycle    45,081.78
    Total L1 Elapsed Cycles          cycle    1,991,310
    Average L2 Active Cycles         cycle    43,688.16
    Total L2 Elapsed Cycles          cycle    1,717,376
    Average SM Active Cycles         cycle    45,081.78
    Total SM Elapsed Cycles          cycle    1,991,310
    Average SMSP Active Cycles       cycle    45,112.49
    Total SMSP Elapsed Cycles        cycle    7,965,240
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.39%                                                                                           
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 5.95% above the average, while the minimum instance value is 3.02% below the average.       
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.915%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 6.53% above the average, while the minimum instance value is 3.79% below the        
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.39%                                                                                           
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 5.95% above the average, while the minimum instance value is 3.02% below the        
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.04
    Branch Instructions              inst        6,907
    Branch Efficiency                   %        99.95
    Avg. Divergent Branches                       0.01
    ------------------------- ----------- ------------

  cuda_right_hand_side(const double *, double *, int, double, double, double, double) (256, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.86
    SM Frequency                    Ghz         1.60
    Elapsed Cycles                cycle      355,519
    Memory Throughput                 %         3.60
    DRAM Throughput                   %         3.60
    Duration                         us       220.86
    L1/TEX Cache Throughput           %         4.52
    L2 Cache Throughput               %         3.02
    SM Active Cycles              cycle   308,177.67
    Compute (SM) Throughput           %        75.58
    ----------------------- ----------- ------------

    OPT   Compute is more heavily utilized than Memory: Look at the Compute Workload Analysis section to see what the   
          compute pipelines are spending their time doing. Also, consider whether any computation is redundant and      
          could be reduced or moved to look-up tables.                                                                  

    Section: GPU Speed Of Light Roofline Chart
    OPT   Est. Speedup: 82.32%                                                                                          
          The ratio of peak float (FP32) to double (FP64) performance on this device is 32:1. The workload achieved     
          close to 0% of this device's FP32 peak performance and 56% of its FP64 peak performance. If Compute Workload  
          Analysis determines that this workload is FP64 bound, consider using 32-bit precision floating point          
          operations to improve its performance. See the Profiling Guide                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Kbyte       393.22
    Dropped Samples                sample            0
    Maximum Sampling Interval       cycle       20,000
    # Pass Groups                                    1
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.11
    Executed Ipc Elapsed  inst/cycle         0.09
    Issue Slots Busy               %         2.65
    Issued Ipc Active     inst/cycle         0.11
    SM Busy                        %        86.99
    -------------------- ----------- ------------

    OPT   FP64 is the highest-utilized pipeline (87.0%) based on active cycles, taking into account the rates of its    
          different instructions. It executes 64-bit floating point operations. The pipeline is over-utilized and       
          likely a performance bottleneck. Based on the number of executed instructions, the highest utilized pipeline  
          (87.0%) is FP64. It executes 64-bit floating point operations. Comparing the two, the overall pipeline        
          utilization appears to be caused by frequent, low-latency instructions. See the Profiling Guide               
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-decoder) or hover over the          
          pipeline name to understand the workloads handled by each pipeline. The Instruction Statistics section shows  
          the mix of executed instructions for this workload. Check the Warp State Statistics section for which         
          reasons cause warps to stall.                                                                                 

    Section: Memory Workload Analysis
    ----------------- ----------- ------------
    Metric Name       Metric Unit Metric Value
    ----------------- ----------- ------------
    Memory Throughput     Gbyte/s        15.82
    Mem Busy                    %         3.02
    Max Bandwidth               %         3.60
    L1/TEX Hit Rate             %        79.28
    L2 Hit Rate                 %        80.12
    Mem Pipes Busy              %        19.75
    ----------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 3.236%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 9.1 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 3.391%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32     
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %         2.65
    Issued Warp Per Scheduler                        0.03
    No Eligible                            %        97.35
    Active Warps Per Scheduler          warp         6.63
    Eligible Warps Per Scheduler        warp         0.07
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 24.42%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 37.7 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 8 warps per scheduler, this workload allocates an average of  
          6.63 active warps per scheduler, but only an average of 0.07 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle       249.93
    Warp Cycles Per Executed Instruction           cycle       251.16
    Avg. Active Threads Per Warp                                31.99
    Avg. Not Predicated Off Threads Per Warp                    30.66
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 24.42%                                                                                          
          On average, each warp of this workload spends 121.1 cycles being stalled waiting for a scoreboard dependency  
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 48.5% of the total average of 249.9 cycles       
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 24.42%                                                                                          
          On average, each warp of this workload spends 120.9 cycles being stalled waiting for the L1 instruction queue 
          for texture operations to be not full. This stall reason is high in cases of extreme utilization of the       
          L1TEX pipeline. Try issuing fewer texture fetches, surface loads, surface stores, or decoupled math           
          operations. If applicable, consider combining multiple lower-width memory operations into fewer wider memory  
          operations and try interleaving memory operations and math instructions. Consider converting texture lookups  
          or surface loads into global memory lookups. Texture can accept four threads' requests per cycle, whereas     
          global accepts 32 threads. This stall type represents about 48.4% of the total average of 249.9 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Profiling Guide                                                                            
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst     8,125.43
    Executed Instructions                           inst    1,300,068
    Avg. Issued Instructions Per Scheduler          inst     8,165.52
    Issued Instructions                             inst    1,306,484
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 16.53%                                                                                          
          This kernel executes 388846 fused and 238388 non-fused FP64 instructions. By converting pairs of non-fused    
          instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point),           
          higher-throughput equivalent, the achieved FP64 performance could be increased by up to 19% (relative to its  
          current performance).                                                                                         

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    256
    Registers Per Thread             register/thread              56
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              40
    Stack Size                                                 1,024
    Threads                                   thread          65,536
    # TPCs                                                        20
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                1.60
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 97 thread blocks.   
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 50.0% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        82.75
    Achieved Active Warps Per SM           warp        26.48
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 17.25%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (82.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    54,588.50
    Total DRAM Elapsed Cycles        cycle   12,128,256
    Average L1 Active Cycles         cycle   308,177.67
    Total L1 Elapsed Cycles          cycle   14,188,690
    Average L2 Active Cycles         cycle    46,821.72
    Total L2 Elapsed Cycles          cycle   10,807,200
    Average SM Active Cycles         cycle   308,177.67
    Total SM Elapsed Cycles          cycle   14,188,690
    Average SMSP Active Cycles       cycle   307,917.47
    Total SMSP Elapsed Cycles        cycle   56,754,760
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 11.08%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 12.75% above the average, while the minimum instance value is 1.90% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.12%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 12.82% above the average, while the minimum instance value is 1.89% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.08%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 12.75% above the average, while the minimum instance value is 1.90% below the       
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.07
    Branch Instructions              inst       88,024
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 9.554%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 421875 excessive sectors (69% of the      
          total 612171 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source         
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  cuda_aux_computation(const double *, const double *, double *, double, int, double) (88, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.72
    SM Frequency                    Ghz         1.58
    Elapsed Cycles                cycle       25,759
    Memory Throughput                 %        87.63
    DRAM Throughput                   %        87.63
    Duration                         us        16.29
    L1/TEX Cache Throughput           %        18.16
    L2 Cache Throughput               %        24.06
    SM Active Cycles              cycle    18,129.92
    Compute (SM) Throughput           %        15.74
    ----------------------- ----------- ------------

    INF   This workload is utilizing greater than 80.0% of the available compute or memory performance of the device.   
          To further improve performance, work will likely need to be shifted from the most utilized to another unit.   
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: GPU Speed Of Light Roofline Chart
    OPT   Est. Speedup: 18.21%                                                                                          
          The ratio of peak float (FP32) to double (FP64) performance on this device is 32:1. The workload achieved 0%  
          of this device's FP32 peak performance and 15% of its FP64 peak performance. If Compute Workload Analysis     
          determines that this workload is FP64 bound, consider using 32-bit precision floating point operations to     
          improve its performance. See the Profiling Guide                                                              
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Kbyte       393.22
    Dropped Samples                sample            0
    Maximum Sampling Interval       cycle       20,000
    # Pass Groups                                    1
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.14
    Executed Ipc Elapsed  inst/cycle         0.12
    Issue Slots Busy               %         3.60
    Issued Ipc Active     inst/cycle         0.14
    SM Busy                        %        18.80
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 81.2%                                                                                     
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ----------------- ----------- ------------
    Metric Name       Metric Unit Metric Value
    ----------------- ----------- ------------
    Memory Throughput     Gbyte/s       320.86
    Mem Busy                    %        24.06
    Max Bandwidth               %        87.63
    L1/TEX Hit Rate             %            0
    L2 Hit Rate                 %        33.98
    Mem Pipes Busy              %         7.38
    ----------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %         3.64
    Issued Warp Per Scheduler                        0.04
    No Eligible                            %        96.36
    Active Warps Per Scheduler          warp         4.30
    Eligible Warps Per Scheduler        warp         0.05
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 12.37%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 27.5 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 8 warps per scheduler, this workload allocates an average of  
          4.30 active warps per scheduler, but only an average of 0.05 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp  
          State Statistics and Source Counters sections.                                                                

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle       117.99
    Warp Cycles Per Executed Instruction           cycle       121.15
    Avg. Active Threads Per Warp                                31.99
    Avg. Not Predicated Off Threads Per Warp                    30.04
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 12.37%                                                                                          
          On average, each warp of this workload spends 107.5 cycles being stalled waiting for a scoreboard dependency  
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 91.1% of the total average of 118.0 cycles       
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Profiling Guide                                                                            
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       636.09
    Executed Instructions                           inst      101,775
    Avg. Issued Instructions Per Scheduler          inst       653.13
    Issued Instructions                             inst      104,501
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     88
    Registers Per Thread             register/thread              28
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              40
    Stack Size                                                 1,024
    Threads                                   thread          22,528
    # TPCs                                                        20
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                0.55
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        52.90
    Achieved Active Warps Per SM           warp        16.93
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 12.37%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (52.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       81,659
    Total DRAM Elapsed Cycles        cycle      745,472
    Average L1 Active Cycles         cycle    18,129.92
    Total L1 Elapsed Cycles          cycle      866,298
    Average L2 Active Cycles         cycle    17,065.84
    Total L2 Elapsed Cycles          cycle      789,088
    Average SM Active Cycles         cycle    18,129.92
    Total SM Elapsed Cycles          cycle      866,298
    Average SMSP Active Cycles       cycle    17,929.99
    Total SMSP Elapsed Cycles        cycle    3,465,192
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 8.34%                                                                                           
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 9.96% above the average, while the minimum instance value is 5.53% below the average.       
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 9.114%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 11.01% above the average, while the minimum instance value is 4.93% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.34%                                                                                           
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 9.96% above the average, while the minimum instance value is 5.53% below the        
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.07
    Branch Instructions              inst        6,907
    Branch Efficiency                   %        99.95
    Avg. Divergent Branches                       0.01
    ------------------------- ----------- ------------

  cuda_right_hand_side(const double *, double *, int, double, double, double, double) (256, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.80
    SM Frequency                    Ghz         1.60
    Elapsed Cycles                cycle      355,731
    Memory Throughput                 %         3.60
    DRAM Throughput                   %         3.60
    Duration                         us       220.90
    L1/TEX Cache Throughput           %         4.52
    L2 Cache Throughput               %         3.05
    SM Active Cycles              cycle   308,282.55
    Compute (SM) Throughput           %        75.59
    ----------------------- ----------- ------------

    OPT   Compute is more heavily utilized than Memory: Look at the Compute Workload Analysis section to see what the   
          compute pipelines are spending their time doing. Also, consider whether any computation is redundant and      
          could be reduced or moved to look-up tables.                                                                  

    Section: GPU Speed Of Light Roofline Chart
    OPT   Est. Speedup: 82.29%                                                                                          
          The ratio of peak float (FP32) to double (FP64) performance on this device is 32:1. The workload achieved     
          close to 0% of this device's FP32 peak performance and 56% of its FP64 peak performance. If Compute Workload  
          Analysis determines that this workload is FP64 bound, consider using 32-bit precision floating point          
          operations to improve its performance. See the Profiling Guide                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Kbyte       393.22
    Dropped Samples                sample            0
    Maximum Sampling Interval       cycle       20,000
    # Pass Groups                                    1
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.11
    Executed Ipc Elapsed  inst/cycle         0.09
    Issue Slots Busy               %         2.65
    Issued Ipc Active     inst/cycle         0.11
    SM Busy                        %        86.96
    -------------------- ----------- ------------

    OPT   FP64 is the highest-utilized pipeline (87.0%) based on active cycles, taking into account the rates of its    
          different instructions. It executes 64-bit floating point operations. The pipeline is over-utilized and       
          likely a performance bottleneck. Based on the number of executed instructions, the highest utilized pipeline  
          (87.0%) is FP64. It executes 64-bit floating point operations. Comparing the two, the overall pipeline        
          utilization appears to be caused by frequent, low-latency instructions. See the Profiling Guide               
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-decoder) or hover over the          
          pipeline name to understand the workloads handled by each pipeline. The Instruction Statistics section shows  
          the mix of executed instructions for this workload. Check the Warp State Statistics section for which         
          reasons cause warps to stall.                                                                                 

    Section: Memory Workload Analysis
    ----------------- ----------- ------------
    Metric Name       Metric Unit Metric Value
    ----------------- ----------- ------------
    Memory Throughput     Gbyte/s        15.68
    Mem Busy                    %         3.01
    Max Bandwidth               %         3.60
    L1/TEX Hit Rate             %        79.32
    L2 Hit Rate                 %        80.66
    Mem Pipes Busy              %        19.75
    ----------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 3.237%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 9.1 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 3.392%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32     
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %         2.65
    Issued Warp Per Scheduler                        0.03
    No Eligible                            %        97.35
    Active Warps Per Scheduler          warp         6.63
    Eligible Warps Per Scheduler        warp         0.07
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 24.41%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 37.7 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 8 warps per scheduler, this workload allocates an average of  
          6.63 active warps per scheduler, but only an average of 0.07 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle       249.83
    Warp Cycles Per Executed Instruction           cycle       251.07
    Avg. Active Threads Per Warp                                31.99
    Avg. Not Predicated Off Threads Per Warp                    30.66
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 24.41%                                                                                          
          On average, each warp of this workload spends 121.1 cycles being stalled waiting for a scoreboard dependency  
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 48.5% of the total average of 249.8 cycles       
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 24.41%                                                                                          
          On average, each warp of this workload spends 120.8 cycles being stalled waiting for the L1 instruction queue 
          for texture operations to be not full. This stall reason is high in cases of extreme utilization of the       
          L1TEX pipeline. Try issuing fewer texture fetches, surface loads, surface stores, or decoupled math           
          operations. If applicable, consider combining multiple lower-width memory operations into fewer wider memory  
          operations and try interleaving memory operations and math instructions. Consider converting texture lookups  
          or surface loads into global memory lookups. Texture can accept four threads' requests per cycle, whereas     
          global accepts 32 threads. This stall type represents about 48.3% of the total average of 249.8 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Profiling Guide                                                                            
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst     8,125.43
    Executed Instructions                           inst    1,300,068
    Avg. Issued Instructions Per Scheduler          inst     8,165.45
    Issued Instructions                             inst    1,306,472
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 16.53%                                                                                          
          This kernel executes 388846 fused and 238388 non-fused FP64 instructions. By converting pairs of non-fused    
          instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point),           
          higher-throughput equivalent, the achieved FP64 performance could be increased by up to 19% (relative to its  
          current performance).                                                                                         

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    256
    Registers Per Thread             register/thread              56
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              40
    Stack Size                                                 1,024
    Threads                                   thread          65,536
    # TPCs                                                        20
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                1.60
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 97 thread blocks.   
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 50.0% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        82.71
    Achieved Active Warps Per SM           warp        26.47
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 17.29%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (82.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       54,121
    Total DRAM Elapsed Cycles        cycle   12,013,568
    Average L1 Active Cycles         cycle   308,282.55
    Total L1 Elapsed Cycles          cycle   14,187,152
    Average L2 Active Cycles         cycle    41,935.03
    Total L2 Elapsed Cycles          cycle   10,809,024
    Average SM Active Cycles         cycle   308,282.55
    Total SM Elapsed Cycles          cycle   14,187,152
    Average SMSP Active Cycles       cycle   307,863.55
    Total SMSP Elapsed Cycles        cycle   56,748,608
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 11.1%                                                                                           
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 12.77% above the average, while the minimum instance value is 1.88% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.1%                                                                                           
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 12.79% above the average, while the minimum instance value is 1.82% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.1%                                                                                           
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 12.77% above the average, while the minimum instance value is 1.88% below the       
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.07
    Branch Instructions              inst       88,024
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 8.556%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 421875 excessive sectors (69% of the      
          total 612171 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source         
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  cuda_aux_computation(const double *, const double *, double *, double, int, double) (88, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.72
    SM Frequency                    Ghz         1.58
    Elapsed Cycles                cycle       25,636
    Memory Throughput                 %        89.64
    DRAM Throughput                   %        89.64
    Duration                         us        16.19
    L1/TEX Cache Throughput           %        18.00
    L2 Cache Throughput               %        24.17
    SM Active Cycles              cycle       17,999
    Compute (SM) Throughput           %        15.61
    ----------------------- ----------- ------------

    INF   This workload is utilizing greater than 80.0% of the available compute or memory performance of the device.   
          To further improve performance, work will likely need to be shifted from the most utilized to another unit.   
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (FP32) to double (FP64) performance on this device is 32:1. The workload achieved 0%  
          of this device's FP32 peak performance and 15% of its FP64 peak performance. See the Profiling Guide          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Kbyte       393.22
    Dropped Samples                sample            0
    Maximum Sampling Interval       cycle       20,000
    # Pass Groups                                    1
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.14
    Executed Ipc Elapsed  inst/cycle         0.12
    Issue Slots Busy               %         3.63
    Issued Ipc Active     inst/cycle         0.15
    SM Busy                        %        18.93
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 81.07%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ----------------- ----------- ------------
    Metric Name       Metric Unit Metric Value
    ----------------- ----------- ------------
    Memory Throughput     Gbyte/s       328.36
    Mem Busy                    %        24.17
    Max Bandwidth               %        89.64
    L1/TEX Hit Rate             %            0
    L2 Hit Rate                 %        33.94
    Mem Pipes Busy              %         7.32
    ----------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %         3.57
    Issued Warp Per Scheduler                        0.04
    No Eligible                            %        96.43
    Active Warps Per Scheduler          warp         4.24
    Eligible Warps Per Scheduler        warp         0.04
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 10.36%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 28.0 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 8 warps per scheduler, this workload allocates an average of  
          4.24 active warps per scheduler, but only an average of 0.04 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp  
          State Statistics and Source Counters sections.                                                                

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle       118.82
    Warp Cycles Per Executed Instruction           cycle       122.00
    Avg. Active Threads Per Warp                                31.99
    Avg. Not Predicated Off Threads Per Warp                    30.04
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 10.36%                                                                                          
          On average, each warp of this workload spends 107.9 cycles being stalled waiting for a scoreboard dependency  
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 90.8% of the total average of 118.8 cycles       
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Profiling Guide                                                                            
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       636.09
    Executed Instructions                           inst      101,775
    Avg. Issued Instructions Per Scheduler          inst       653.09
    Issued Instructions                             inst      104,495
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     88
    Registers Per Thread             register/thread              28
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              40
    Stack Size                                                 1,024
    Threads                                   thread          22,528
    # TPCs                                                        20
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                0.55
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        53.42
    Achieved Active Warps Per SM           warp        17.09
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 10.36%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (53.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       83,074
    Total DRAM Elapsed Cycles        cycle      741,376
    Average L1 Active Cycles         cycle       17,999
    Total L1 Elapsed Cycles          cycle      873,306
    Average L2 Active Cycles         cycle    17,403.62
    Total L2 Elapsed Cycles          cycle      783,680
    Average SM Active Cycles         cycle       17,999
    Total SM Elapsed Cycles          cycle      873,306
    Average SMSP Active Cycles       cycle    18,308.61
    Total SMSP Elapsed Cycles        cycle    3,493,224
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 8.559%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 10.38% above the average, while the minimum instance value is 4.67% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 9.443%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 11.26% above the average, while the minimum instance value is 7.10% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.559%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 10.38% above the average, while the minimum instance value is 4.67% below the       
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.07
    Branch Instructions              inst        6,907
    Branch Efficiency                   %        99.95
    Avg. Divergent Branches                       0.01
    ------------------------- ----------- ------------

  cuda_right_hand_side(const double *, double *, int, double, double, double, double) (256, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.72
    SM Frequency                    Ghz         1.59
    Elapsed Cycles                cycle      356,289
    Memory Throughput                 %         3.97
    DRAM Throughput                   %         3.97
    Duration                         us       223.07
    L1/TEX Cache Throughput           %         4.52
    L2 Cache Throughput               %         3.02
    SM Active Cycles              cycle   308,220.25
    Compute (SM) Throughput           %        75.50
    ----------------------- ----------- ------------

    OPT   Compute is more heavily utilized than Memory: Look at the Compute Workload Analysis section to see what the   
          compute pipelines are spending their time doing. Also, consider whether any computation is redundant and      
          could be reduced or moved to look-up tables.                                                                  

    Section: GPU Speed Of Light Roofline Chart
    OPT   Est. Speedup: 82.31%                                                                                          
          The ratio of peak float (FP32) to double (FP64) performance on this device is 32:1. The workload achieved     
          close to 0% of this device's FP32 peak performance and 56% of its FP64 peak performance. If Compute Workload  
          Analysis determines that this workload is FP64 bound, consider using 32-bit precision floating point          
          operations to improve its performance. See the Profiling Guide                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Kbyte       393.22
    Dropped Samples                sample            0
    Maximum Sampling Interval       cycle       20,000
    # Pass Groups                                    1
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.11
    Executed Ipc Elapsed  inst/cycle         0.09
    Issue Slots Busy               %         2.65
    Issued Ipc Active     inst/cycle         0.11
    SM Busy                        %        86.98
    -------------------- ----------- ------------

    OPT   FP64 is the highest-utilized pipeline (87.0%) based on active cycles, taking into account the rates of its    
          different instructions. It executes 64-bit floating point operations. The pipeline is over-utilized and       
          likely a performance bottleneck. Based on the number of executed instructions, the highest utilized pipeline  
          (87.0%) is FP64. It executes 64-bit floating point operations. Comparing the two, the overall pipeline        
          utilization appears to be caused by frequent, low-latency instructions. See the Profiling Guide               
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-decoder) or hover over the          
          pipeline name to understand the workloads handled by each pipeline. The Instruction Statistics section shows  
          the mix of executed instructions for this workload. Check the Warp State Statistics section for which         
          reasons cause warps to stall.                                                                                 

    Section: Memory Workload Analysis
    ----------------- ----------- ------------
    Metric Name       Metric Unit Metric Value
    ----------------- ----------- ------------
    Memory Throughput     Gbyte/s        17.10
    Mem Busy                    %         2.99
    Max Bandwidth               %         3.97
    L1/TEX Hit Rate             %        79.19
    L2 Hit Rate                 %        80.37
    Mem Pipes Busy              %        19.73
    ----------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 3.233%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 9.1 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 3.388%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32     
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %         2.65
    Issued Warp Per Scheduler                        0.03
    No Eligible                            %        97.35
    Active Warps Per Scheduler          warp         6.63
    Eligible Warps Per Scheduler        warp         0.07
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 24.5%                                                                                     
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 37.7 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 8 warps per scheduler, this workload allocates an average of  
          6.63 active warps per scheduler, but only an average of 0.07 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle       249.95
    Warp Cycles Per Executed Instruction           cycle       251.18
    Avg. Active Threads Per Warp                                31.99
    Avg. Not Predicated Off Threads Per Warp                    30.66
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 24.5%                                                                                           
          On average, each warp of this workload spends 121.2 cycles being stalled waiting for a scoreboard dependency  
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 48.5% of the total average of 250.0 cycles       
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 24.5%                                                                                           
          On average, each warp of this workload spends 120.9 cycles being stalled waiting for the L1 instruction queue 
          for texture operations to be not full. This stall reason is high in cases of extreme utilization of the       
          L1TEX pipeline. Try issuing fewer texture fetches, surface loads, surface stores, or decoupled math           
          operations. If applicable, consider combining multiple lower-width memory operations into fewer wider memory  
          operations and try interleaving memory operations and math instructions. Consider converting texture lookups  
          or surface loads into global memory lookups. Texture can accept four threads' requests per cycle, whereas     
          global accepts 32 threads. This stall type represents about 48.4% of the total average of 250.0 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Profiling Guide                                                                            
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst     8,125.43
    Executed Instructions                           inst    1,300,068
    Avg. Issued Instructions Per Scheduler          inst     8,165.46
    Issued Instructions                             inst    1,306,473
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 16.53%                                                                                          
          This kernel executes 388846 fused and 238388 non-fused FP64 instructions. By converting pairs of non-fused    
          instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point),           
          higher-throughput equivalent, the achieved FP64 performance could be increased by up to 19% (relative to its  
          current performance).                                                                                         

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    256
    Registers Per Thread             register/thread              56
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              40
    Stack Size                                                 1,024
    Threads                                   thread          65,536
    # TPCs                                                        20
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                1.60
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 97 thread blocks.   
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 50.0% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        82.77
    Achieved Active Warps Per SM           warp        26.49
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 17.23%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (82.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       59,594
    Total DRAM Elapsed Cycles        cycle   11,999,232
    Average L1 Active Cycles         cycle   308,220.25
    Total L1 Elapsed Cycles          cycle   14,204,122
    Average L2 Active Cycles         cycle    42,676.88
    Total L2 Elapsed Cycles          cycle   10,898,208
    Average SM Active Cycles         cycle   308,220.25
    Total SM Elapsed Cycles          cycle   14,204,122
    Average SMSP Active Cycles       cycle   307,938.67
    Total SMSP Elapsed Cycles        cycle   56,816,488
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 11.09%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 12.77% above the average, while the minimum instance value is 2.00% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.19%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 12.90% above the average, while the minimum instance value is 1.87% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.09%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 12.77% above the average, while the minimum instance value is 2.00% below the       
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.07
    Branch Instructions              inst       88,024
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 8.636%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 421875 excessive sectors (69% of the      
          total 612171 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source         
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  cuda_aux_computation(const double *, const double *, double *, double, int, double) (88, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.76
    SM Frequency                    Ghz         1.58
    Elapsed Cycles                cycle       25,974
    Memory Throughput                 %        87.64
    DRAM Throughput                   %        87.64
    Duration                         us        16.42
    L1/TEX Cache Throughput           %        17.86
    L2 Cache Throughput               %        23.85
    SM Active Cycles              cycle    18,022.95
    Compute (SM) Throughput           %        15.48
    ----------------------- ----------- ------------

    INF   This workload is utilizing greater than 80.0% of the available compute or memory performance of the device.   
          To further improve performance, work will likely need to be shifted from the most utilized to another unit.   
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (FP32) to double (FP64) performance on this device is 32:1. The workload achieved 0%  
          of this device's FP32 peak performance and 15% of its FP64 peak performance. See the Profiling Guide          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Kbyte       393.22
    Dropped Samples                sample            0
    Maximum Sampling Interval       cycle       20,000
    # Pass Groups                                    1
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.14
    Executed Ipc Elapsed  inst/cycle         0.12
    Issue Slots Busy               %         3.62
    Issued Ipc Active     inst/cycle         0.14
    SM Busy                        %        18.91
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 81.09%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ----------------- ----------- ------------
    Metric Name       Metric Unit Metric Value
    ----------------- ----------- ------------
    Memory Throughput     Gbyte/s       323.21
    Mem Busy                    %        23.85
    Max Bandwidth               %        87.64
    L1/TEX Hit Rate             %            0
    L2 Hit Rate                 %        33.95
    Mem Pipes Busy              %         7.26
    ----------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %         3.63
    Issued Warp Per Scheduler                        0.04
    No Eligible                            %        96.37
    Active Warps Per Scheduler          warp         4.31
    Eligible Warps Per Scheduler        warp         0.04
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 12.36%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 27.6 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 8 warps per scheduler, this workload allocates an average of  
          4.31 active warps per scheduler, but only an average of 0.04 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp  
          State Statistics and Source Counters sections.                                                                

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle       118.91
    Warp Cycles Per Executed Instruction           cycle       122.09
    Avg. Active Threads Per Warp                                31.99
    Avg. Not Predicated Off Threads Per Warp                    30.04
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 12.36%                                                                                          
          On average, each warp of this workload spends 107.7 cycles being stalled waiting for a scoreboard dependency  
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 90.6% of the total average of 118.9 cycles       
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Profiling Guide                                                                            
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       636.09
    Executed Instructions                           inst      101,775
    Avg. Issued Instructions Per Scheduler          inst       653.11
    Issued Instructions                             inst      104,497
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     88
    Registers Per Thread             register/thread              28
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              40
    Stack Size                                                 1,024
    Threads                                   thread          22,528
    # TPCs                                                        20
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                0.55
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        53.28
    Achieved Active Warps Per SM           warp        17.05
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 12.36%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (53.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    82,902.50
    Total DRAM Elapsed Cycles        cycle      756,736
    Average L1 Active Cycles         cycle    18,022.95
    Total L1 Elapsed Cycles          cycle      880,630
    Average L2 Active Cycles         cycle    17,440.97
    Total L2 Elapsed Cycles          cycle      794,272
    Average SM Active Cycles         cycle    18,022.95
    Total SM Elapsed Cycles          cycle      880,630
    Average SMSP Active Cycles       cycle    18,009.67
    Total SMSP Elapsed Cycles        cycle    3,522,520
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 9.125%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 11.15% above the average, while the minimum instance value is 6.80% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 9.973%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 12.19% above the average, while the minimum instance value is 4.87% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 9.125%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 11.15% above the average, while the minimum instance value is 6.80% below the       
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.07
    Branch Instructions              inst        6,907
    Branch Efficiency                   %        99.95
    Avg. Divergent Branches                       0.01
    ------------------------- ----------- ------------

  cuda_right_hand_side(const double *, double *, int, double, double, double, double) (256, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.73
    SM Frequency                    Ghz         1.59
    Elapsed Cycles                cycle      355,762
    Memory Throughput                 %         3.66
    DRAM Throughput                   %         3.66
    Duration                         us       222.91
    L1/TEX Cache Throughput           %         4.51
    L2 Cache Throughput               %         3.02
    SM Active Cycles              cycle   308,188.03
    Compute (SM) Throughput           %        75.35
    ----------------------- ----------- ------------

    OPT   Compute is more heavily utilized than Memory: Look at the Compute Workload Analysis section to see what the   
          compute pipelines are spending their time doing. Also, consider whether any computation is redundant and      
          could be reduced or moved to look-up tables.                                                                  

    Section: GPU Speed Of Light Roofline Chart
    OPT   Est. Speedup: 82.32%                                                                                          
          The ratio of peak float (FP32) to double (FP64) performance on this device is 32:1. The workload achieved     
          close to 0% of this device's FP32 peak performance and 56% of its FP64 peak performance. If Compute Workload  
          Analysis determines that this workload is FP64 bound, consider using 32-bit precision floating point          
          operations to improve its performance. See the Profiling Guide                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Kbyte       393.22
    Dropped Samples                sample            0
    Maximum Sampling Interval       cycle       20,000
    # Pass Groups                                    1
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.11
    Executed Ipc Elapsed  inst/cycle         0.09
    Issue Slots Busy               %         2.65
    Issued Ipc Active     inst/cycle         0.11
    SM Busy                        %        86.99
    -------------------- ----------- ------------

    OPT   FP64 is the highest-utilized pipeline (87.0%) based on active cycles, taking into account the rates of its    
          different instructions. It executes 64-bit floating point operations. The pipeline is over-utilized and       
          likely a performance bottleneck. Based on the number of executed instructions, the highest utilized pipeline  
          (87.0%) is FP64. It executes 64-bit floating point operations. Comparing the two, the overall pipeline        
          utilization appears to be caused by frequent, low-latency instructions. See the Profiling Guide               
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-decoder) or hover over the          
          pipeline name to understand the workloads handled by each pipeline. The Instruction Statistics section shows  
          the mix of executed instructions for this workload. Check the Warp State Statistics section for which         
          reasons cause warps to stall.                                                                                 

    Section: Memory Workload Analysis
    ----------------- ----------- ------------
    Metric Name       Metric Unit Metric Value
    ----------------- ----------- ------------
    Memory Throughput     Gbyte/s        15.77
    Mem Busy                    %         2.99
    Max Bandwidth               %         3.66
    L1/TEX Hit Rate             %        79.19
    L2 Hit Rate                 %        80.23
    Mem Pipes Busy              %        19.69
    ----------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 3.226%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 9.1 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 3.381%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32     
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %         2.65
    Issued Warp Per Scheduler                        0.03
    No Eligible                            %        97.35
    Active Warps Per Scheduler          warp         6.63
    Eligible Warps Per Scheduler        warp         0.07
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 24.65%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 37.7 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 8 warps per scheduler, this workload allocates an average of  
          6.63 active warps per scheduler, but only an average of 0.07 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle       249.86
    Warp Cycles Per Executed Instruction           cycle       251.09
    Avg. Active Threads Per Warp                                31.99
    Avg. Not Predicated Off Threads Per Warp                    30.70
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 24.65%                                                                                          
          On average, each warp of this workload spends 121.1 cycles being stalled waiting for a scoreboard dependency  
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 48.5% of the total average of 249.9 cycles       
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 24.65%                                                                                          
          On average, each warp of this workload spends 120.9 cycles being stalled waiting for the L1 instruction queue 
          for texture operations to be not full. This stall reason is high in cases of extreme utilization of the       
          L1TEX pipeline. Try issuing fewer texture fetches, surface loads, surface stores, or decoupled math           
          operations. If applicable, consider combining multiple lower-width memory operations into fewer wider memory  
          operations and try interleaving memory operations and math instructions. Consider converting texture lookups  
          or surface loads into global memory lookups. Texture can accept four threads' requests per cycle, whereas     
          global accepts 32 threads. This stall type represents about 48.4% of the total average of 249.9 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Profiling Guide                                                                            
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst     8,125.43
    Executed Instructions                           inst    1,300,068
    Avg. Issued Instructions Per Scheduler          inst     8,165.49
    Issued Instructions                             inst    1,306,479
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 16.53%                                                                                          
          This kernel executes 388846 fused and 238388 non-fused FP64 instructions. By converting pairs of non-fused    
          instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point),           
          higher-throughput equivalent, the achieved FP64 performance could be increased by up to 19% (relative to its  
          current performance).                                                                                         

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    256
    Registers Per Thread             register/thread              56
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              40
    Stack Size                                                 1,024
    Threads                                   thread          65,536
    # TPCs                                                        20
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                1.60
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 97 thread blocks.   
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 50.0% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        82.78
    Achieved Active Warps Per SM           warp        26.49
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 17.22%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (82.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    54,925.50
    Total DRAM Elapsed Cycles        cycle   12,009,472
    Average L1 Active Cycles         cycle   308,188.03
    Total L1 Elapsed Cycles          cycle   14,230,984
    Average L2 Active Cycles         cycle    42,180.06
    Total L2 Elapsed Cycles          cycle   10,892,448
    Average SM Active Cycles         cycle   308,188.03
    Total SM Elapsed Cycles          cycle   14,230,984
    Average SMSP Active Cycles       cycle   307,952.63
    Total SMSP Elapsed Cycles        cycle   56,923,936
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 11%                                                                                             
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 12.69% above the average, while the minimum instance value is 1.87% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.09%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 12.81% above the average, while the minimum instance value is 1.89% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11%                                                                                             
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 12.69% above the average, while the minimum instance value is 1.87% below the       
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.07
    Branch Instructions              inst       88,024
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 8.54%                                                                                           
          This kernel has uncoalesced global accesses resulting in a total of 421875 excessive sectors (69% of the      
          total 612171 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source         
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  cuda_final_update(double *, const double *, const double *, const double *, const double *, double, int) (88, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.11
    SM Frequency                    Ghz         1.58
    Elapsed Cycles                cycle       55,679
    Memory Throughput                 %        89.59
    DRAM Throughput                   %        89.59
    Duration                         us        35.17
    L1/TEX Cache Throughput           %        17.29
    L2 Cache Throughput               %        22.11
    SM Active Cycles              cycle    45,194.20
    Compute (SM) Throughput           %        25.46
    ----------------------- ----------- ------------

    INF   This workload is utilizing greater than 80.0% of the available compute or memory performance of the device.   
          To further improve performance, work will likely need to be shifted from the most utilized to another unit.   
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: GPU Speed Of Light Roofline Chart
    OPT   Est. Speedup: 27.41%                                                                                          
          The ratio of peak float (FP32) to double (FP64) performance on this device is 32:1. The workload achieved 0%  
          of this device's FP32 peak performance and 22% of its FP64 peak performance. If Compute Workload Analysis     
          determines that this workload is FP64 bound, consider using 32-bit precision floating point operations to     
          improve its performance. See the Profiling Guide                                                              
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Kbyte       393.22
    Dropped Samples                sample            0
    Maximum Sampling Interval       cycle       20,000
    # Pass Groups                                    1
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.09
    Executed Ipc Elapsed  inst/cycle         0.08
    Issue Slots Busy               %         2.33
    Issued Ipc Active     inst/cycle         0.09
    SM Busy                        %        28.29
    -------------------- ----------- ------------

    INF   FP64 is the highest-utilized pipeline (28.3%) based on active cycles, taking into account the rates of its    
          different instructions. It executes 64-bit floating point operations. It is well-utilized, but should not be  
          a bottleneck.                                                                                                 

    Section: Memory Workload Analysis
    ----------------- ----------- ------------
    Metric Name       Metric Unit Metric Value
    ----------------- ----------- ------------
    Memory Throughput     Gbyte/s       350.61
    Mem Busy                    %        22.11
    Max Bandwidth               %        89.59
    L1/TEX Hit Rate             %        14.17
    L2 Hit Rate                 %        17.08
    Mem Pipes Busy              %         7.85
    ----------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %         2.34
    Issued Warp Per Scheduler                        0.02
    No Eligible                            %        97.66
    Active Warps Per Scheduler          warp         4.34
    Eligible Warps Per Scheduler        warp         0.03
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 10.41%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 42.7 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 8 warps per scheduler, this workload allocates an average of  
          4.34 active warps per scheduler, but only an average of 0.03 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp  
          State Statistics and Source Counters sections.                                                                

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle       185.24
    Warp Cycles Per Executed Instruction           cycle       188.58
    Avg. Active Threads Per Warp                                31.99
    Avg. Not Predicated Off Threads Per Warp                    30.79
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 10.41%                                                                                          
          On average, each warp of this workload spends 174.8 cycles being stalled waiting for a scoreboard dependency  
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 94.4% of the total average of 185.2 cycles       
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Profiling Guide                                                                            
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst     1,034.84
    Executed Instructions                           inst      165,575
    Avg. Issued Instructions Per Scheduler          inst     1,053.54
    Issued Instructions                             inst      168,566
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 3.77%                                                                                           
          This kernel executes 23448 fused and 8520 non-fused FP64 instructions. By converting pairs of non-fused       
          instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point),           
          higher-throughput equivalent, the achieved FP64 performance could be increased by up to 13% (relative to its  
          current performance).                                                                                         

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     88
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              40
    Stack Size                                                 1,024
    Threads                                   thread          22,528
    # TPCs                                                        20
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                0.55
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        53.75
    Achieved Active Warps Per SM           warp        17.20
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 10.41%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (53.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   192,659.50
    Total DRAM Elapsed Cycles        cycle    1,720,320
    Average L1 Active Cycles         cycle    45,194.20
    Total L1 Elapsed Cycles          cycle    2,009,368
    Average L2 Active Cycles         cycle    43,128.72
    Total L2 Elapsed Cycles          cycle    1,705,472
    Average SM Active Cycles         cycle    45,194.20
    Total SM Elapsed Cycles          cycle    2,009,368
    Average SMSP Active Cycles       cycle    45,006.94
    Total SMSP Elapsed Cycles        cycle    8,037,472
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.232%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 5.82% above the average, while the minimum instance value is 2.89% below the average.       
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.814%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 6.49% above the average, while the minimum instance value is 3.96% below the        
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.232%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 5.82% above the average, while the minimum instance value is 2.89% below the        
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.04
    Branch Instructions              inst        6,907
    Branch Efficiency                   %        99.95
    Avg. Divergent Branches                       0.01
    ------------------------- ----------- ------------

  cuda_right_hand_side(const double *, double *, int, double, double, double, double) (256, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.73
    SM Frequency                    Ghz         1.59
    Elapsed Cycles                cycle      355,681
    Memory Throughput                 %         3.67
    DRAM Throughput                   %         3.67
    Duration                         us       222.91
    L1/TEX Cache Throughput           %         4.52
    L2 Cache Throughput               %         3.01
    SM Active Cycles              cycle   308,215.95
    Compute (SM) Throughput           %        75.61
    ----------------------- ----------- ------------

    OPT   Compute is more heavily utilized than Memory: Look at the Compute Workload Analysis section to see what the   
          compute pipelines are spending their time doing. Also, consider whether any computation is redundant and      
          could be reduced or moved to look-up tables.                                                                  

    Section: GPU Speed Of Light Roofline Chart
    OPT   Est. Speedup: 82.31%                                                                                          
          The ratio of peak float (FP32) to double (FP64) performance on this device is 32:1. The workload achieved     
          close to 0% of this device's FP32 peak performance and 56% of its FP64 peak performance. If Compute Workload  
          Analysis determines that this workload is FP64 bound, consider using 32-bit precision floating point          
          operations to improve its performance. See the Profiling Guide                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Kbyte       393.22
    Dropped Samples                sample            0
    Maximum Sampling Interval       cycle       20,000
    # Pass Groups                                    1
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.11
    Executed Ipc Elapsed  inst/cycle         0.09
    Issue Slots Busy               %         2.65
    Issued Ipc Active     inst/cycle         0.11
    SM Busy                        %        86.98
    -------------------- ----------- ------------

    OPT   FP64 is the highest-utilized pipeline (87.0%) based on active cycles, taking into account the rates of its    
          different instructions. It executes 64-bit floating point operations. The pipeline is over-utilized and       
          likely a performance bottleneck. Based on the number of executed instructions, the highest utilized pipeline  
          (87.0%) is FP64. It executes 64-bit floating point operations. Comparing the two, the overall pipeline        
          utilization appears to be caused by frequent, low-latency instructions. See the Profiling Guide               
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-decoder) or hover over the          
          pipeline name to understand the workloads handled by each pipeline. The Instruction Statistics section shows  
          the mix of executed instructions for this workload. Check the Warp State Statistics section for which         
          reasons cause warps to stall.                                                                                 

    Section: Memory Workload Analysis
    ----------------- ----------- ------------
    Metric Name       Metric Unit Metric Value
    ----------------- ----------- ------------
    Memory Throughput     Gbyte/s        15.79
    Mem Busy                    %         2.98
    Max Bandwidth               %         3.67
    L1/TEX Hit Rate             %        79.28
    L2 Hit Rate                 %        80.35
    Mem Pipes Busy              %        19.76
    ----------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 3.238%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 9.1 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 3.393%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32     
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %         2.65
    Issued Warp Per Scheduler                        0.03
    No Eligible                            %        97.35
    Active Warps Per Scheduler          warp         6.63
    Eligible Warps Per Scheduler        warp         0.07
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 24.39%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 37.7 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 8 warps per scheduler, this workload allocates an average of  
          6.63 active warps per scheduler, but only an average of 0.07 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle       249.91
    Warp Cycles Per Executed Instruction           cycle       251.14
    Avg. Active Threads Per Warp                                31.99
    Avg. Not Predicated Off Threads Per Warp                    30.70
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 24.39%                                                                                          
          On average, each warp of this workload spends 121.1 cycles being stalled waiting for a scoreboard dependency  
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 48.4% of the total average of 249.9 cycles       
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 24.39%                                                                                          
          On average, each warp of this workload spends 120.9 cycles being stalled waiting for the L1 instruction queue 
          for texture operations to be not full. This stall reason is high in cases of extreme utilization of the       
          L1TEX pipeline. Try issuing fewer texture fetches, surface loads, surface stores, or decoupled math           
          operations. If applicable, consider combining multiple lower-width memory operations into fewer wider memory  
          operations and try interleaving memory operations and math instructions. Consider converting texture lookups  
          or surface loads into global memory lookups. Texture can accept four threads' requests per cycle, whereas     
          global accepts 32 threads. This stall type represents about 48.4% of the total average of 249.9 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Profiling Guide                                                                            
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst     8,125.43
    Executed Instructions                           inst    1,300,068
    Avg. Issued Instructions Per Scheduler          inst     8,165.50
    Issued Instructions                             inst    1,306,480
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 16.53%                                                                                          
          This kernel executes 388846 fused and 238388 non-fused FP64 instructions. By converting pairs of non-fused    
          instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point),           
          higher-throughput equivalent, the achieved FP64 performance could be increased by up to 19% (relative to its  
          current performance).                                                                                         

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    256
    Registers Per Thread             register/thread              56
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              40
    Stack Size                                                 1,024
    Threads                                   thread          65,536
    # TPCs                                                        20
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                1.60
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 97 thread blocks.   
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 50.0% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        82.75
    Achieved Active Warps Per SM           warp        26.48
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 17.25%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (82.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       55,010
    Total DRAM Elapsed Cycles        cycle   11,993,088
    Average L1 Active Cycles         cycle   308,215.95
    Total L1 Elapsed Cycles          cycle   14,182,618
    Average L2 Active Cycles         cycle    42,455.53
    Total L2 Elapsed Cycles          cycle   10,889,568
    Average SM Active Cycles         cycle   308,215.95
    Total SM Elapsed Cycles          cycle   14,182,618
    Average SMSP Active Cycles       cycle   307,901.64
    Total SMSP Elapsed Cycles        cycle   56,730,472
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 11.07%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 12.73% above the average, while the minimum instance value is 1.94% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.11%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 12.80% above the average, while the minimum instance value is 1.85% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.07%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 12.73% above the average, while the minimum instance value is 1.94% below the       
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.07
    Branch Instructions              inst       88,024
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 8.598%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 421875 excessive sectors (69% of the      
          total 612171 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source         
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  cuda_aux_computation(const double *, const double *, double *, double, int, double) (88, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.80
    SM Frequency                    Ghz         1.58
    Elapsed Cycles                cycle       25,711
    Memory Throughput                 %        87.05
    DRAM Throughput                   %        87.05
    Duration                         us        16.26
    L1/TEX Cache Throughput           %        17.95
    L2 Cache Throughput               %        24.07
    SM Active Cycles              cycle    18,014.38
    Compute (SM) Throughput           %        15.57
    ----------------------- ----------- ------------

    INF   This workload is utilizing greater than 80.0% of the available compute or memory performance of the device.   
          To further improve performance, work will likely need to be shifted from the most utilized to another unit.   
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (FP32) to double (FP64) performance on this device is 32:1. The workload achieved 0%  
          of this device's FP32 peak performance and 15% of its FP64 peak performance. See the Profiling Guide          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Kbyte       393.22
    Dropped Samples                sample            0
    Maximum Sampling Interval       cycle       20,000
    # Pass Groups                                    1
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.14
    Executed Ipc Elapsed  inst/cycle         0.12
    Issue Slots Busy               %         3.63
    Issued Ipc Active     inst/cycle         0.15
    SM Busy                        %        18.92
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 81.08%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ----------------- ----------- ------------
    Metric Name       Metric Unit Metric Value
    ----------------- ----------- ------------
    Memory Throughput     Gbyte/s       322.85
    Mem Busy                    %        24.07
    Max Bandwidth               %        87.05
    L1/TEX Hit Rate             %            0
    L2 Hit Rate                 %        33.97
    Mem Pipes Busy              %         7.30
    ----------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %         3.62
    Issued Warp Per Scheduler                        0.04
    No Eligible                            %        96.38
    Active Warps Per Scheduler          warp         4.29
    Eligible Warps Per Scheduler        warp         0.04
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 12.95%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 27.6 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 8 warps per scheduler, this workload allocates an average of  
          4.29 active warps per scheduler, but only an average of 0.04 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp  
          State Statistics and Source Counters sections.                                                                

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle       118.40
    Warp Cycles Per Executed Instruction           cycle       121.57
    Avg. Active Threads Per Warp                                31.99
    Avg. Not Predicated Off Threads Per Warp                    30.04
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 12.95%                                                                                          
          On average, each warp of this workload spends 107.4 cycles being stalled waiting for a scoreboard dependency  
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 90.7% of the total average of 118.4 cycles       
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Profiling Guide                                                                            
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       636.09
    Executed Instructions                           inst      101,775
    Avg. Issued Instructions Per Scheduler          inst       653.09
    Issued Instructions                             inst      104,495
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     88
    Registers Per Thread             register/thread              28
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              40
    Stack Size                                                 1,024
    Threads                                   thread          22,528
    # TPCs                                                        20
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                0.55
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        53.22
    Achieved Active Warps Per SM           warp        17.03
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 12.95%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (53.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       82,004
    Total DRAM Elapsed Cycles        cycle      753,664
    Average L1 Active Cycles         cycle    18,014.38
    Total L1 Elapsed Cycles          cycle      875,322
    Average L2 Active Cycles         cycle    17,267.81
    Total L2 Elapsed Cycles          cycle      786,720
    Average SM Active Cycles         cycle    18,014.38
    Total SM Elapsed Cycles          cycle      875,322
    Average SMSP Active Cycles       cycle    18,022.43
    Total SMSP Elapsed Cycles        cycle    3,501,288
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 8.121%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 9.87% above the average, while the minimum instance value is 4.04% below the average.       
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 9.513%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 11.55% above the average, while the minimum instance value is 7.37% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.121%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 9.87% above the average, while the minimum instance value is 4.04% below the        
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.07
    Branch Instructions              inst        6,907
    Branch Efficiency                   %        99.95
    Avg. Divergent Branches                       0.01
    ------------------------- ----------- ------------

  cuda_right_hand_side(const double *, double *, int, double, double, double, double) (256, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.74
    SM Frequency                    Ghz         1.59
    Elapsed Cycles                cycle      355,496
    Memory Throughput                 %         3.63
    DRAM Throughput                   %         3.63
    Duration                         us       222.72
    L1/TEX Cache Throughput           %         4.53
    L2 Cache Throughput               %         3.03
    SM Active Cycles              cycle   308,196.03
    Compute (SM) Throughput           %        75.65
    ----------------------- ----------- ------------

    OPT   Compute is more heavily utilized than Memory: Look at the Compute Workload Analysis section to see what the   
          compute pipelines are spending their time doing. Also, consider whether any computation is redundant and      
          could be reduced or moved to look-up tables.                                                                  

    Section: GPU Speed Of Light Roofline Chart
    OPT   Est. Speedup: 82.32%                                                                                          
          The ratio of peak float (FP32) to double (FP64) performance on this device is 32:1. The workload achieved     
          close to 0% of this device's FP32 peak performance and 56% of its FP64 peak performance. If Compute Workload  
          Analysis determines that this workload is FP64 bound, consider using 32-bit precision floating point          
          operations to improve its performance. See the Profiling Guide                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Kbyte       393.22
    Dropped Samples                sample            0
    Maximum Sampling Interval       cycle       20,000
    # Pass Groups                                    1
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.11
    Executed Ipc Elapsed  inst/cycle         0.09
    Issue Slots Busy               %         2.65
    Issued Ipc Active     inst/cycle         0.11
    SM Busy                        %        86.99
    -------------------- ----------- ------------

    OPT   FP64 is the highest-utilized pipeline (87.0%) based on active cycles, taking into account the rates of its    
          different instructions. It executes 64-bit floating point operations. The pipeline is over-utilized and       
          likely a performance bottleneck. Based on the number of executed instructions, the highest utilized pipeline  
          (87.0%) is FP64. It executes 64-bit floating point operations. Comparing the two, the overall pipeline        
          utilization appears to be caused by frequent, low-latency instructions. See the Profiling Guide               
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-decoder) or hover over the          
          pipeline name to understand the workloads handled by each pipeline. The Instruction Statistics section shows  
          the mix of executed instructions for this workload. Check the Warp State Statistics section for which         
          reasons cause warps to stall.                                                                                 

    Section: Memory Workload Analysis
    ----------------- ----------- ------------
    Metric Name       Metric Unit Metric Value
    ----------------- ----------- ------------
    Memory Throughput     Gbyte/s        15.67
    Mem Busy                    %         2.99
    Max Bandwidth               %         3.63
    L1/TEX Hit Rate             %        79.31
    L2 Hit Rate                 %        80.20
    Mem Pipes Busy              %        19.77
    ----------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 3.239%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 9.1 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 3.395%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32     
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %         2.65
    Issued Warp Per Scheduler                        0.03
    No Eligible                            %        97.35
    Active Warps Per Scheduler          warp         6.62
    Eligible Warps Per Scheduler        warp         0.07
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 24.35%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 37.7 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 8 warps per scheduler, this workload allocates an average of  
          6.62 active warps per scheduler, but only an average of 0.07 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle       249.88
    Warp Cycles Per Executed Instruction           cycle       251.11
    Avg. Active Threads Per Warp                                31.99
    Avg. Not Predicated Off Threads Per Warp                    30.71
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 24.35%                                                                                          
          On average, each warp of this workload spends 121.2 cycles being stalled waiting for a scoreboard dependency  
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 48.5% of the total average of 249.9 cycles       
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 24.35%                                                                                          
          On average, each warp of this workload spends 120.9 cycles being stalled waiting for the L1 instruction queue 
          for texture operations to be not full. This stall reason is high in cases of extreme utilization of the       
          L1TEX pipeline. Try issuing fewer texture fetches, surface loads, surface stores, or decoupled math           
          operations. If applicable, consider combining multiple lower-width memory operations into fewer wider memory  
          operations and try interleaving memory operations and math instructions. Consider converting texture lookups  
          or surface loads into global memory lookups. Texture can accept four threads' requests per cycle, whereas     
          global accepts 32 threads. This stall type represents about 48.4% of the total average of 249.9 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Profiling Guide                                                                            
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst     8,125.43
    Executed Instructions                           inst    1,300,068
    Avg. Issued Instructions Per Scheduler          inst     8,165.43
    Issued Instructions                             inst    1,306,468
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 16.53%                                                                                          
          This kernel executes 388846 fused and 238388 non-fused FP64 instructions. By converting pairs of non-fused    
          instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point),           
          higher-throughput equivalent, the achieved FP64 performance could be increased by up to 19% (relative to its  
          current performance).                                                                                         

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    256
    Registers Per Thread             register/thread              56
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              40
    Stack Size                                                 1,024
    Threads                                   thread          65,536
    # TPCs                                                        20
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                1.60
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 97 thread blocks.   
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 50.0% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        82.77
    Achieved Active Warps Per SM           warp        26.49
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 17.23%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (82.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    54,530.50
    Total DRAM Elapsed Cycles        cycle   12,010,496
    Average L1 Active Cycles         cycle   308,196.03
    Total L1 Elapsed Cycles          cycle   14,175,694
    Average L2 Active Cycles         cycle    42,003.22
    Total L2 Elapsed Cycles          cycle   10,882,240
    Average SM Active Cycles         cycle   308,196.03
    Total SM Elapsed Cycles          cycle   14,175,694
    Average SMSP Active Cycles       cycle   307,984.58
    Total SMSP Elapsed Cycles        cycle   56,702,776
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 11.09%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 12.75% above the average, while the minimum instance value is 1.86% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.16%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 12.84% above the average, while the minimum instance value is 1.87% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.09%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 12.75% above the average, while the minimum instance value is 1.86% below the       
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.07
    Branch Instructions              inst       88,024
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 8.512%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 421875 excessive sectors (69% of the      
          total 612171 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source         
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  cuda_aux_computation(const double *, const double *, double *, double, int, double) (88, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.63
    SM Frequency                    Ghz         1.57
    Elapsed Cycles                cycle       26,115
    Memory Throughput                 %        89.35
    DRAM Throughput                   %        89.35
    Duration                         us        16.54
    L1/TEX Cache Throughput           %        18.23
    L2 Cache Throughput               %        23.67
    SM Active Cycles              cycle    18,036.72
    Compute (SM) Throughput           %        15.80
    ----------------------- ----------- ------------

    INF   This workload is utilizing greater than 80.0% of the available compute or memory performance of the device.   
          To further improve performance, work will likely need to be shifted from the most utilized to another unit.   
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: GPU Speed Of Light Roofline Chart
    OPT   Est. Speedup: 18.3%                                                                                           
          The ratio of peak float (FP32) to double (FP64) performance on this device is 32:1. The workload achieved 0%  
          of this device's FP32 peak performance and 15% of its FP64 peak performance. If Compute Workload Analysis     
          determines that this workload is FP64 bound, consider using 32-bit precision floating point operations to     
          improve its performance. See the Profiling Guide                                                              
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Kbyte       393.22
    Dropped Samples                sample            0
    Maximum Sampling Interval       cycle       20,000
    # Pass Groups                                    1
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.14
    Executed Ipc Elapsed  inst/cycle         0.12
    Issue Slots Busy               %         3.62
    Issued Ipc Active     inst/cycle         0.14
    SM Busy                        %        18.89
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 81.11%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ----------------- ----------- ------------
    Metric Name       Metric Unit Metric Value
    ----------------- ----------- ------------
    Memory Throughput     Gbyte/s       322.08
    Mem Busy                    %        23.67
    Max Bandwidth               %        89.35
    L1/TEX Hit Rate             %            0
    L2 Hit Rate                 %        33.94
    Mem Pipes Busy              %         7.41
    ----------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %         3.61
    Issued Warp Per Scheduler                        0.04
    No Eligible                            %        96.39
    Active Warps Per Scheduler          warp         4.22
    Eligible Warps Per Scheduler        warp         0.04
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 10.65%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 27.7 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 8 warps per scheduler, this workload allocates an average of  
          4.22 active warps per scheduler, but only an average of 0.04 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp  
          State Statistics and Source Counters sections.                                                                

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle       117.19
    Warp Cycles Per Executed Instruction           cycle       120.32
    Avg. Active Threads Per Warp                                31.99
    Avg. Not Predicated Off Threads Per Warp                    30.04
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 10.65%                                                                                          
          On average, each warp of this workload spends 107.3 cycles being stalled waiting for a scoreboard dependency  
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 91.6% of the total average of 117.2 cycles       
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Profiling Guide                                                                            
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       636.09
    Executed Instructions                           inst      101,775
    Avg. Issued Instructions Per Scheduler          inst       653.10
    Issued Instructions                             inst      104,496
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     88
    Registers Per Thread             register/thread              28
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              40
    Stack Size                                                 1,024
    Threads                                   thread          22,528
    # TPCs                                                        20
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                0.55
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        53.17
    Achieved Active Warps Per SM           warp        17.02
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 10.65%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (53.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       83,258
    Total DRAM Elapsed Cycles        cycle      745,472
    Average L1 Active Cycles         cycle    18,036.72
    Total L1 Elapsed Cycles          cycle      862,890
    Average L2 Active Cycles         cycle    17,335.97
    Total L2 Elapsed Cycles          cycle      800,448
    Average SM Active Cycles         cycle    18,036.72
    Total SM Elapsed Cycles          cycle      862,890
    Average SMSP Active Cycles       cycle    18,115.05
    Total SMSP Elapsed Cycles        cycle    3,451,560
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 7.992%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 9.56% above the average, while the minimum instance value is 4.96% below the average.       
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 9.384%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 11.17% above the average, while the minimum instance value is 4.70% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.992%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 9.56% above the average, while the minimum instance value is 4.96% below the        
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.07
    Branch Instructions              inst        6,907
    Branch Efficiency                   %        99.95
    Avg. Divergent Branches                       0.01
    ------------------------- ----------- ------------

  cuda_right_hand_side(const double *, double *, int, double, double, double, double) (256, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.79
    SM Frequency                    Ghz         1.61
    Elapsed Cycles                cycle      356,260
    Memory Throughput                 %         3.66
    DRAM Throughput                   %         3.66
    Duration                         us       221.18
    L1/TEX Cache Throughput           %         4.51
    L2 Cache Throughput               %         3.04
    SM Active Cycles              cycle   308,174.12
    Compute (SM) Throughput           %        75.44
    ----------------------- ----------- ------------

    OPT   Compute is more heavily utilized than Memory: Look at the Compute Workload Analysis section to see what the   
          compute pipelines are spending their time doing. Also, consider whether any computation is redundant and      
          could be reduced or moved to look-up tables.                                                                  

    Section: GPU Speed Of Light Roofline Chart
    OPT   Est. Speedup: 82.32%                                                                                          
          The ratio of peak float (FP32) to double (FP64) performance on this device is 32:1. The workload achieved     
          close to 0% of this device's FP32 peak performance and 56% of its FP64 peak performance. If Compute Workload  
          Analysis determines that this workload is FP64 bound, consider using 32-bit precision floating point          
          operations to improve its performance. See the Profiling Guide                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Kbyte       393.22
    Dropped Samples                sample            0
    Maximum Sampling Interval       cycle       20,000
    # Pass Groups                                    1
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.11
    Executed Ipc Elapsed  inst/cycle         0.09
    Issue Slots Busy               %         2.65
    Issued Ipc Active     inst/cycle         0.11
    SM Busy                        %        86.99
    -------------------- ----------- ------------

    OPT   FP64 is the highest-utilized pipeline (87.0%) based on active cycles, taking into account the rates of its    
          different instructions. It executes 64-bit floating point operations. The pipeline is over-utilized and       
          likely a performance bottleneck. Based on the number of executed instructions, the highest utilized pipeline  
          (87.0%) is FP64. It executes 64-bit floating point operations. Comparing the two, the overall pipeline        
          utilization appears to be caused by frequent, low-latency instructions. See the Profiling Guide               
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-decoder) or hover over the          
          pipeline name to understand the workloads handled by each pipeline. The Instruction Statistics section shows  
          the mix of executed instructions for this workload. Check the Warp State Statistics section for which         
          reasons cause warps to stall.                                                                                 

    Section: Memory Workload Analysis
    ----------------- ----------- ------------
    Metric Name       Metric Unit Metric Value
    ----------------- ----------- ------------
    Memory Throughput     Gbyte/s        15.91
    Mem Busy                    %         3.01
    Max Bandwidth               %         3.66
    L1/TEX Hit Rate             %        79.33
    L2 Hit Rate                 %        80.20
    Mem Pipes Busy              %        19.72
    ----------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 3.23%                                                                                           
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 9.1 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 3.385%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32     
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %         2.65
    Issued Warp Per Scheduler                        0.03
    No Eligible                            %        97.35
    Active Warps Per Scheduler          warp         6.62
    Eligible Warps Per Scheduler        warp         0.07
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 24.56%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 37.7 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 8 warps per scheduler, this workload allocates an average of  
          6.62 active warps per scheduler, but only an average of 0.07 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle       249.85
    Warp Cycles Per Executed Instruction           cycle       251.08
    Avg. Active Threads Per Warp                                31.99
    Avg. Not Predicated Off Threads Per Warp                    30.71
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 24.56%                                                                                          
          On average, each warp of this workload spends 121.2 cycles being stalled waiting for a scoreboard dependency  
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 48.5% of the total average of 249.8 cycles       
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 24.56%                                                                                          
          On average, each warp of this workload spends 120.9 cycles being stalled waiting for the L1 instruction queue 
          for texture operations to be not full. This stall reason is high in cases of extreme utilization of the       
          L1TEX pipeline. Try issuing fewer texture fetches, surface loads, surface stores, or decoupled math           
          operations. If applicable, consider combining multiple lower-width memory operations into fewer wider memory  
          operations and try interleaving memory operations and math instructions. Consider converting texture lookups  
          or surface loads into global memory lookups. Texture can accept four threads' requests per cycle, whereas     
          global accepts 32 threads. This stall type represents about 48.4% of the total average of 249.8 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Profiling Guide                                                                            
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst     8,125.43
    Executed Instructions                           inst    1,300,068
    Avg. Issued Instructions Per Scheduler          inst     8,165.44
    Issued Instructions                             inst    1,306,470
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 16.53%                                                                                          
          This kernel executes 388846 fused and 238388 non-fused FP64 instructions. By converting pairs of non-fused    
          instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point),           
          higher-throughput equivalent, the achieved FP64 performance could be increased by up to 19% (relative to its  
          current performance).                                                                                         

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    256
    Registers Per Thread             register/thread              56
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              40
    Stack Size                                                 1,024
    Threads                                   thread          65,536
    # TPCs                                                        20
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                1.60
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 97 thread blocks.   
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 50.0% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        82.79
    Achieved Active Warps Per SM           warp        26.49
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 17.21%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (82.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    54,981.50
    Total DRAM Elapsed Cycles        cycle   12,021,760
    Average L1 Active Cycles         cycle   308,174.12
    Total L1 Elapsed Cycles          cycle   14,214,598
    Average L2 Active Cycles         cycle    42,109.72
    Total L2 Elapsed Cycles          cycle   10,823,328
    Average SM Active Cycles         cycle   308,174.12
    Total SM Elapsed Cycles          cycle   14,214,598
    Average SMSP Active Cycles       cycle   307,946.21
    Total SMSP Elapsed Cycles        cycle   56,858,392
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 11.05%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 12.74% above the average, while the minimum instance value is 1.94% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.11%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 12.82% above the average, while the minimum instance value is 1.87% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.05%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 12.74% above the average, while the minimum instance value is 1.94% below the       
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.07
    Branch Instructions              inst       88,024
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 8.58%                                                                                           
          This kernel has uncoalesced global accesses resulting in a total of 421875 excessive sectors (69% of the      
          total 612171 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source         
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  cuda_aux_computation(const double *, const double *, double *, double, int, double) (88, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.64
    SM Frequency                    Ghz         1.57
    Elapsed Cycles                cycle       26,215
    Memory Throughput                 %        88.39
    DRAM Throughput                   %        88.39
    Duration                         us        16.61
    L1/TEX Cache Throughput           %        18.00
    L2 Cache Throughput               %        23.63
    SM Active Cycles              cycle    18,024.25
    Compute (SM) Throughput           %        15.61
    ----------------------- ----------- ------------

    INF   This workload is utilizing greater than 80.0% of the available compute or memory performance of the device.   
          To further improve performance, work will likely need to be shifted from the most utilized to another unit.   
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (FP32) to double (FP64) performance on this device is 32:1. The workload achieved 0%  
          of this device's FP32 peak performance and 15% of its FP64 peak performance. See the Profiling Guide          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Kbyte       393.22
    Dropped Samples                sample            0
    Maximum Sampling Interval       cycle       20,000
    # Pass Groups                                    1
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.14
    Executed Ipc Elapsed  inst/cycle         0.12
    Issue Slots Busy               %         3.62
    Issued Ipc Active     inst/cycle         0.14
    SM Busy                        %        18.91
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 81.09%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ----------------- ----------- ------------
    Metric Name       Metric Unit Metric Value
    ----------------- ----------- ------------
    Memory Throughput     Gbyte/s       319.16
    Mem Busy                    %        23.63
    Max Bandwidth               %        88.39
    L1/TEX Hit Rate             %            0
    L2 Hit Rate                 %        33.90
    Mem Pipes Busy              %         7.32
    ----------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %         3.64
    Issued Warp Per Scheduler                        0.04
    No Eligible                            %        96.36
    Active Warps Per Scheduler          warp         4.32
    Eligible Warps Per Scheduler        warp         0.05
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 11.61%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 27.4 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 8 warps per scheduler, this workload allocates an average of  
          4.32 active warps per scheduler, but only an average of 0.05 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp  
          State Statistics and Source Counters sections.                                                                

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle       118.50
    Warp Cycles Per Executed Instruction           cycle       121.66
    Avg. Active Threads Per Warp                                31.99
    Avg. Not Predicated Off Threads Per Warp                    30.04
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 11.61%                                                                                          
          On average, each warp of this workload spends 108.0 cycles being stalled waiting for a scoreboard dependency  
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 91.1% of the total average of 118.5 cycles       
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Profiling Guide                                                                            
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       636.09
    Executed Instructions                           inst      101,775
    Avg. Issued Instructions Per Scheduler          inst       653.09
    Issued Instructions                             inst      104,495
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     88
    Registers Per Thread             register/thread              28
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              40
    Stack Size                                                 1,024
    Threads                                   thread          22,528
    # TPCs                                                        20
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                0.55
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        53.40
    Achieved Active Warps Per SM           warp        17.09
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 11.61%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (53.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    82,821.50
    Total DRAM Elapsed Cycles        cycle      749,568
    Average L1 Active Cycles         cycle    18,024.25
    Total L1 Elapsed Cycles          cycle      873,324
    Average L2 Active Cycles         cycle    17,315.44
    Total L2 Elapsed Cycles          cycle      802,464
    Average SM Active Cycles         cycle    18,024.25
    Total SM Elapsed Cycles          cycle      873,324
    Average SMSP Active Cycles       cycle    17,922.67
    Total SMSP Elapsed Cycles        cycle    3,493,296
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 9.903%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 12.00% above the average, while the minimum instance value is 8.16% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.479%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 10.33% above the average, while the minimum instance value is 4.83% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 9.903%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 12.00% above the average, while the minimum instance value is 8.16% below the       
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.07
    Branch Instructions              inst        6,907
    Branch Efficiency                   %        99.95
    Avg. Divergent Branches                       0.01
    ------------------------- ----------- ------------

  cuda_right_hand_side(const double *, double *, int, double, double, double, double) (256, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.78
    SM Frequency                    Ghz         1.61
    Elapsed Cycles                cycle      356,315
    Memory Throughput                 %         3.64
    DRAM Throughput                   %         3.64
    Duration                         us       221.09
    L1/TEX Cache Throughput           %         4.52
    L2 Cache Throughput               %         3.04
    SM Active Cycles              cycle   308,156.60
    Compute (SM) Throughput           %        75.61
    ----------------------- ----------- ------------

    OPT   Compute is more heavily utilized than Memory: Look at the Compute Workload Analysis section to see what the   
          compute pipelines are spending their time doing. Also, consider whether any computation is redundant and      
          could be reduced or moved to look-up tables.                                                                  

    Section: GPU Speed Of Light Roofline Chart
    OPT   Est. Speedup: 82.33%                                                                                          
          The ratio of peak float (FP32) to double (FP64) performance on this device is 32:1. The workload achieved     
          close to 0% of this device's FP32 peak performance and 56% of its FP64 peak performance. If Compute Workload  
          Analysis determines that this workload is FP64 bound, consider using 32-bit precision floating point          
          operations to improve its performance. See the Profiling Guide                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Kbyte       393.22
    Dropped Samples                sample            0
    Maximum Sampling Interval       cycle       20,000
    # Pass Groups                                    1
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.11
    Executed Ipc Elapsed  inst/cycle         0.09
    Issue Slots Busy               %         2.65
    Issued Ipc Active     inst/cycle         0.11
    SM Busy                        %        87.00
    -------------------- ----------- ------------

    OPT   FP64 is the highest-utilized pipeline (87.0%) based on active cycles, taking into account the rates of its    
          different instructions. It executes 64-bit floating point operations. The pipeline is over-utilized and       
          likely a performance bottleneck. Based on the number of executed instructions, the highest utilized pipeline  
          (87.0%) is FP64. It executes 64-bit floating point operations. Comparing the two, the overall pipeline        
          utilization appears to be caused by frequent, low-latency instructions. See the Profiling Guide               
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-decoder) or hover over the          
          pipeline name to understand the workloads handled by each pipeline. The Instruction Statistics section shows  
          the mix of executed instructions for this workload. Check the Warp State Statistics section for which         
          reasons cause warps to stall.                                                                                 

    Section: Memory Workload Analysis
    ----------------- ----------- ------------
    Metric Name       Metric Unit Metric Value
    ----------------- ----------- ------------
    Memory Throughput     Gbyte/s        15.78
    Mem Busy                    %         3.00
    Max Bandwidth               %         3.64
    L1/TEX Hit Rate             %        79.26
    L2 Hit Rate                 %        80.57
    Mem Pipes Busy              %        19.76
    ----------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 3.237%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 9.1 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 3.393%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32     
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %         2.65
    Issued Warp Per Scheduler                        0.03
    No Eligible                            %        97.35
    Active Warps Per Scheduler          warp         6.62
    Eligible Warps Per Scheduler        warp         0.07
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 24.39%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 37.7 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 8 warps per scheduler, this workload allocates an average of  
          6.62 active warps per scheduler, but only an average of 0.07 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle       249.79
    Warp Cycles Per Executed Instruction           cycle       251.02
    Avg. Active Threads Per Warp                                31.99
    Avg. Not Predicated Off Threads Per Warp                    30.73
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 24.39%                                                                                          
          On average, each warp of this workload spends 121.2 cycles being stalled waiting for a scoreboard dependency  
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 48.5% of the total average of 249.8 cycles       
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 24.39%                                                                                          
          On average, each warp of this workload spends 120.7 cycles being stalled waiting for the L1 instruction queue 
          for texture operations to be not full. This stall reason is high in cases of extreme utilization of the       
          L1TEX pipeline. Try issuing fewer texture fetches, surface loads, surface stores, or decoupled math           
          operations. If applicable, consider combining multiple lower-width memory operations into fewer wider memory  
          operations and try interleaving memory operations and math instructions. Consider converting texture lookups  
          or surface loads into global memory lookups. Texture can accept four threads' requests per cycle, whereas     
          global accepts 32 threads. This stall type represents about 48.3% of the total average of 249.8 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Profiling Guide                                                                            
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst     8,125.43
    Executed Instructions                           inst    1,300,068
    Avg. Issued Instructions Per Scheduler          inst     8,165.46
    Issued Instructions                             inst    1,306,474
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 16.53%                                                                                          
          This kernel executes 388846 fused and 238388 non-fused FP64 instructions. By converting pairs of non-fused    
          instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point),           
          higher-throughput equivalent, the achieved FP64 performance could be increased by up to 19% (relative to its  
          current performance).                                                                                         

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    256
    Registers Per Thread             register/thread              56
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              40
    Stack Size                                                 1,024
    Threads                                   thread          65,536
    # TPCs                                                        20
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                1.60
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 97 thread blocks.   
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 50.0% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        82.78
    Achieved Active Warps Per SM           warp        26.49
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 17.22%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (82.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    54,511.50
    Total DRAM Elapsed Cycles        cycle   11,988,992
    Average L1 Active Cycles         cycle   308,156.60
    Total L1 Elapsed Cycles          cycle   14,182,762
    Average L2 Active Cycles         cycle    42,713.44
    Total L2 Elapsed Cycles          cycle   10,818,912
    Average SM Active Cycles         cycle   308,156.60
    Total SM Elapsed Cycles          cycle   14,182,762
    Average SMSP Active Cycles       cycle   307,971.28
    Total SMSP Elapsed Cycles        cycle   56,731,048
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 11.08%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 12.75% above the average, while the minimum instance value is 1.87% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.16%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 12.85% above the average, while the minimum instance value is 1.85% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.08%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 12.75% above the average, while the minimum instance value is 1.87% below the       
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.07
    Branch Instructions              inst       88,024
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 8.706%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 421875 excessive sectors (69% of the      
          total 612171 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source         
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  cuda_final_update(double *, const double *, const double *, const double *, const double *, double, int) (88, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.21
    SM Frequency                    Ghz         1.58
    Elapsed Cycles                cycle       54,767
    Memory Throughput                 %        89.50
    DRAM Throughput                   %        89.50
    Duration                         us        34.62
    L1/TEX Cache Throughput           %        17.20
    L2 Cache Throughput               %        22.46
    SM Active Cycles              cycle    45,409.60
    Compute (SM) Throughput           %        25.64
    ----------------------- ----------- ------------

    INF   This workload is utilizing greater than 80.0% of the available compute or memory performance of the device.   
          To further improve performance, work will likely need to be shifted from the most utilized to another unit.   
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: GPU Speed Of Light Roofline Chart
    OPT   Est. Speedup: 27.28%                                                                                          
          The ratio of peak float (FP32) to double (FP64) performance on this device is 32:1. The workload achieved 0%  
          of this device's FP32 peak performance and 22% of its FP64 peak performance. If Compute Workload Analysis     
          determines that this workload is FP64 bound, consider using 32-bit precision floating point operations to     
          improve its performance. See the Profiling Guide                                                              
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Kbyte       393.22
    Dropped Samples                sample            0
    Maximum Sampling Interval       cycle       20,000
    # Pass Groups                                    1
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.09
    Executed Ipc Elapsed  inst/cycle         0.08
    Issue Slots Busy               %         2.32
    Issued Ipc Active     inst/cycle         0.09
    SM Busy                        %        28.16
    -------------------- ----------- ------------

    INF   FP64 is the highest-utilized pipeline (28.2%) based on active cycles, taking into account the rates of its    
          different instructions. It executes 64-bit floating point operations. It is well-utilized, but should not be  
          a bottleneck.                                                                                                 

    Section: Memory Workload Analysis
    ----------------- ----------- ------------
    Metric Name       Metric Unit Metric Value
    ----------------- ----------- ------------
    Memory Throughput     Gbyte/s       355.76
    Mem Busy                    %        22.46
    Max Bandwidth               %        89.50
    L1/TEX Hit Rate             %        14.16
    L2 Hit Rate                 %        17.07
    Mem Pipes Busy              %         7.91
    ----------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %         2.33
    Issued Warp Per Scheduler                        0.02
    No Eligible                            %        97.67
    Active Warps Per Scheduler          warp         4.30
    Eligible Warps Per Scheduler        warp         0.03
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 10.5%                                                                                     
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 43.0 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 8 warps per scheduler, this workload allocates an average of  
          4.30 active warps per scheduler, but only an average of 0.03 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp  
          State Statistics and Source Counters sections.                                                                

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle       184.97
    Warp Cycles Per Executed Instruction           cycle       188.31
    Avg. Active Threads Per Warp                                31.99
    Avg. Not Predicated Off Threads Per Warp                    30.79
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 10.5%                                                                                           
          On average, each warp of this workload spends 174.2 cycles being stalled waiting for a scoreboard dependency  
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 94.2% of the total average of 185.0 cycles       
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Profiling Guide                                                                            
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst     1,034.84
    Executed Instructions                           inst      165,575
    Avg. Issued Instructions Per Scheduler          inst     1,053.54
    Issued Instructions                             inst      168,567
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 3.753%                                                                                          
          This kernel executes 23448 fused and 8520 non-fused FP64 instructions. By converting pairs of non-fused       
          instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point),           
          higher-throughput equivalent, the achieved FP64 performance could be increased by up to 13% (relative to its  
          current performance).                                                                                         

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     88
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              40
    Stack Size                                                 1,024
    Threads                                   thread          22,528
    # TPCs                                                        20
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                0.55
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        53.34
    Achieved Active Warps Per SM           warp        17.07
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 10.5%                                                                                           
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (53.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      192,464
    Total DRAM Elapsed Cycles        cycle    1,720,320
    Average L1 Active Cycles         cycle    45,409.60
    Total L1 Elapsed Cycles          cycle    1,994,928
    Average L2 Active Cycles         cycle    43,241.81
    Total L2 Elapsed Cycles          cycle    1,679,488
    Average SM Active Cycles         cycle    45,409.60
    Total SM Elapsed Cycles          cycle    1,994,928
    Average SMSP Active Cycles       cycle    45,306.93
    Total SMSP Elapsed Cycles        cycle    7,979,712
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.055%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 5.55% above the average, while the minimum instance value is 3.73% below the average.       
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.312%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 6.95% above the average, while the minimum instance value is 3.77% below the        
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.055%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 5.55% above the average, while the minimum instance value is 3.73% below the        
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.04
    Branch Instructions              inst        6,907
    Branch Efficiency                   %        99.95
    Avg. Divergent Branches                       0.01
    ------------------------- ----------- ------------

  cuda_right_hand_side(const double *, double *, int, double, double, double, double) (256, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.79
    SM Frequency                    Ghz         1.59
    Elapsed Cycles                cycle      356,021
    Memory Throughput                 %         3.61
    DRAM Throughput                   %         3.61
    Duration                         us       222.98
    L1/TEX Cache Throughput           %         4.53
    L2 Cache Throughput               %         3.02
    SM Active Cycles              cycle   308,247.92
    Compute (SM) Throughput           %        75.73
    ----------------------- ----------- ------------

    OPT   Compute is more heavily utilized than Memory: Look at the Compute Workload Analysis section to see what the   
          compute pipelines are spending their time doing. Also, consider whether any computation is redundant and      
          could be reduced or moved to look-up tables.                                                                  

    Section: GPU Speed Of Light Roofline Chart
    OPT   Est. Speedup: 82.3%                                                                                           
          The ratio of peak float (FP32) to double (FP64) performance on this device is 32:1. The workload achieved     
          close to 0% of this device's FP32 peak performance and 56% of its FP64 peak performance. If Compute Workload  
          Analysis determines that this workload is FP64 bound, consider using 32-bit precision floating point          
          operations to improve its performance. See the Profiling Guide                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Kbyte       393.22
    Dropped Samples                sample            0
    Maximum Sampling Interval       cycle       20,000
    # Pass Groups                                    1
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.11
    Executed Ipc Elapsed  inst/cycle         0.09
    Issue Slots Busy               %         2.65
    Issued Ipc Active     inst/cycle         0.11
    SM Busy                        %        86.97
    -------------------- ----------- ------------

    OPT   FP64 is the highest-utilized pipeline (87.0%) based on active cycles, taking into account the rates of its    
          different instructions. It executes 64-bit floating point operations. The pipeline is over-utilized and       
          likely a performance bottleneck. Based on the number of executed instructions, the highest utilized pipeline  
          (87.0%) is FP64. It executes 64-bit floating point operations. Comparing the two, the overall pipeline        
          utilization appears to be caused by frequent, low-latency instructions. See the Profiling Guide               
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-decoder) or hover over the          
          pipeline name to understand the workloads handled by each pipeline. The Instruction Statistics section shows  
          the mix of executed instructions for this workload. Check the Warp State Statistics section for which         
          reasons cause warps to stall.                                                                                 

    Section: Memory Workload Analysis
    ----------------- ----------- ------------
    Metric Name       Metric Unit Metric Value
    ----------------- ----------- ------------
    Memory Throughput     Gbyte/s        15.67
    Mem Busy                    %         2.98
    Max Bandwidth               %         3.61
    L1/TEX Hit Rate             %        79.13
    L2 Hit Rate                 %        80.55
    Mem Pipes Busy              %        19.79
    ----------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 3.243%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 9.1 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 3.398%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32     
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %         2.65
    Issued Warp Per Scheduler                        0.03
    No Eligible                            %        97.35
    Active Warps Per Scheduler          warp         6.63
    Eligible Warps Per Scheduler        warp         0.07
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 24.27%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 37.7 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 8 warps per scheduler, this workload allocates an average of  
          6.63 active warps per scheduler, but only an average of 0.07 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle       249.87
    Warp Cycles Per Executed Instruction           cycle       251.10
    Avg. Active Threads Per Warp                                31.99
    Avg. Not Predicated Off Threads Per Warp                    30.73
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 24.27%                                                                                          
          On average, each warp of this workload spends 121.1 cycles being stalled waiting for a scoreboard dependency  
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 48.5% of the total average of 249.9 cycles       
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 24.27%                                                                                          
          On average, each warp of this workload spends 120.9 cycles being stalled waiting for the L1 instruction queue 
          for texture operations to be not full. This stall reason is high in cases of extreme utilization of the       
          L1TEX pipeline. Try issuing fewer texture fetches, surface loads, surface stores, or decoupled math           
          operations. If applicable, consider combining multiple lower-width memory operations into fewer wider memory  
          operations and try interleaving memory operations and math instructions. Consider converting texture lookups  
          or surface loads into global memory lookups. Texture can accept four threads' requests per cycle, whereas     
          global accepts 32 threads. This stall type represents about 48.4% of the total average of 249.9 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Profiling Guide                                                                            
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst     8,125.43
    Executed Instructions                           inst    1,300,068
    Avg. Issued Instructions Per Scheduler          inst     8,165.50
    Issued Instructions                             inst    1,306,480
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 16.53%                                                                                          
          This kernel executes 388846 fused and 238388 non-fused FP64 instructions. By converting pairs of non-fused    
          instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point),           
          higher-throughput equivalent, the achieved FP64 performance could be increased by up to 19% (relative to its  
          current performance).                                                                                         

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    256
    Registers Per Thread             register/thread              56
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              40
    Stack Size                                                 1,024
    Threads                                   thread          65,536
    # TPCs                                                        20
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                1.60
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 97 thread blocks.   
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 50.0% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        82.74
    Achieved Active Warps Per SM           warp        26.48
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 17.26%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (82.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       54,605
    Total DRAM Elapsed Cycles        cycle   12,107,776
    Average L1 Active Cycles         cycle   308,247.92
    Total L1 Elapsed Cycles          cycle   14,160,542
    Average L2 Active Cycles         cycle    42,229.09
    Total L2 Elapsed Cycles          cycle   10,893,952
    Average SM Active Cycles         cycle   308,247.92
    Total SM Elapsed Cycles          cycle   14,160,542
    Average SMSP Active Cycles       cycle   307,888.39
    Total SMSP Elapsed Cycles        cycle   56,642,168
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 11.07%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 12.71% above the average, while the minimum instance value is 1.85% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.17%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 12.85% above the average, while the minimum instance value is 1.86% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.07%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 12.71% above the average, while the minimum instance value is 1.85% below the       
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.07
    Branch Instructions              inst       88,024
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 8.548%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 421875 excessive sectors (69% of the      
          total 612171 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source         
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  cuda_aux_computation(const double *, const double *, double *, double, int, double) (88, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.78
    SM Frequency                    Ghz         1.57
    Elapsed Cycles                cycle       25,744
    Memory Throughput                 %        87.21
    DRAM Throughput                   %        87.21
    Duration                         us        16.29
    L1/TEX Cache Throughput           %        18.03
    L2 Cache Throughput               %        24.07
    SM Active Cycles              cycle    18,085.03
    Compute (SM) Throughput           %        15.64
    ----------------------- ----------- ------------

    INF   This workload is utilizing greater than 80.0% of the available compute or memory performance of the device.   
          To further improve performance, work will likely need to be shifted from the most utilized to another unit.   
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (FP32) to double (FP64) performance on this device is 32:1. The workload achieved 0%  
          of this device's FP32 peak performance and 15% of its FP64 peak performance. See the Profiling Guide          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Kbyte       393.22
    Dropped Samples                sample            0
    Maximum Sampling Interval       cycle       20,000
    # Pass Groups                                    1
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.14
    Executed Ipc Elapsed  inst/cycle         0.12
    Issue Slots Busy               %         3.61
    Issued Ipc Active     inst/cycle         0.14
    SM Busy                        %        18.84
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 81.16%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ----------------- ----------- ------------
    Metric Name       Metric Unit Metric Value
    ----------------- ----------- ------------
    Memory Throughput     Gbyte/s       322.81
    Mem Busy                    %        24.07
    Max Bandwidth               %        87.21
    L1/TEX Hit Rate             %            0
    L2 Hit Rate                 %        33.92
    Mem Pipes Busy              %         7.33
    ----------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %         3.64
    Issued Warp Per Scheduler                        0.04
    No Eligible                            %        96.36
    Active Warps Per Scheduler          warp         4.28
    Eligible Warps Per Scheduler        warp         0.04
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 12.79%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 27.5 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 8 warps per scheduler, this workload allocates an average of  
          4.28 active warps per scheduler, but only an average of 0.04 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp  
          State Statistics and Source Counters sections.                                                                

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle       117.64
    Warp Cycles Per Executed Instruction           cycle       120.79
    Avg. Active Threads Per Warp                                31.99
    Avg. Not Predicated Off Threads Per Warp                    30.04
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 12.79%                                                                                          
          On average, each warp of this workload spends 107.4 cycles being stalled waiting for a scoreboard dependency  
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 91.3% of the total average of 117.6 cycles       
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Profiling Guide                                                                            
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       636.09
    Executed Instructions                           inst      101,775
    Avg. Issued Instructions Per Scheduler          inst       653.09
    Issued Instructions                             inst      104,495
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     88
    Registers Per Thread             register/thread              28
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              40
    Stack Size                                                 1,024
    Threads                                   thread          22,528
    # TPCs                                                        20
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                0.55
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        52.97
    Achieved Active Warps Per SM           warp        16.95
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 12.79%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (53.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    82,155.50
    Total DRAM Elapsed Cycles        cycle      753,664
    Average L1 Active Cycles         cycle    18,085.03
    Total L1 Elapsed Cycles          cycle      871,658
    Average L2 Active Cycles         cycle    17,140.56
    Total L2 Elapsed Cycles          cycle      787,776
    Average SM Active Cycles         cycle    18,085.03
    Total SM Elapsed Cycles          cycle      871,658
    Average SMSP Active Cycles       cycle    17,965.94
    Total SMSP Elapsed Cycles        cycle    3,486,632
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 8.13%                                                                                           
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 9.80% above the average, while the minimum instance value is 4.44% below the average.       
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 9.601%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 11.65% above the average, while the minimum instance value is 5.09% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.13%                                                                                           
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 9.80% above the average, while the minimum instance value is 4.44% below the        
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.07
    Branch Instructions              inst        6,907
    Branch Efficiency                   %        99.95
    Avg. Divergent Branches                       0.01
    ------------------------- ----------- ------------

  cuda_right_hand_side(const double *, double *, int, double, double, double, double) (256, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.73
    SM Frequency                    Ghz         1.59
    Elapsed Cycles                cycle      355,666
    Memory Throughput                 %         3.65
    DRAM Throughput                   %         3.65
    Duration                         us       222.78
    L1/TEX Cache Throughput           %         4.52
    L2 Cache Throughput               %         3.01
    SM Active Cycles              cycle   308,166.47
    Compute (SM) Throughput           %        75.62
    ----------------------- ----------- ------------

    OPT   Compute is more heavily utilized than Memory: Look at the Compute Workload Analysis section to see what the   
          compute pipelines are spending their time doing. Also, consider whether any computation is redundant and      
          could be reduced or moved to look-up tables.                                                                  

    Section: GPU Speed Of Light Roofline Chart
    OPT   Est. Speedup: 82.32%                                                                                          
          The ratio of peak float (FP32) to double (FP64) performance on this device is 32:1. The workload achieved     
          close to 0% of this device's FP32 peak performance and 56% of its FP64 peak performance. If Compute Workload  
          Analysis determines that this workload is FP64 bound, consider using 32-bit precision floating point          
          operations to improve its performance. See the Profiling Guide                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Kbyte       393.22
    Dropped Samples                sample            0
    Maximum Sampling Interval       cycle       20,000
    # Pass Groups                                    1
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.11
    Executed Ipc Elapsed  inst/cycle         0.09
    Issue Slots Busy               %         2.65
    Issued Ipc Active     inst/cycle         0.11
    SM Busy                        %        86.99
    -------------------- ----------- ------------

    OPT   FP64 is the highest-utilized pipeline (87.0%) based on active cycles, taking into account the rates of its    
          different instructions. It executes 64-bit floating point operations. The pipeline is over-utilized and       
          likely a performance bottleneck. Based on the number of executed instructions, the highest utilized pipeline  
          (87.0%) is FP64. It executes 64-bit floating point operations. Comparing the two, the overall pipeline        
          utilization appears to be caused by frequent, low-latency instructions. See the Profiling Guide               
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-decoder) or hover over the          
          pipeline name to understand the workloads handled by each pipeline. The Instruction Statistics section shows  
          the mix of executed instructions for this workload. Check the Warp State Statistics section for which         
          reasons cause warps to stall.                                                                                 

    Section: Memory Workload Analysis
    ----------------- ----------- ------------
    Metric Name       Metric Unit Metric Value
    ----------------- ----------- ------------
    Memory Throughput     Gbyte/s        15.72
    Mem Busy                    %         2.99
    Max Bandwidth               %         3.65
    L1/TEX Hit Rate             %        79.25
    L2 Hit Rate                 %        80.39
    Mem Pipes Busy              %        19.76
    ----------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 3.239%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 9.1 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 3.394%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32     
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %         2.65
    Issued Warp Per Scheduler                        0.03
    No Eligible                            %        97.35
    Active Warps Per Scheduler          warp         6.63
    Eligible Warps Per Scheduler        warp         0.07
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 24.38%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 37.7 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 8 warps per scheduler, this workload allocates an average of  
          6.63 active warps per scheduler, but only an average of 0.07 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle       249.92
    Warp Cycles Per Executed Instruction           cycle       251.16
    Avg. Active Threads Per Warp                                31.99
    Avg. Not Predicated Off Threads Per Warp                    30.70
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 24.38%                                                                                          
          On average, each warp of this workload spends 121.2 cycles being stalled waiting for a scoreboard dependency  
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 48.5% of the total average of 249.9 cycles       
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 24.38%                                                                                          
          On average, each warp of this workload spends 120.9 cycles being stalled waiting for the L1 instruction queue 
          for texture operations to be not full. This stall reason is high in cases of extreme utilization of the       
          L1TEX pipeline. Try issuing fewer texture fetches, surface loads, surface stores, or decoupled math           
          operations. If applicable, consider combining multiple lower-width memory operations into fewer wider memory  
          operations and try interleaving memory operations and math instructions. Consider converting texture lookups  
          or surface loads into global memory lookups. Texture can accept four threads' requests per cycle, whereas     
          global accepts 32 threads. This stall type represents about 48.4% of the total average of 249.9 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Profiling Guide                                                                            
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst     8,125.43
    Executed Instructions                           inst    1,300,068
    Avg. Issued Instructions Per Scheduler          inst     8,165.61
    Issued Instructions                             inst    1,306,498
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 16.53%                                                                                          
          This kernel executes 388846 fused and 238388 non-fused FP64 instructions. By converting pairs of non-fused    
          instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point),           
          higher-throughput equivalent, the achieved FP64 performance could be increased by up to 19% (relative to its  
          current performance).                                                                                         

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    256
    Registers Per Thread             register/thread              56
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              40
    Stack Size                                                 1,024
    Threads                                   thread          65,536
    # TPCs                                                        20
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                1.60
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 97 thread blocks.   
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 50.0% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        82.79
    Achieved Active Warps Per SM           warp        26.49
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 17.21%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (82.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       54,711
    Total DRAM Elapsed Cycles        cycle   11,997,184
    Average L1 Active Cycles         cycle   308,166.47
    Total L1 Elapsed Cycles          cycle   14,180,224
    Average L2 Active Cycles         cycle    42,062.78
    Total L2 Elapsed Cycles          cycle   10,888,704
    Average SM Active Cycles         cycle   308,166.47
    Total SM Elapsed Cycles          cycle   14,180,224
    Average SMSP Active Cycles       cycle   308,002.21
    Total SMSP Elapsed Cycles        cycle   56,720,896
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 11.09%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 12.76% above the average, while the minimum instance value is 1.87% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.13%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 12.81% above the average, while the minimum instance value is 1.93% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.09%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 12.76% above the average, while the minimum instance value is 1.87% below the       
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.07
    Branch Instructions              inst       88,024
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 8.519%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 421875 excessive sectors (69% of the      
          total 612171 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source         
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  cuda_aux_computation(const double *, const double *, double *, double, int, double) (88, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.70
    SM Frequency                    Ghz         1.57
    Elapsed Cycles                cycle       25,649
    Memory Throughput                 %        89.57
    DRAM Throughput                   %        89.57
    Duration                         us        16.26
    L1/TEX Cache Throughput           %        18.26
    L2 Cache Throughput               %        24.12
    SM Active Cycles              cycle    18,034.08
    Compute (SM) Throughput           %        15.83
    ----------------------- ----------- ------------

    INF   This workload is utilizing greater than 80.0% of the available compute or memory performance of the device.   
          To further improve performance, work will likely need to be shifted from the most utilized to another unit.   
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: GPU Speed Of Light Roofline Chart
    OPT   Est. Speedup: 18.31%                                                                                          
          The ratio of peak float (FP32) to double (FP64) performance on this device is 32:1. The workload achieved 0%  
          of this device's FP32 peak performance and 15% of its FP64 peak performance. If Compute Workload Analysis     
          determines that this workload is FP64 bound, consider using 32-bit precision floating point operations to     
          improve its performance. See the Profiling Guide                                                              
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Kbyte       393.22
    Dropped Samples                sample            0
    Maximum Sampling Interval       cycle       20,000
    # Pass Groups                                    1
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.14
    Executed Ipc Elapsed  inst/cycle         0.12
    Issue Slots Busy               %         3.62
    Issued Ipc Active     inst/cycle         0.14
    SM Busy                        %        18.90
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 81.1%                                                                                     
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ----------------- ----------- ------------
    Metric Name       Metric Unit Metric Value
    ----------------- ----------- ------------
    Memory Throughput     Gbyte/s       326.78
    Mem Busy                    %        24.12
    Max Bandwidth               %        89.57
    L1/TEX Hit Rate             %            0
    L2 Hit Rate                 %        33.94
    Mem Pipes Busy              %         7.42
    ----------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %         3.62
    Issued Warp Per Scheduler                        0.04
    No Eligible                            %        96.38
    Active Warps Per Scheduler          warp         4.26
    Eligible Warps Per Scheduler        warp         0.04
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 10.43%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 27.6 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 8 warps per scheduler, this workload allocates an average of  
          4.26 active warps per scheduler, but only an average of 0.04 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp  
          State Statistics and Source Counters sections.                                                                

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle       117.57
    Warp Cycles Per Executed Instruction           cycle       120.71
    Avg. Active Threads Per Warp                                31.99
    Avg. Not Predicated Off Threads Per Warp                    30.04
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 10.43%                                                                                          
          On average, each warp of this workload spends 107.9 cycles being stalled waiting for a scoreboard dependency  
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 91.8% of the total average of 117.6 cycles       
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Profiling Guide                                                                            
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       636.09
    Executed Instructions                           inst      101,775
    Avg. Issued Instructions Per Scheduler          inst       653.12
    Issued Instructions                             inst      104,499
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     88
    Registers Per Thread             register/thread              28
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              40
    Stack Size                                                 1,024
    Threads                                   thread          22,528
    # TPCs                                                        20
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                0.55
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        53.35
    Achieved Active Warps Per SM           warp        17.07
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 10.43%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (53.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       83,003
    Total DRAM Elapsed Cycles        cycle      741,376
    Average L1 Active Cycles         cycle    18,034.08
    Total L1 Elapsed Cycles          cycle      861,316
    Average L2 Active Cycles         cycle    17,110.84
    Total L2 Elapsed Cycles          cycle      785,504
    Average SM Active Cycles         cycle    18,034.08
    Total SM Elapsed Cycles          cycle      861,316
    Average SMSP Active Cycles       cycle    18,019.40
    Total SMSP Elapsed Cycles        cycle    3,445,264
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 7.277%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 8.69% above the average, while the minimum instance value is 3.62% below the average.       
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.022%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 9.59% above the average, while the minimum instance value is 4.70% below the        
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.277%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 8.69% above the average, while the minimum instance value is 3.62% below the        
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.07
    Branch Instructions              inst        6,907
    Branch Efficiency                   %        99.95
    Avg. Divergent Branches                       0.01
    ------------------------- ----------- ------------

  cuda_right_hand_side(const double *, double *, int, double, double, double, double) (256, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.77
    SM Frequency                    Ghz         1.60
    Elapsed Cycles                cycle      356,095
    Memory Throughput                 %         3.66
    DRAM Throughput                   %         3.66
    Duration                         us       220.96
    L1/TEX Cache Throughput           %         4.52
    L2 Cache Throughput               %         3.04
    SM Active Cycles              cycle   308,196.33
    Compute (SM) Throughput           %        75.63
    ----------------------- ----------- ------------

    OPT   Compute is more heavily utilized than Memory: Look at the Compute Workload Analysis section to see what the   
          compute pipelines are spending their time doing. Also, consider whether any computation is redundant and      
          could be reduced or moved to look-up tables.                                                                  

    Section: GPU Speed Of Light Roofline Chart
    OPT   Est. Speedup: 82.32%                                                                                          
          The ratio of peak float (FP32) to double (FP64) performance on this device is 32:1. The workload achieved     
          close to 0% of this device's FP32 peak performance and 56% of its FP64 peak performance. If Compute Workload  
          Analysis determines that this workload is FP64 bound, consider using 32-bit precision floating point          
          operations to improve its performance. See the Profiling Guide                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Kbyte       393.22
    Dropped Samples                sample            0
    Maximum Sampling Interval       cycle       20,000
    # Pass Groups                                    1
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.11
    Executed Ipc Elapsed  inst/cycle         0.09
    Issue Slots Busy               %         2.65
    Issued Ipc Active     inst/cycle         0.11
    SM Busy                        %        86.99
    -------------------- ----------- ------------

    OPT   FP64 is the highest-utilized pipeline (87.0%) based on active cycles, taking into account the rates of its    
          different instructions. It executes 64-bit floating point operations. The pipeline is over-utilized and       
          likely a performance bottleneck. Based on the number of executed instructions, the highest utilized pipeline  
          (87.0%) is FP64. It executes 64-bit floating point operations. Comparing the two, the overall pipeline        
          utilization appears to be caused by frequent, low-latency instructions. See the Profiling Guide               
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-decoder) or hover over the          
          pipeline name to understand the workloads handled by each pipeline. The Instruction Statistics section shows  
          the mix of executed instructions for this workload. Check the Warp State Statistics section for which         
          reasons cause warps to stall.                                                                                 

    Section: Memory Workload Analysis
    ----------------- ----------- ------------
    Metric Name       Metric Unit Metric Value
    ----------------- ----------- ------------
    Memory Throughput     Gbyte/s        15.84
    Mem Busy                    %         3.00
    Max Bandwidth               %         3.66
    L1/TEX Hit Rate             %        79.27
    L2 Hit Rate                 %        80.25
    Mem Pipes Busy              %        19.77
    ----------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 3.238%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 9.1 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 3.393%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32     
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %         2.65
    Issued Warp Per Scheduler                        0.03
    No Eligible                            %        97.35
    Active Warps Per Scheduler          warp         6.63
    Eligible Warps Per Scheduler        warp         0.07
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 24.37%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 37.7 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 8 warps per scheduler, this workload allocates an average of  
          6.63 active warps per scheduler, but only an average of 0.07 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle       249.87
    Warp Cycles Per Executed Instruction           cycle       251.10
    Avg. Active Threads Per Warp                                31.99
    Avg. Not Predicated Off Threads Per Warp                    30.70
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 24.37%                                                                                          
          On average, each warp of this workload spends 121.2 cycles being stalled waiting for a scoreboard dependency  
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 48.5% of the total average of 249.9 cycles       
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 24.37%                                                                                          
          On average, each warp of this workload spends 120.9 cycles being stalled waiting for the L1 instruction queue 
          for texture operations to be not full. This stall reason is high in cases of extreme utilization of the       
          L1TEX pipeline. Try issuing fewer texture fetches, surface loads, surface stores, or decoupled math           
          operations. If applicable, consider combining multiple lower-width memory operations into fewer wider memory  
          operations and try interleaving memory operations and math instructions. Consider converting texture lookups  
          or surface loads into global memory lookups. Texture can accept four threads' requests per cycle, whereas     
          global accepts 32 threads. This stall type represents about 48.4% of the total average of 249.9 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Profiling Guide                                                                            
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst     8,125.43
    Executed Instructions                           inst    1,300,068
    Avg. Issued Instructions Per Scheduler          inst     8,165.49
    Issued Instructions                             inst    1,306,479
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 16.53%                                                                                          
          This kernel executes 388846 fused and 238388 non-fused FP64 instructions. By converting pairs of non-fused    
          instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point),           
          higher-throughput equivalent, the achieved FP64 performance could be increased by up to 19% (relative to its  
          current performance).                                                                                         

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    256
    Registers Per Thread             register/thread              56
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              40
    Stack Size                                                 1,024
    Threads                                   thread          65,536
    # TPCs                                                        20
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                1.60
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 97 thread blocks.   
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 50.0% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        82.77
    Achieved Active Warps Per SM           warp        26.49
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 17.23%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (82.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       54,684
    Total DRAM Elapsed Cycles        cycle   11,968,512
    Average L1 Active Cycles         cycle   308,196.33
    Total L1 Elapsed Cycles          cycle   14,178,098
    Average L2 Active Cycles         cycle    42,129.97
    Total L2 Elapsed Cycles          cycle   10,811,936
    Average SM Active Cycles         cycle   308,196.33
    Total SM Elapsed Cycles          cycle   14,178,098
    Average SMSP Active Cycles       cycle   307,946.57
    Total SMSP Elapsed Cycles        cycle   56,712,392
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 11.13%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 12.80% above the average, while the minimum instance value is 1.90% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.16%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 12.84% above the average, while the minimum instance value is 1.91% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.13%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 12.80% above the average, while the minimum instance value is 1.90% below the       
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.07
    Branch Instructions              inst       88,024
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 8.593%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 421875 excessive sectors (69% of the      
          total 612171 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source         
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  cuda_aux_computation(const double *, const double *, double *, double, int, double) (88, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.70
    SM Frequency                    Ghz         1.57
    Elapsed Cycles                cycle       26,210
    Memory Throughput                 %        87.68
    DRAM Throughput                   %        87.68
    Duration                         us        16.61
    L1/TEX Cache Throughput           %        18.20
    L2 Cache Throughput               %        23.56
    SM Active Cycles              cycle    18,184.08
    Compute (SM) Throughput           %        15.80
    ----------------------- ----------- ------------

    INF   This workload is utilizing greater than 80.0% of the available compute or memory performance of the device.   
          To further improve performance, work will likely need to be shifted from the most utilized to another unit.   
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: GPU Speed Of Light Roofline Chart
    OPT   Est. Speedup: 18.16%                                                                                          
          The ratio of peak float (FP32) to double (FP64) performance on this device is 32:1. The workload achieved 0%  
          of this device's FP32 peak performance and 15% of its FP64 peak performance. If Compute Workload Analysis     
          determines that this workload is FP64 bound, consider using 32-bit precision floating point operations to     
          improve its performance. See the Profiling Guide                                                              
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Kbyte       393.22
    Dropped Samples                sample            0
    Maximum Sampling Interval       cycle       20,000
    # Pass Groups                                    1
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.14
    Executed Ipc Elapsed  inst/cycle         0.12
    Issue Slots Busy               %         3.59
    Issued Ipc Active     inst/cycle         0.14
    SM Busy                        %        18.74
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 81.26%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ----------------- ----------- ------------
    Metric Name       Metric Unit Metric Value
    ----------------- ----------- ------------
    Memory Throughput     Gbyte/s       320.03
    Mem Busy                    %        23.56
    Max Bandwidth               %        87.68
    L1/TEX Hit Rate             %            0
    L2 Hit Rate                 %        33.98
    Mem Pipes Busy              %         7.41
    ----------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %         3.56
    Issued Warp Per Scheduler                        0.04
    No Eligible                            %        96.44
    Active Warps Per Scheduler          warp         4.21
    Eligible Warps Per Scheduler        warp         0.04
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 12.32%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 28.1 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 8 warps per scheduler, this workload allocates an average of  
          4.21 active warps per scheduler, but only an average of 0.04 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp  
          State Statistics and Source Counters sections.                                                                

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle       118.19
    Warp Cycles Per Executed Instruction           cycle       121.35
    Avg. Active Threads Per Warp                                31.99
    Avg. Not Predicated Off Threads Per Warp                    30.04
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 12.32%                                                                                          
          On average, each warp of this workload spends 107.3 cycles being stalled waiting for a scoreboard dependency  
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 90.8% of the total average of 118.2 cycles       
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Profiling Guide                                                                            
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       636.09
    Executed Instructions                           inst      101,775
    Avg. Issued Instructions Per Scheduler          inst       653.12
    Issued Instructions                             inst      104,500
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     88
    Registers Per Thread             register/thread              28
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              40
    Stack Size                                                 1,024
    Threads                                   thread          22,528
    # TPCs                                                        20
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                0.55
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        52.66
    Achieved Active Warps Per SM           warp        16.85
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 12.32%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (52.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       83,047
    Total DRAM Elapsed Cycles        cycle      757,760
    Average L1 Active Cycles         cycle    18,184.08
    Total L1 Elapsed Cycles          cycle      862,764
    Average L2 Active Cycles         cycle    17,366.56
    Total L2 Elapsed Cycles          cycle      803,904
    Average SM Active Cycles         cycle    18,184.08
    Total SM Elapsed Cycles          cycle      862,764
    Average SMSP Active Cycles       cycle    18,329.08
    Total SMSP Elapsed Cycles        cycle    3,451,056
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 8.18%                                                                                           
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 9.70% above the average, while the minimum instance value is 5.82% below the average.       
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 9.239%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 10.87% above the average, while the minimum instance value is 7.19% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.18%                                                                                           
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 9.70% above the average, while the minimum instance value is 5.82% below the        
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.07
    Branch Instructions              inst        6,907
    Branch Efficiency                   %        99.95
    Avg. Divergent Branches                       0.01
    ------------------------- ----------- ------------

  cuda_right_hand_side(const double *, double *, int, double, double, double, double) (256, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.70
    SM Frequency                    Ghz         1.59
    Elapsed Cycles                cycle      356,272
    Memory Throughput                 %         3.64
    DRAM Throughput                   %         3.64
    Duration                         us       223.17
    L1/TEX Cache Throughput           %         4.54
    L2 Cache Throughput               %         3.02
    SM Active Cycles              cycle   308,255.97
    Compute (SM) Throughput           %        75.85
    ----------------------- ----------- ------------

    OPT   Compute is more heavily utilized than Memory: Look at the Compute Workload Analysis section to see what the   
          compute pipelines are spending their time doing. Also, consider whether any computation is redundant and      
          could be reduced or moved to look-up tables.                                                                  

    Section: GPU Speed Of Light Roofline Chart
    OPT   Est. Speedup: 82.3%                                                                                           
          The ratio of peak float (FP32) to double (FP64) performance on this device is 32:1. The workload achieved     
          close to 0% of this device's FP32 peak performance and 56% of its FP64 peak performance. If Compute Workload  
          Analysis determines that this workload is FP64 bound, consider using 32-bit precision floating point          
          operations to improve its performance. See the Profiling Guide                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Kbyte       393.22
    Dropped Samples                sample            0
    Maximum Sampling Interval       cycle       20,000
    # Pass Groups                                    1
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.11
    Executed Ipc Elapsed  inst/cycle         0.09
    Issue Slots Busy               %         2.65
    Issued Ipc Active     inst/cycle         0.11
    SM Busy                        %        86.97
    -------------------- ----------- ------------

    OPT   FP64 is the highest-utilized pipeline (87.0%) based on active cycles, taking into account the rates of its    
          different instructions. It executes 64-bit floating point operations. The pipeline is over-utilized and       
          likely a performance bottleneck. Based on the number of executed instructions, the highest utilized pipeline  
          (87.0%) is FP64. It executes 64-bit floating point operations. Comparing the two, the overall pipeline        
          utilization appears to be caused by frequent, low-latency instructions. See the Profiling Guide               
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-decoder) or hover over the          
          pipeline name to understand the workloads handled by each pipeline. The Instruction Statistics section shows  
          the mix of executed instructions for this workload. Check the Warp State Statistics section for which         
          reasons cause warps to stall.                                                                                 

    Section: Memory Workload Analysis
    ----------------- ----------- ------------
    Metric Name       Metric Unit Metric Value
    ----------------- ----------- ------------
    Memory Throughput     Gbyte/s        15.63
    Mem Busy                    %         2.97
    Max Bandwidth               %         3.64
    L1/TEX Hit Rate             %        79.25
    L2 Hit Rate                 %        80.83
    Mem Pipes Busy              %        19.82
    ----------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 3.248%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 9.1 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 3.403%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32     
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %         2.65
    Issued Warp Per Scheduler                        0.03
    No Eligible                            %        97.35
    Active Warps Per Scheduler          warp         6.62
    Eligible Warps Per Scheduler        warp         0.07
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 24.15%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 37.7 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 8 warps per scheduler, this workload allocates an average of  
          6.62 active warps per scheduler, but only an average of 0.07 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle       249.72
    Warp Cycles Per Executed Instruction           cycle       251.02
    Avg. Active Threads Per Warp                                31.99
    Avg. Not Predicated Off Threads Per Warp                    30.68
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 24.15%                                                                                          
          On average, each warp of this workload spends 121.1 cycles being stalled waiting for a scoreboard dependency  
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 48.5% of the total average of 249.7 cycles       
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 24.15%                                                                                          
          On average, each warp of this workload spends 120.7 cycles being stalled waiting for the L1 instruction queue 
          for texture operations to be not full. This stall reason is high in cases of extreme utilization of the       
          L1TEX pipeline. Try issuing fewer texture fetches, surface loads, surface stores, or decoupled math           
          operations. If applicable, consider combining multiple lower-width memory operations into fewer wider memory  
          operations and try interleaving memory operations and math instructions. Consider converting texture lookups  
          or surface loads into global memory lookups. Texture can accept four threads' requests per cycle, whereas     
          global accepts 32 threads. This stall type represents about 48.4% of the total average of 249.7 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Profiling Guide                                                                            
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst     8,125.43
    Executed Instructions                           inst    1,300,068
    Avg. Issued Instructions Per Scheduler          inst     8,167.56
    Issued Instructions                             inst    1,306,809
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 16.53%                                                                                          
          This kernel executes 388846 fused and 238388 non-fused FP64 instructions. By converting pairs of non-fused    
          instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point),           
          higher-throughput equivalent, the achieved FP64 performance could be increased by up to 19% (relative to its  
          current performance).                                                                                         

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    256
    Registers Per Thread             register/thread              56
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              40
    Stack Size                                                 1,024
    Threads                                   thread          65,536
    # TPCs                                                        20
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                1.60
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 97 thread blocks.   
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 50.0% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        82.75
    Achieved Active Warps Per SM           warp        26.48
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 17.25%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (82.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    54,496.50
    Total DRAM Elapsed Cycles        cycle   11,964,416
    Average L1 Active Cycles         cycle   308,255.97
    Total L1 Elapsed Cycles          cycle   14,138,392
    Average L2 Active Cycles         cycle    42,354.12
    Total L2 Elapsed Cycles          cycle   10,899,520
    Average SM Active Cycles         cycle   308,255.97
    Total SM Elapsed Cycles          cycle   14,138,392
    Average SMSP Active Cycles       cycle   307,981.79
    Total SMSP Elapsed Cycles        cycle   56,553,568
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 11.14%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 12.77% above the average, while the minimum instance value is 1.84% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.18%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 12.83% above the average, while the minimum instance value is 1.85% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.14%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 12.77% above the average, while the minimum instance value is 1.84% below the       
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.07
    Branch Instructions              inst       88,024
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 8.569%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 421875 excessive sectors (69% of the      
          total 612171 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source         
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  cuda_final_update(double *, const double *, const double *, const double *, const double *, double, int) (88, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Ghz         1.58
    Elapsed Cycles                cycle       54,547
    Memory Throughput                 %        90.72
    DRAM Throughput                   %        90.72
    Duration                         us        34.50
    L1/TEX Cache Throughput           %        17.22
    L2 Cache Throughput               %        22.57
    SM Active Cycles              cycle    45,368.65
    Compute (SM) Throughput           %        25.65
    ----------------------- ----------- ------------

    INF   This workload is utilizing greater than 80.0% of the available compute or memory performance of the device.   
          To further improve performance, work will likely need to be shifted from the most utilized to another unit.   
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: GPU Speed Of Light Roofline Chart
    OPT   Est. Speedup: 27.3%                                                                                           
          The ratio of peak float (FP32) to double (FP64) performance on this device is 32:1. The workload achieved 0%  
          of this device's FP32 peak performance and 22% of its FP64 peak performance. If Compute Workload Analysis     
          determines that this workload is FP64 bound, consider using 32-bit precision floating point operations to     
          improve its performance. See the Profiling Guide                                                              
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Kbyte       393.22
    Dropped Samples                sample            0
    Maximum Sampling Interval       cycle       20,000
    # Pass Groups                                    1
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.09
    Executed Ipc Elapsed  inst/cycle         0.08
    Issue Slots Busy               %         2.32
    Issued Ipc Active     inst/cycle         0.09
    SM Busy                        %        28.19
    -------------------- ----------- ------------

    INF   FP64 is the highest-utilized pipeline (28.2%) based on active cycles, taking into account the rates of its    
          different instructions. It executes 64-bit floating point operations. It is well-utilized, but should not be  
          a bottleneck.                                                                                                 

    Section: Memory Workload Analysis
    ----------------- ----------- ------------
    Metric Name       Metric Unit Metric Value
    ----------------- ----------- ------------
    Memory Throughput     Gbyte/s       357.64
    Mem Busy                    %        22.57
    Max Bandwidth               %        90.72
    L1/TEX Hit Rate             %        14.10
    L2 Hit Rate                 %        17.07
    Mem Pipes Busy              %         7.91
    ----------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %         2.34
    Issued Warp Per Scheduler                        0.02
    No Eligible                            %        97.66
    Active Warps Per Scheduler          warp         4.31
    Eligible Warps Per Scheduler        warp         0.03
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 9.278%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 42.8 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 8 warps per scheduler, this workload allocates an average of  
          4.31 active warps per scheduler, but only an average of 0.03 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp  
          State Statistics and Source Counters sections.                                                                

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle       184.45
    Warp Cycles Per Executed Instruction           cycle       187.79
    Avg. Active Threads Per Warp                                31.99
    Avg. Not Predicated Off Threads Per Warp                    30.79
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 9.278%                                                                                          
          On average, each warp of this workload spends 173.0 cycles being stalled waiting for a scoreboard dependency  
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 93.8% of the total average of 184.4 cycles       
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Profiling Guide                                                                            
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst     1,034.84
    Executed Instructions                           inst      165,575
    Avg. Issued Instructions Per Scheduler          inst     1,053.58
    Issued Instructions                             inst      168,572
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 3.756%                                                                                          
          This kernel executes 23448 fused and 8520 non-fused FP64 instructions. By converting pairs of non-fused       
          instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point),           
          higher-throughput equivalent, the achieved FP64 performance could be increased by up to 13% (relative to its  
          current performance).                                                                                         

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     88
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              40
    Stack Size                                                 1,024
    Threads                                   thread          22,528
    # TPCs                                                        20
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                0.55
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        53.14
    Achieved Active Warps Per SM           warp        17.00
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 9.278%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (53.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      192,766
    Total DRAM Elapsed Cycles        cycle    1,699,840
    Average L1 Active Cycles         cycle    45,368.65
    Total L1 Elapsed Cycles          cycle    1,994,290
    Average L2 Active Cycles         cycle    43,296.31
    Total L2 Elapsed Cycles          cycle    1,671,072
    Average SM Active Cycles         cycle    45,368.65
    Total SM Elapsed Cycles          cycle    1,994,290
    Average SMSP Active Cycles       cycle    45,117.46
    Total SMSP Elapsed Cycles        cycle    7,977,160
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.397%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 5.93% above the average, while the minimum instance value is 2.82% below the average.       
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.594%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 6.18% above the average, while the minimum instance value is 3.21% below the        
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.397%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 5.93% above the average, while the minimum instance value is 2.82% below the        
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.04
    Branch Instructions              inst        6,907
    Branch Efficiency                   %        99.95
    Avg. Divergent Branches                       0.01
    ------------------------- ----------- ------------

  cuda_right_hand_side(const double *, double *, int, double, double, double, double) (256, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.77
    SM Frequency                    Ghz         1.60
    Elapsed Cycles                cycle      357,678
    Memory Throughput                 %         3.63
    DRAM Throughput                   %         3.63
    Duration                         us       221.98
    L1/TEX Cache Throughput           %         4.54
    L2 Cache Throughput               %         3.03
    SM Active Cycles              cycle   308,271.28
    Compute (SM) Throughput           %        75.87
    ----------------------- ----------- ------------

    OPT   Compute is more heavily utilized than Memory: Look at the Compute Workload Analysis section to see what the   
          compute pipelines are spending their time doing. Also, consider whether any computation is redundant and      
          could be reduced or moved to look-up tables.                                                                  

    Section: GPU Speed Of Light Roofline Chart
    OPT   Est. Speedup: 82.3%                                                                                           
          The ratio of peak float (FP32) to double (FP64) performance on this device is 32:1. The workload achieved     
          close to 0% of this device's FP32 peak performance and 56% of its FP64 peak performance. If Compute Workload  
          Analysis determines that this workload is FP64 bound, consider using 32-bit precision floating point          
          operations to improve its performance. See the Profiling Guide                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Kbyte       393.22
    Dropped Samples                sample            0
    Maximum Sampling Interval       cycle       20,000
    # Pass Groups                                    1
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.11
    Executed Ipc Elapsed  inst/cycle         0.09
    Issue Slots Busy               %         2.65
    Issued Ipc Active     inst/cycle         0.11
    SM Busy                        %        86.97
    -------------------- ----------- ------------

    OPT   FP64 is the highest-utilized pipeline (87.0%) based on active cycles, taking into account the rates of its    
          different instructions. It executes 64-bit floating point operations. The pipeline is over-utilized and       
          likely a performance bottleneck. Based on the number of executed instructions, the highest utilized pipeline  
          (87.0%) is FP64. It executes 64-bit floating point operations. Comparing the two, the overall pipeline        
          utilization appears to be caused by frequent, low-latency instructions. See the Profiling Guide               
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-decoder) or hover over the          
          pipeline name to understand the workloads handled by each pipeline. The Instruction Statistics section shows  
          the mix of executed instructions for this workload. Check the Warp State Statistics section for which         
          reasons cause warps to stall.                                                                                 

    Section: Memory Workload Analysis
    ----------------- ----------- ------------
    Metric Name       Metric Unit Metric Value
    ----------------- ----------- ------------
    Memory Throughput     Gbyte/s        15.72
    Mem Busy                    %         2.99
    Max Bandwidth               %         3.63
    L1/TEX Hit Rate             %        79.33
    L2 Hit Rate                 %        80.41
    Mem Pipes Busy              %        19.83
    ----------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 3.249%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 9.1 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 3.405%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32     
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %         2.65
    Issued Warp Per Scheduler                        0.03
    No Eligible                            %        97.35
    Active Warps Per Scheduler          warp         6.62
    Eligible Warps Per Scheduler        warp         0.07
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 24.13%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 37.7 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 8 warps per scheduler, this workload allocates an average of  
          6.62 active warps per scheduler, but only an average of 0.07 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle       249.78
    Warp Cycles Per Executed Instruction           cycle       251.01
    Avg. Active Threads Per Warp                                31.99
    Avg. Not Predicated Off Threads Per Warp                    30.68
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 24.13%                                                                                          
          On average, each warp of this workload spends 121.2 cycles being stalled waiting for a scoreboard dependency  
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 48.5% of the total average of 249.8 cycles       
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 24.13%                                                                                          
          On average, each warp of this workload spends 121.0 cycles being stalled waiting for the L1 instruction queue 
          for texture operations to be not full. This stall reason is high in cases of extreme utilization of the       
          L1TEX pipeline. Try issuing fewer texture fetches, surface loads, surface stores, or decoupled math           
          operations. If applicable, consider combining multiple lower-width memory operations into fewer wider memory  
          operations and try interleaving memory operations and math instructions. Consider converting texture lookups  
          or surface loads into global memory lookups. Texture can accept four threads' requests per cycle, whereas     
          global accepts 32 threads. This stall type represents about 48.4% of the total average of 249.8 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Profiling Guide                                                                            
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst     8,125.43
    Executed Instructions                           inst    1,300,068
    Avg. Issued Instructions Per Scheduler          inst     8,165.46
    Issued Instructions                             inst    1,306,473
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 16.53%                                                                                          
          This kernel executes 388846 fused and 238388 non-fused FP64 instructions. By converting pairs of non-fused    
          instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point),           
          higher-throughput equivalent, the achieved FP64 performance could be increased by up to 19% (relative to its  
          current performance).                                                                                         

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    256
    Registers Per Thread             register/thread              56
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              40
    Stack Size                                                 1,024
    Threads                                   thread          65,536
    # TPCs                                                        20
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                1.60
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 97 thread blocks.   
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 50.0% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        82.74
    Achieved Active Warps Per SM           warp        26.48
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 17.26%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (82.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       54,531
    Total DRAM Elapsed Cycles        cycle   12,025,856
    Average L1 Active Cycles         cycle   308,271.28
    Total L1 Elapsed Cycles          cycle   14,133,288
    Average L2 Active Cycles         cycle    42,432.56
    Total L2 Elapsed Cycles          cycle   10,861,728
    Average SM Active Cycles         cycle   308,271.28
    Total SM Elapsed Cycles          cycle   14,133,288
    Average SMSP Active Cycles       cycle   307,929.88
    Total SMSP Elapsed Cycles        cycle   56,533,152
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 11.13%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 12.76% above the average, while the minimum instance value is 1.88% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.21%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 12.86% above the average, while the minimum instance value is 1.92% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.13%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 12.76% above the average, while the minimum instance value is 1.88% below the       
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.07
    Branch Instructions              inst       88,024
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 8.615%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 421875 excessive sectors (69% of the      
          total 612171 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source         
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  cuda_aux_computation(const double *, const double *, double *, double, int, double) (88, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.79
    SM Frequency                    Ghz         1.57
    Elapsed Cycles                cycle       25,528
    Memory Throughput                 %        87.44
    DRAM Throughput                   %        87.44
    Duration                         us        16.19
    L1/TEX Cache Throughput           %        18.02
    L2 Cache Throughput               %        24.23
    SM Active Cycles              cycle    18,017.58
    Compute (SM) Throughput           %        15.63
    ----------------------- ----------- ------------

    INF   This workload is utilizing greater than 80.0% of the available compute or memory performance of the device.   
          To further improve performance, work will likely need to be shifted from the most utilized to another unit.   
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (FP32) to double (FP64) performance on this device is 32:1. The workload achieved 0%  
          of this device's FP32 peak performance and 15% of its FP64 peak performance. See the Profiling Guide          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Kbyte       393.22
    Dropped Samples                sample            0
    Maximum Sampling Interval       cycle       20,000
    # Pass Groups                                    1
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.14
    Executed Ipc Elapsed  inst/cycle         0.12
    Issue Slots Busy               %         3.62
    Issued Ipc Active     inst/cycle         0.14
    SM Busy                        %        18.91
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 81.09%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ----------------- ----------- ------------
    Metric Name       Metric Unit Metric Value
    ----------------- ----------- ------------
    Memory Throughput     Gbyte/s       323.82
    Mem Busy                    %        24.23
    Max Bandwidth               %        87.44
    L1/TEX Hit Rate             %            0
    L2 Hit Rate                 %        33.93
    Mem Pipes Busy              %         7.33
    ----------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %         3.66
    Issued Warp Per Scheduler                        0.04
    No Eligible                            %        96.34
    Active Warps Per Scheduler          warp         4.31
    Eligible Warps Per Scheduler        warp         0.05
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 12.56%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 27.3 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 8 warps per scheduler, this workload allocates an average of  
          4.31 active warps per scheduler, but only an average of 0.05 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp  
          State Statistics and Source Counters sections.                                                                

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle       117.82
    Warp Cycles Per Executed Instruction           cycle       120.97
    Avg. Active Threads Per Warp                                31.99
    Avg. Not Predicated Off Threads Per Warp                    30.04
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 12.56%                                                                                          
          On average, each warp of this workload spends 109.0 cycles being stalled waiting for a scoreboard dependency  
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 92.6% of the total average of 117.8 cycles       
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Profiling Guide                                                                            
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       636.09
    Executed Instructions                           inst      101,775
    Avg. Issued Instructions Per Scheduler          inst       653.13
    Issued Instructions                             inst      104,501
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     88
    Registers Per Thread             register/thread              28
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              40
    Stack Size                                                 1,024
    Threads                                   thread          22,528
    # TPCs                                                        20
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                0.55
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        53.92
    Achieved Active Warps Per SM           warp        17.25
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 12.56%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (53.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    81,925.50
    Total DRAM Elapsed Cycles        cycle      749,568
    Average L1 Active Cycles         cycle    18,017.58
    Total L1 Elapsed Cycles          cycle      872,008
    Average L2 Active Cycles         cycle    17,180.19
    Total L2 Elapsed Cycles          cycle      782,176
    Average SM Active Cycles         cycle    18,017.58
    Total SM Elapsed Cycles          cycle      872,008
    Average SMSP Active Cycles       cycle    17,842.22
    Total SMSP Elapsed Cycles        cycle    3,488,032
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 8.886%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 10.75% above the average, while the minimum instance value is 6.09% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 9.135%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 11.16% above the average, while the minimum instance value is 5.00% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.886%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 10.75% above the average, while the minimum instance value is 6.09% below the       
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.07
    Branch Instructions              inst        6,907
    Branch Efficiency                   %        99.95
    Avg. Divergent Branches                       0.01
    ------------------------- ----------- ------------

  cuda_right_hand_side(const double *, double *, int, double, double, double, double) (256, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.71
    SM Frequency                    Ghz         1.59
    Elapsed Cycles                cycle      355,997
    Memory Throughput                 %         3.65
    DRAM Throughput                   %         3.65
    Duration                         us       223.01
    L1/TEX Cache Throughput           %         4.53
    L2 Cache Throughput               %         3.02
    SM Active Cycles              cycle   308,209.60
    Compute (SM) Throughput           %        75.64
    ----------------------- ----------- ------------

    OPT   Compute is more heavily utilized than Memory: Look at the Compute Workload Analysis section to see what the   
          compute pipelines are spending their time doing. Also, consider whether any computation is redundant and      
          could be reduced or moved to look-up tables.                                                                  

    Section: GPU Speed Of Light Roofline Chart
    OPT   Est. Speedup: 82.31%                                                                                          
          The ratio of peak float (FP32) to double (FP64) performance on this device is 32:1. The workload achieved     
          close to 0% of this device's FP32 peak performance and 56% of its FP64 peak performance. If Compute Workload  
          Analysis determines that this workload is FP64 bound, consider using 32-bit precision floating point          
          operations to improve its performance. See the Profiling Guide                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Kbyte       393.22
    Dropped Samples                sample            0
    Maximum Sampling Interval       cycle       20,000
    # Pass Groups                                    1
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.11
    Executed Ipc Elapsed  inst/cycle         0.09
    Issue Slots Busy               %         2.65
    Issued Ipc Active     inst/cycle         0.11
    SM Busy                        %        86.98
    -------------------- ----------- ------------

    OPT   FP64 is the highest-utilized pipeline (87.0%) based on active cycles, taking into account the rates of its    
          different instructions. It executes 64-bit floating point operations. The pipeline is over-utilized and       
          likely a performance bottleneck. Based on the number of executed instructions, the highest utilized pipeline  
          (87.0%) is FP64. It executes 64-bit floating point operations. Comparing the two, the overall pipeline        
          utilization appears to be caused by frequent, low-latency instructions. See the Profiling Guide               
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-decoder) or hover over the          
          pipeline name to understand the workloads handled by each pipeline. The Instruction Statistics section shows  
          the mix of executed instructions for this workload. Check the Warp State Statistics section for which         
          reasons cause warps to stall.                                                                                 

    Section: Memory Workload Analysis
    ----------------- ----------- ------------
    Metric Name       Metric Unit Metric Value
    ----------------- ----------- ------------
    Memory Throughput     Gbyte/s        15.68
    Mem Busy                    %         2.98
    Max Bandwidth               %         3.65
    L1/TEX Hit Rate             %        79.42
    L2 Hit Rate                 %        80.60
    Mem Pipes Busy              %        19.77
    ----------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 3.24%                                                                                           
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 9.1 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 3.395%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32     
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %         2.65
    Issued Warp Per Scheduler                        0.03
    No Eligible                            %        97.35
    Active Warps Per Scheduler          warp         6.63
    Eligible Warps Per Scheduler        warp         0.07
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 24.36%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 37.7 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 8 warps per scheduler, this workload allocates an average of  
          6.63 active warps per scheduler, but only an average of 0.07 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle       249.87
    Warp Cycles Per Executed Instruction           cycle       251.10
    Avg. Active Threads Per Warp                                31.99
    Avg. Not Predicated Off Threads Per Warp                    30.67
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 24.36%                                                                                          
          On average, each warp of this workload spends 121.1 cycles being stalled waiting for a scoreboard dependency  
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 48.5% of the total average of 249.9 cycles       
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 24.36%                                                                                          
          On average, each warp of this workload spends 121.0 cycles being stalled waiting for the L1 instruction queue 
          for texture operations to be not full. This stall reason is high in cases of extreme utilization of the       
          L1TEX pipeline. Try issuing fewer texture fetches, surface loads, surface stores, or decoupled math           
          operations. If applicable, consider combining multiple lower-width memory operations into fewer wider memory  
          operations and try interleaving memory operations and math instructions. Consider converting texture lookups  
          or surface loads into global memory lookups. Texture can accept four threads' requests per cycle, whereas     
          global accepts 32 threads. This stall type represents about 48.4% of the total average of 249.9 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Profiling Guide                                                                            
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst     8,125.43
    Executed Instructions                           inst    1,300,068
    Avg. Issued Instructions Per Scheduler          inst     8,165.48
    Issued Instructions                             inst    1,306,477
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 16.53%                                                                                          
          This kernel executes 388846 fused and 238388 non-fused FP64 instructions. By converting pairs of non-fused    
          instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point),           
          higher-throughput equivalent, the achieved FP64 performance could be increased by up to 19% (relative to its  
          current performance).                                                                                         

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    256
    Registers Per Thread             register/thread              56
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              40
    Stack Size                                                 1,024
    Threads                                   thread          65,536
    # TPCs                                                        20
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                1.60
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 97 thread blocks.   
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 50.0% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        82.76
    Achieved Active Warps Per SM           warp        26.48
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 17.24%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (82.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       54,641
    Total DRAM Elapsed Cycles        cycle   11,964,416
    Average L1 Active Cycles         cycle   308,209.60
    Total L1 Elapsed Cycles          cycle   14,177,288
    Average L2 Active Cycles         cycle    42,202.66
    Total L2 Elapsed Cycles          cycle   10,895,744
    Average SM Active Cycles         cycle   308,209.60
    Total SM Elapsed Cycles          cycle   14,177,288
    Average SMSP Active Cycles       cycle   307,910.58
    Total SMSP Elapsed Cycles        cycle   56,709,152
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 11.11%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 12.78% above the average, while the minimum instance value is 1.86% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.19%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 12.88% above the average, while the minimum instance value is 1.84% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.11%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 12.78% above the average, while the minimum instance value is 1.86% below the       
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.07
    Branch Instructions              inst       88,024
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 8.542%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 421875 excessive sectors (69% of the      
          total 612171 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source         
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  cuda_aux_computation(const double *, const double *, double *, double, int, double) (88, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.52
    SM Frequency                    Ghz         1.57
    Elapsed Cycles                cycle       26,537
    Memory Throughput                 %        89.77
    DRAM Throughput                   %        89.77
    Duration                         us        16.80
    L1/TEX Cache Throughput           %        18.21
    L2 Cache Throughput               %        23.33
    SM Active Cycles              cycle    17,957.75
    Compute (SM) Throughput           %        15.78
    ----------------------- ----------- ------------

    INF   This workload is utilizing greater than 80.0% of the available compute or memory performance of the device.   
          To further improve performance, work will likely need to be shifted from the most utilized to another unit.   
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: GPU Speed Of Light Roofline Chart
    OPT   Est. Speedup: 18.38%                                                                                          
          The ratio of peak float (FP32) to double (FP64) performance on this device is 32:1. The workload achieved 0%  
          of this device's FP32 peak performance and 15% of its FP64 peak performance. If Compute Workload Analysis     
          determines that this workload is FP64 bound, consider using 32-bit precision floating point operations to     
          improve its performance. See the Profiling Guide                                                              
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Kbyte       393.22
    Dropped Samples                sample            0
    Maximum Sampling Interval       cycle       20,000
    # Pass Groups                                    1
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.14
    Executed Ipc Elapsed  inst/cycle         0.12
    Issue Slots Busy               %         3.64
    Issued Ipc Active     inst/cycle         0.15
    SM Busy                        %        18.98
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 81.02%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ----------------- ----------- ------------
    Metric Name       Metric Unit Metric Value
    ----------------- ----------- ------------
    Memory Throughput     Gbyte/s       316.93
    Mem Busy                    %        23.33
    Max Bandwidth               %        89.77
    L1/TEX Hit Rate             %            0
    L2 Hit Rate                 %        33.90
    Mem Pipes Busy              %         7.40
    ----------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %         3.63
    Issued Warp Per Scheduler                        0.04
    No Eligible                            %        96.37
    Active Warps Per Scheduler          warp         4.27
    Eligible Warps Per Scheduler        warp         0.04
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 10.23%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 27.6 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 8 warps per scheduler, this workload allocates an average of  
          4.27 active warps per scheduler, but only an average of 0.04 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp  
          State Statistics and Source Counters sections.                                                                

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle       117.72
    Warp Cycles Per Executed Instruction           cycle       120.87
    Avg. Active Threads Per Warp                                31.99
    Avg. Not Predicated Off Threads Per Warp                    30.04
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 10.23%                                                                                          
          On average, each warp of this workload spends 108.2 cycles being stalled waiting for a scoreboard dependency  
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 91.9% of the total average of 117.7 cycles       
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Profiling Guide                                                                            
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       636.09
    Executed Instructions                           inst      101,775
    Avg. Issued Instructions Per Scheduler          inst       653.12
    Issued Instructions                             inst      104,500
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     88
    Registers Per Thread             register/thread              28
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              40
    Stack Size                                                 1,024
    Threads                                   thread          22,528
    # TPCs                                                        20
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                0.55
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        53.69
    Achieved Active Warps Per SM           warp        17.18
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 10.23%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (53.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    83,193.50
    Total DRAM Elapsed Cycles        cycle      741,376
    Average L1 Active Cycles         cycle    17,957.75
    Total L1 Elapsed Cycles          cycle      863,982
    Average L2 Active Cycles         cycle    17,224.66
    Total L2 Elapsed Cycles          cycle      812,672
    Average SM Active Cycles         cycle    17,957.75
    Total SM Elapsed Cycles          cycle      863,982
    Average SMSP Active Cycles       cycle    17,998.04
    Total SMSP Elapsed Cycles        cycle    3,455,928
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 8.665%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 10.42% above the average, while the minimum instance value is 4.82% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 9.63%                                                                                           
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 11.56% above the average, while the minimum instance value is 3.91% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.665%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 10.42% above the average, while the minimum instance value is 4.82% below the       
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.07
    Branch Instructions              inst        6,907
    Branch Efficiency                   %        99.95
    Avg. Divergent Branches                       0.01
    ------------------------- ----------- ------------

  cuda_right_hand_side(const double *, double *, int, double, double, double, double) (256, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.73
    SM Frequency                    Ghz         1.59
    Elapsed Cycles                cycle      356,266
    Memory Throughput                 %         3.62
    DRAM Throughput                   %         3.62
    Duration                         us       223.30
    L1/TEX Cache Throughput           %         4.52
    L2 Cache Throughput               %         3.02
    SM Active Cycles              cycle   308,201.17
    Compute (SM) Throughput           %        75.63
    ----------------------- ----------- ------------

    OPT   Compute is more heavily utilized than Memory: Look at the Compute Workload Analysis section to see what the   
          compute pipelines are spending their time doing. Also, consider whether any computation is redundant and      
          could be reduced or moved to look-up tables.                                                                  

    Section: GPU Speed Of Light Roofline Chart
    OPT   Est. Speedup: 82.31%                                                                                          
          The ratio of peak float (FP32) to double (FP64) performance on this device is 32:1. The workload achieved     
          close to 0% of this device's FP32 peak performance and 56% of its FP64 peak performance. If Compute Workload  
          Analysis determines that this workload is FP64 bound, consider using 32-bit precision floating point          
          operations to improve its performance. See the Profiling Guide                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Kbyte       393.22
    Dropped Samples                sample            0
    Maximum Sampling Interval       cycle       20,000
    # Pass Groups                                    1
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.11
    Executed Ipc Elapsed  inst/cycle         0.09
    Issue Slots Busy               %         2.65
    Issued Ipc Active     inst/cycle         0.11
    SM Busy                        %        86.99
    -------------------- ----------- ------------

    OPT   FP64 is the highest-utilized pipeline (87.0%) based on active cycles, taking into account the rates of its    
          different instructions. It executes 64-bit floating point operations. The pipeline is over-utilized and       
          likely a performance bottleneck. Based on the number of executed instructions, the highest utilized pipeline  
          (87.0%) is FP64. It executes 64-bit floating point operations. Comparing the two, the overall pipeline        
          utilization appears to be caused by frequent, low-latency instructions. See the Profiling Guide               
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-decoder) or hover over the          
          pipeline name to understand the workloads handled by each pipeline. The Instruction Statistics section shows  
          the mix of executed instructions for this workload. Check the Warp State Statistics section for which         
          reasons cause warps to stall.                                                                                 

    Section: Memory Workload Analysis
    ----------------- ----------- ------------
    Metric Name       Metric Unit Metric Value
    ----------------- ----------- ------------
    Memory Throughput     Gbyte/s        15.57
    Mem Busy                    %         2.98
    Max Bandwidth               %         3.62
    L1/TEX Hit Rate             %        79.24
    L2 Hit Rate                 %        80.55
    Mem Pipes Busy              %        19.76
    ----------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 3.238%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 9.1 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 3.393%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32     
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %         2.65
    Issued Warp Per Scheduler                        0.03
    No Eligible                            %        97.35
    Active Warps Per Scheduler          warp         6.63
    Eligible Warps Per Scheduler        warp         0.07
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 24.37%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 37.7 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 8 warps per scheduler, this workload allocates an average of  
          6.63 active warps per scheduler, but only an average of 0.07 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle       249.89
    Warp Cycles Per Executed Instruction           cycle       251.13
    Avg. Active Threads Per Warp                                31.99
    Avg. Not Predicated Off Threads Per Warp                    30.67
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 24.37%                                                                                          
          On average, each warp of this workload spends 121.1 cycles being stalled waiting for a scoreboard dependency  
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 48.5% of the total average of 249.9 cycles       
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 24.37%                                                                                          
          On average, each warp of this workload spends 120.9 cycles being stalled waiting for the L1 instruction queue 
          for texture operations to be not full. This stall reason is high in cases of extreme utilization of the       
          L1TEX pipeline. Try issuing fewer texture fetches, surface loads, surface stores, or decoupled math           
          operations. If applicable, consider combining multiple lower-width memory operations into fewer wider memory  
          operations and try interleaving memory operations and math instructions. Consider converting texture lookups  
          or surface loads into global memory lookups. Texture can accept four threads' requests per cycle, whereas     
          global accepts 32 threads. This stall type represents about 48.4% of the total average of 249.9 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Profiling Guide                                                                            
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst     8,125.43
    Executed Instructions                           inst    1,300,068
    Avg. Issued Instructions Per Scheduler          inst     8,165.51
    Issued Instructions                             inst    1,306,482
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 16.53%                                                                                          
          This kernel executes 388846 fused and 238388 non-fused FP64 instructions. By converting pairs of non-fused    
          instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point),           
          higher-throughput equivalent, the achieved FP64 performance could be increased by up to 19% (relative to its  
          current performance).                                                                                         

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    256
    Registers Per Thread             register/thread              56
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              40
    Stack Size                                                 1,024
    Threads                                   thread          65,536
    # TPCs                                                        20
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                1.60
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 97 thread blocks.   
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 50.0% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        82.77
    Achieved Active Warps Per SM           warp        26.49
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 17.23%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (82.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       54,321
    Total DRAM Elapsed Cycles        cycle   12,013,568
    Average L1 Active Cycles         cycle   308,201.17
    Total L1 Elapsed Cycles          cycle   14,179,538
    Average L2 Active Cycles         cycle    42,380.84
    Total L2 Elapsed Cycles          cycle   10,907,968
    Average SM Active Cycles         cycle   308,201.17
    Total SM Elapsed Cycles          cycle   14,179,538
    Average SMSP Active Cycles       cycle   307,935.96
    Total SMSP Elapsed Cycles        cycle   56,718,152
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 11.01%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 12.67% above the average, while the minimum instance value is 1.90% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.17%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 12.86% above the average, while the minimum instance value is 1.90% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.01%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 12.67% above the average, while the minimum instance value is 1.90% below the       
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.07
    Branch Instructions              inst       88,024
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 8.568%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 421875 excessive sectors (69% of the      
          total 612171 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source         
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  cuda_aux_computation(const double *, const double *, double *, double, int, double) (88, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.70
    SM Frequency                    Ghz         1.58
    Elapsed Cycles                cycle       26,012
    Memory Throughput                 %        89.36
    DRAM Throughput                   %        89.36
    Duration                         us        16.35
    L1/TEX Cache Throughput           %        18.17
    L2 Cache Throughput               %        23.85
    SM Active Cycles              cycle    18,054.05
    Compute (SM) Throughput           %        15.76
    ----------------------- ----------- ------------

    INF   This workload is utilizing greater than 80.0% of the available compute or memory performance of the device.   
          To further improve performance, work will likely need to be shifted from the most utilized to another unit.   
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: GPU Speed Of Light Roofline Chart
    OPT   Est. Speedup: 18.29%                                                                                          
          The ratio of peak float (FP32) to double (FP64) performance on this device is 32:1. The workload achieved 0%  
          of this device's FP32 peak performance and 15% of its FP64 peak performance. If Compute Workload Analysis     
          determines that this workload is FP64 bound, consider using 32-bit precision floating point operations to     
          improve its performance. See the Profiling Guide                                                              
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Kbyte       393.22
    Dropped Samples                sample            0
    Maximum Sampling Interval       cycle       20,000
    # Pass Groups                                    1
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.14
    Executed Ipc Elapsed  inst/cycle         0.12
    Issue Slots Busy               %         3.62
    Issued Ipc Active     inst/cycle         0.14
    SM Busy                        %        18.88
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 81.12%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ----------------- ----------- ------------
    Metric Name       Metric Unit Metric Value
    ----------------- ----------- ------------
    Memory Throughput     Gbyte/s       325.90
    Mem Busy                    %        23.85
    Max Bandwidth               %        89.36
    L1/TEX Hit Rate             %            0
    L2 Hit Rate                 %        33.94
    Mem Pipes Busy              %         7.39
    ----------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %         3.64
    Issued Warp Per Scheduler                        0.04
    No Eligible                            %        96.36
    Active Warps Per Scheduler          warp         4.33
    Eligible Warps Per Scheduler        warp         0.05
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 10.64%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 27.5 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 8 warps per scheduler, this workload allocates an average of  
          4.33 active warps per scheduler, but only an average of 0.05 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp  
          State Statistics and Source Counters sections.                                                                

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle       119.14
    Warp Cycles Per Executed Instruction           cycle       122.33
    Avg. Active Threads Per Warp                                31.99
    Avg. Not Predicated Off Threads Per Warp                    30.04
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 10.64%                                                                                          
          On average, each warp of this workload spends 108.1 cycles being stalled waiting for a scoreboard dependency  
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 90.8% of the total average of 119.1 cycles       
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Profiling Guide                                                                            
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       636.09
    Executed Instructions                           inst      101,775
    Avg. Issued Instructions Per Scheduler          inst       653.14
    Issued Instructions                             inst      104,503
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     88
    Registers Per Thread             register/thread              28
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              40
    Stack Size                                                 1,024
    Threads                                   thread          22,528
    # TPCs                                                        20
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                0.55
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        53.38
    Achieved Active Warps Per SM           warp        17.08
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 10.64%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (53.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    83,266.50
    Total DRAM Elapsed Cycles        cycle      745,472
    Average L1 Active Cycles         cycle    18,054.05
    Total L1 Elapsed Cycles          cycle      864,984
    Average L2 Active Cycles         cycle       17,304
    Total L2 Elapsed Cycles          cycle      794,400
    Average SM Active Cycles         cycle    18,054.05
    Total SM Elapsed Cycles          cycle      864,984
    Average SMSP Active Cycles       cycle    17,956.96
    Total SMSP Elapsed Cycles        cycle    3,459,936
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 10.12%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 12.12% above the average, while the minimum instance value is 7.07% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.691%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 10.47% above the average, while the minimum instance value is 5.98% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 10.12%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 12.12% above the average, while the minimum instance value is 7.07% below the       
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.07
    Branch Instructions              inst        6,907
    Branch Efficiency                   %        99.95
    Avg. Divergent Branches                       0.01
    ------------------------- ----------- ------------

  cuda_right_hand_side(const double *, double *, int, double, double, double, double) (256, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.84
    SM Frequency                    Ghz         1.60
    Elapsed Cycles                cycle      355,996
    Memory Throughput                 %         3.61
    DRAM Throughput                   %         3.61
    Duration                         us       221.02
    L1/TEX Cache Throughput           %         4.53
    L2 Cache Throughput               %         3.04
    SM Active Cycles              cycle   308,212.95
    Compute (SM) Throughput           %        75.74
    ----------------------- ----------- ------------

    OPT   Compute is more heavily utilized than Memory: Look at the Compute Workload Analysis section to see what the   
          compute pipelines are spending their time doing. Also, consider whether any computation is redundant and      
          could be reduced or moved to look-up tables.                                                                  

    Section: GPU Speed Of Light Roofline Chart
    OPT   Est. Speedup: 82.31%                                                                                          
          The ratio of peak float (FP32) to double (FP64) performance on this device is 32:1. The workload achieved     
          close to 0% of this device's FP32 peak performance and 56% of its FP64 peak performance. If Compute Workload  
          Analysis determines that this workload is FP64 bound, consider using 32-bit precision floating point          
          operations to improve its performance. See the Profiling Guide                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Kbyte       393.22
    Dropped Samples                sample            0
    Maximum Sampling Interval       cycle       20,000
    # Pass Groups                                    1
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.11
    Executed Ipc Elapsed  inst/cycle         0.09
    Issue Slots Busy               %         2.65
    Issued Ipc Active     inst/cycle         0.11
    SM Busy                        %        86.98
    -------------------- ----------- ------------

    OPT   FP64 is the highest-utilized pipeline (87.0%) based on active cycles, taking into account the rates of its    
          different instructions. It executes 64-bit floating point operations. The pipeline is over-utilized and       
          likely a performance bottleneck. Based on the number of executed instructions, the highest utilized pipeline  
          (87.0%) is FP64. It executes 64-bit floating point operations. Comparing the two, the overall pipeline        
          utilization appears to be caused by frequent, low-latency instructions. See the Profiling Guide               
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-decoder) or hover over the          
          pipeline name to understand the workloads handled by each pipeline. The Instruction Statistics section shows  
          the mix of executed instructions for this workload. Check the Warp State Statistics section for which         
          reasons cause warps to stall.                                                                                 

    Section: Memory Workload Analysis
    ----------------- ----------- ------------
    Metric Name       Metric Unit Metric Value
    ----------------- ----------- ------------
    Memory Throughput     Gbyte/s        15.81
    Mem Busy                    %         2.99
    Max Bandwidth               %         3.61
    L1/TEX Hit Rate             %        79.34
    L2 Hit Rate                 %        80.67
    Mem Pipes Busy              %        19.79
    ----------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 3.244%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 9.1 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 3.399%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32     
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %         2.65
    Issued Warp Per Scheduler                        0.03
    No Eligible                            %        97.35
    Active Warps Per Scheduler          warp         6.63
    Eligible Warps Per Scheduler        warp         0.07
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 24.26%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 37.7 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 8 warps per scheduler, this workload allocates an average of  
          6.63 active warps per scheduler, but only an average of 0.07 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle       249.90
    Warp Cycles Per Executed Instruction           cycle       251.13
    Avg. Active Threads Per Warp                                31.99
    Avg. Not Predicated Off Threads Per Warp                    30.66
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 24.26%                                                                                          
          On average, each warp of this workload spends 121.1 cycles being stalled waiting for a scoreboard dependency  
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 48.4% of the total average of 249.9 cycles       
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 24.26%                                                                                          
          On average, each warp of this workload spends 120.9 cycles being stalled waiting for the L1 instruction queue 
          for texture operations to be not full. This stall reason is high in cases of extreme utilization of the       
          L1TEX pipeline. Try issuing fewer texture fetches, surface loads, surface stores, or decoupled math           
          operations. If applicable, consider combining multiple lower-width memory operations into fewer wider memory  
          operations and try interleaving memory operations and math instructions. Consider converting texture lookups  
          or surface loads into global memory lookups. Texture can accept four threads' requests per cycle, whereas     
          global accepts 32 threads. This stall type represents about 48.4% of the total average of 249.9 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Profiling Guide                                                                            
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst     8,125.43
    Executed Instructions                           inst    1,300,068
    Avg. Issued Instructions Per Scheduler          inst     8,165.48
    Issued Instructions                             inst    1,306,476
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 16.53%                                                                                          
          This kernel executes 388846 fused and 238388 non-fused FP64 instructions. By converting pairs of non-fused    
          instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point),           
          higher-throughput equivalent, the achieved FP64 performance could be increased by up to 19% (relative to its  
          current performance).                                                                                         

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    256
    Registers Per Thread             register/thread              56
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              40
    Stack Size                                                 1,024
    Threads                                   thread          65,536
    # TPCs                                                        20
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                1.60
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 97 thread blocks.   
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 50.0% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        82.74
    Achieved Active Warps Per SM           warp        26.48
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 17.26%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (82.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       54,613
    Total DRAM Elapsed Cycles        cycle   12,099,584
    Average L1 Active Cycles         cycle   308,212.95
    Total L1 Elapsed Cycles          cycle   14,158,936
    Average L2 Active Cycles         cycle    42,370.16
    Total L2 Elapsed Cycles          cycle   10,815,456
    Average SM Active Cycles         cycle   308,212.95
    Total SM Elapsed Cycles          cycle   14,158,936
    Average SMSP Active Cycles       cycle   307,953.91
    Total SMSP Elapsed Cycles        cycle   56,635,744
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 11.11%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 12.76% above the average, while the minimum instance value is 1.87% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.16%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 12.83% above the average, while the minimum instance value is 1.93% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.11%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 12.76% above the average, while the minimum instance value is 1.87% below the       
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.07
    Branch Instructions              inst       88,024
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 8.639%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 421875 excessive sectors (69% of the      
          total 612171 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source         
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  cuda_final_update(double *, const double *, const double *, const double *, const double *, double, int) (88, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.22
    SM Frequency                    Ghz         1.58
    Elapsed Cycles                cycle       54,965
    Memory Throughput                 %        89.27
    DRAM Throughput                   %        89.27
    Duration                         us        34.75
    L1/TEX Cache Throughput           %        17.18
    L2 Cache Throughput               %        22.38
    SM Active Cycles              cycle    45,481.22
    Compute (SM) Throughput           %        25.67
    ----------------------- ----------- ------------

    INF   This workload is utilizing greater than 80.0% of the available compute or memory performance of the device.   
          To further improve performance, work will likely need to be shifted from the most utilized to another unit.   
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: GPU Speed Of Light Roofline Chart
    OPT   Est. Speedup: 27.24%                                                                                          
          The ratio of peak float (FP32) to double (FP64) performance on this device is 32:1. The workload achieved 0%  
          of this device's FP32 peak performance and 22% of its FP64 peak performance. If Compute Workload Analysis     
          determines that this workload is FP64 bound, consider using 32-bit precision floating point operations to     
          improve its performance. See the Profiling Guide                                                              
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Kbyte       393.22
    Dropped Samples                sample            0
    Maximum Sampling Interval       cycle       20,000
    # Pass Groups                                    1
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.09
    Executed Ipc Elapsed  inst/cycle         0.08
    Issue Slots Busy               %         2.32
    Issued Ipc Active     inst/cycle         0.09
    SM Busy                        %        28.12
    -------------------- ----------- ------------

    INF   FP64 is the highest-utilized pipeline (28.1%) based on active cycles, taking into account the rates of its    
          different instructions. It executes 64-bit floating point operations. It is well-utilized, but should not be  
          a bottleneck.                                                                                                 

    Section: Memory Workload Analysis
    ----------------- ----------- ------------
    Metric Name       Metric Unit Metric Value
    ----------------- ----------- ------------
    Memory Throughput     Gbyte/s       355.20
    Mem Busy                    %        22.38
    Max Bandwidth               %        89.27
    L1/TEX Hit Rate             %        14.12
    L2 Hit Rate                 %        17.13
    Mem Pipes Busy              %         7.92
    ----------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %         2.35
    Issued Warp Per Scheduler                        0.02
    No Eligible                            %        97.65
    Active Warps Per Scheduler          warp         4.33
    Eligible Warps Per Scheduler        warp         0.03
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 10.73%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 42.6 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 8 warps per scheduler, this workload allocates an average of  
          4.33 active warps per scheduler, but only an average of 0.03 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp  
          State Statistics and Source Counters sections.                                                                

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle       184.29
    Warp Cycles Per Executed Instruction           cycle       187.62
    Avg. Active Threads Per Warp                                31.99
    Avg. Not Predicated Off Threads Per Warp                    30.79
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 10.73%                                                                                          
          On average, each warp of this workload spends 174.6 cycles being stalled waiting for a scoreboard dependency  
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 94.7% of the total average of 184.3 cycles       
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Profiling Guide                                                                            
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst     1,034.84
    Executed Instructions                           inst      165,575
    Avg. Issued Instructions Per Scheduler          inst     1,053.56
    Issued Instructions                             inst      168,570
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 3.747%                                                                                          
          This kernel executes 23448 fused and 8520 non-fused FP64 instructions. By converting pairs of non-fused       
          instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point),           
          higher-throughput equivalent, the achieved FP64 performance could be increased by up to 13% (relative to its  
          current performance).                                                                                         

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     88
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              40
    Stack Size                                                 1,024
    Threads                                   thread          22,528
    # TPCs                                                        20
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                0.55
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        53.40
    Achieved Active Warps Per SM           warp        17.09
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 10.73%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (53.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      192,876
    Total DRAM Elapsed Cycles        cycle    1,728,512
    Average L1 Active Cycles         cycle    45,481.22
    Total L1 Elapsed Cycles          cycle    1,992,458
    Average L2 Active Cycles         cycle    42,989.88
    Total L2 Elapsed Cycles          cycle    1,685,056
    Average SM Active Cycles         cycle    45,481.22
    Total SM Elapsed Cycles          cycle    1,992,458
    Average SMSP Active Cycles       cycle    44,883.99
    Total SMSP Elapsed Cycles        cycle    7,969,832
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.803%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 6.36% above the average, while the minimum instance value is 3.16% below the average.       
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.206%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 6.89% above the average, while the minimum instance value is 4.75% below the        
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.803%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 6.36% above the average, while the minimum instance value is 3.16% below the        
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.04
    Branch Instructions              inst        6,907
    Branch Efficiency                   %        99.95
    Avg. Divergent Branches                       0.01
    ------------------------- ----------- ------------

